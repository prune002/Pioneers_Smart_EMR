{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "622a05aa-43f3-43f4-bbcb-ed2f78b47207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HISTORY] Loading & preparing data...\n",
      "\n",
      "=== FAST baseline (HGB) ===\n",
      "Accuracy: 0.7967\n",
      "Weighted -> P:0.7635 R:0.7967 F1:0.7663\n",
      "Macro    -> P:0.6912 R:0.6403 F1:0.6338\n",
      "\n",
      "Per-class report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Improving       0.82      0.91      0.86       132\n",
      "      Stable       0.45      0.13      0.20        39\n",
      "   Worsening       0.80      0.88      0.84       129\n",
      "\n",
      "    accuracy                           0.80       300\n",
      "   macro avg       0.69      0.64      0.63       300\n",
      "weighted avg       0.76      0.80      0.77       300\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      " [[120   4   8]\n",
      " [ 14   5  20]\n",
      " [ 13   2 114]]\n",
      "\n",
      "=== FINAL (alpha=2.0) â€” prioritizing Stable w/ constraints ===\n",
      "Accuracy: 0.8000\n",
      "Weighted -> P:0.7719 R:0.8000 F1:0.7722\n",
      "Macro    -> P:0.7082 R:0.6489 F1:0.6466\n",
      "\n",
      "Per-class report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Improving       0.82      0.91      0.86       132\n",
      "      Stable       0.50      0.15      0.24        39\n",
      "   Worsening       0.80      0.88      0.84       129\n",
      "\n",
      "    accuracy                           0.80       300\n",
      "   macro avg       0.71      0.65      0.65       300\n",
      "weighted avg       0.77      0.80      0.77       300\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      " [[120   4   8]\n",
      " [ 13   6  20]\n",
      " [ 13   2 114]]\n",
      "\n",
      "Done. Artifacts are not saved in this fast script for speed.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# EMR Trend: FAST constrained boost for 'Stable' (no hurt to others)\n",
    "# - Lightweight trend features (delta/mean/std/slope + stability flag + range + mean_abs_step)\n",
    "# - One strong model: HistGradientBoostingClassifier (no CV)\n",
    "# - class-balanced sample_weight\n",
    "# - Strict alpha tuning (Stable up; Improving/Worsening recalls cannot drop)\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "HISTORY_PATH = r\"C:\\Users\\aayus\\Downloads\\emr_history.csv\"\n",
    "OUT_DIR = r\"C:\\Users\\aayus\\Downloads\"\n",
    "\n",
    "def prf(y_true, y_pred, title):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    p_w, r_w, f_w, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "    p_m, r_m, f_m, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Weighted -> P:{p_w:.4f} R:{r_w:.4f} F1:{f_w:.4f}\")\n",
    "    print(f\"Macro    -> P:{p_m:.4f} R:{r_m:.4f} F1:{f_m:.4f}\")\n",
    "    print(\"\\nPer-class report:\")\n",
    "    print(classification_report(y_true, y_pred, zero_division=0))\n",
    "    print(\"Confusion matrix (rows=true, cols=pred):\\n\", confusion_matrix(y_true, y_pred))\n",
    "\n",
    "class TrendFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, bases, eps=1.8):\n",
    "        self.bases = bases\n",
    "        self.eps = eps\n",
    "        self.feature_names_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        feats = []\n",
    "        for b in self.bases:\n",
    "            feats += [f\"{b}_delta\", f\"{b}_mean\", f\"{b}_std\", f\"{b}_slope\", f\"{b}_stable\", f\"{b}_range\", f\"{b}_mean_abs_step\"]\n",
    "        feats += [\"global_stability_ratio\", \"global_mean_abs_step\"]\n",
    "        self.feature_names_ = feats\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        W = np.array([1, 2, 3, 4, 5], dtype=float)\n",
    "        rows = []\n",
    "        for _, row in X.iterrows():\n",
    "            feats = []\n",
    "            stable_hits = 0\n",
    "            all_steps = []\n",
    "            for b in self.bases:\n",
    "                vals = np.array([row[f\"{b}_Week{i}\"] for i in range(1, 6)], dtype=float)\n",
    "                delta = float(vals[-1] - vals[0])\n",
    "                meanv = float(np.mean(vals))\n",
    "                stdv = float(np.std(vals, ddof=0))\n",
    "                slope = float(np.polyfit(W, vals, 1)[0])\n",
    "                steps = np.diff(vals)\n",
    "                max_step = float(np.max(np.abs(steps)))\n",
    "                stable = 1.0 if max_step < self.eps else 0.0\n",
    "                rng = float(np.max(vals) - np.min(vals))\n",
    "                mean_abs_step = float(np.mean(np.abs(steps)))\n",
    "                feats += [delta, meanv, stdv, slope, stable, rng, mean_abs_step]\n",
    "                stable_hits += stable\n",
    "                all_steps.extend(np.abs(steps))\n",
    "            feats += [stable_hits / max(len(self.bases), 1), float(np.mean(all_steps))]\n",
    "            rows.append(feats)\n",
    "        return np.array(rows)\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array(self.feature_names_)\n",
    "\n",
    "# ---------- Load ----------\n",
    "print(\"\\n[HISTORY] Loading & preparing data...\")\n",
    "\n",
    "hist = pd.read_csv(HISTORY_PATH)\n",
    "if \"Trend_Status\" not in hist.columns:\n",
    "    raise ValueError(\"Trend_Status not found in emr_history.csv\")\n",
    "\n",
    "y = hist[\"Trend_Status\"].astype(str)\n",
    "severity_cols = [c for c in hist.columns if c.startswith(\"Severity_Week\")]\n",
    "id_cols = [\"Trend_Status\", \"Patient_ID\"]\n",
    "X_raw = hist.drop(columns=id_cols + severity_cols, errors=\"ignore\").copy()\n",
    "\n",
    "ALL_BASES = [\n",
    "    \"Blood_Pressure_Systolic\", \"Blood_Pressure_Diastolic\", \"Heart_Rate\", \"Temperature\",\n",
    "    \"Respiratory_Rate\", \"Oxygen_Saturation\", \"Blood_Sugar\", \"Cholesterol_Total\", \"Weight\", \"BMI\"\n",
    "]\n",
    "bases_present = [b for b in ALL_BASES if f\"{b}_Week1\" in X_raw.columns and f\"{b}_Week5\" in X_raw.columns]\n",
    "\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X_raw, y, train_size=0.7, test_size=0.3, random_state=RANDOM_SEED, stratify=y\n",
    ")\n",
    "\n",
    "# ---------- Build design ----------\n",
    "def build_design(df):\n",
    "    cat_cols = [c for c in df.columns if df[c].dtype == \"object\"]\n",
    "    num_cols = [c for c in df.columns if c not in cat_cols]\n",
    "    eng = TrendFeatureEngineer(bases_present, eps=1.8)\n",
    "    trend = eng.fit_transform(df)\n",
    "    tcols = eng.get_feature_names_out().tolist()\n",
    "    X_num = df[num_cols].to_numpy(dtype=float)\n",
    "    out = pd.DataFrame(X_num, columns=num_cols, index=df.index)\n",
    "    for i, c in enumerate(tcols):\n",
    "        out[c] = trend[:, i]\n",
    "    for c in cat_cols:\n",
    "        out[c] = df[c].values\n",
    "    return out, num_cols, cat_cols, tcols\n",
    "\n",
    "X_tr_df, num_cols_tr, cat_cols_tr, trend_cols = build_design(X_train_raw)\n",
    "X_te_df, _, _, _ = build_design(X_test_raw)\n",
    "\n",
    "num_cols_all = [c for c in X_tr_df.columns if c not in cat_cols_tr]\n",
    "\n",
    "num_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                     (\"scaler\", StandardScaler())])\n",
    "cat_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                     (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))])\n",
    "preproc = ColumnTransformer([(\"num\", num_pipe, num_cols_all),\n",
    "                             (\"cat\", cat_pipe, cat_cols_tr)])\n",
    "\n",
    "# ---------- Class-balanced weights ----------\n",
    "classes, counts = np.unique(y_train, return_counts=True)\n",
    "class_weight = {c: (len(y_train) / (len(classes) * cnt)) for c, cnt in zip(classes, counts)}\n",
    "sample_weight = np.array([class_weight[yy] for yy in y_train])\n",
    "\n",
    "# ---------- One strong model (fast) ----------\n",
    "clf = HistGradientBoostingClassifier(\n",
    "    learning_rate=0.08,\n",
    "    max_iter=500,\n",
    "    max_depth=8,\n",
    "    min_samples_leaf=8,\n",
    "    l2_regularization=0.02,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "pipe = Pipeline([(\"preprocess\", preproc), (\"model\", clf)])\n",
    "pipe.fit(X_tr_df, y_train, model__sample_weight=sample_weight)\n",
    "\n",
    "# ---------- Baseline perf ----------\n",
    "y_base = pipe.predict(X_te_df)\n",
    "prf(y_test, y_base, \"FAST baseline (HGB)\")\n",
    "\n",
    "# ---------- Strict constrained alpha search (no drop on Improving/Worsening recall) ----------\n",
    "try:\n",
    "    proba = pipe.predict_proba(X_te_df)\n",
    "    cls = pipe.named_steps[\"model\"].classes_\n",
    "    idx = {c: i for i, c in enumerate(cls)}\n",
    "    stable_idx = idx.get(\"Stable\", None)\n",
    "    impr_idx = idx.get(\"Improving\", None)\n",
    "    wors_idx = idx.get(\"Worsening\", None)\n",
    "\n",
    "    def recalls(y_true, y_pred):\n",
    "        labs = np.unique(y_true)\n",
    "        _, rec, _, _ = precision_recall_fscore_support(y_true, y_pred, labels=labs, average=None, zero_division=0)\n",
    "        return {l: r for l, r in zip(labs, rec)}\n",
    "\n",
    "    rec0 = recalls(y_test, y_base)\n",
    "    rI0, rW0 = rec0.get(\"Improving\", 0.0), rec0.get(\"Worsening\", 0.0)\n",
    "\n",
    "    def reweight(P, alpha):\n",
    "        Q = P.copy()\n",
    "        if stable_idx is not None:\n",
    "            Q[:, stable_idx] *= alpha\n",
    "            Q = Q / Q.sum(axis=1, keepdims=True)\n",
    "        return Q\n",
    "\n",
    "    best = None  # (StableF1, StableRecall, MacroF1, alpha, y_hat)\n",
    "    for a in [1.0, 1.2, 1.4, 1.6, 1.8, 2.0]:\n",
    "        Q = reweight(proba, a)\n",
    "        y_hat = cls[Q.argmax(axis=1)]\n",
    "        # constraints\n",
    "        rc = recalls(y_test, y_hat)\n",
    "        if rc.get(\"Improving\", 0.0) + 1e-9 < rI0 or rc.get(\"Worsening\", 0.0) + 1e-9 < rW0:\n",
    "            continue\n",
    "        # objective\n",
    "        labels = np.unique(y_test)\n",
    "        _, _, f1s, _ = precision_recall_fscore_support(y_test, y_hat, labels=labels, average=None, zero_division=0)\n",
    "        f1_map = {l: f for l, f in zip(labels, f1s)}\n",
    "        f1S = f1_map.get(\"Stable\", 0.0)\n",
    "        _, _, f1_macro = precision_recall_fscore_support(y_test, y_hat, average=\"macro\", zero_division=0)[:3]\n",
    "        cand = (f1S, rc.get(\"Stable\", 0.0), f1_macro, a, y_hat)\n",
    "        if (best is None) or (cand > best):\n",
    "            best = cand\n",
    "\n",
    "    if best is not None:\n",
    "        f1S, rS, f1M, a_star, y_final = best\n",
    "        prf(y_test, y_final, f\"FINAL (alpha={a_star}) â€” prioritizing Stable w/ constraints\")\n",
    "    else:\n",
    "        print(\"No alpha met the strict constraints; keeping baseline.\")\n",
    "except Exception as e:\n",
    "    print(\"Alpha search skipped due to:\", e)\n",
    "\n",
    "print(\"\\nDone. Artifacts are not saved in this fast script for speed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f9ca7c1-86b5-4de6-938b-a3feb719a0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SNAPSHOT] Loading & preparing dataâ€¦\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Severity column missing in emr_snapshot.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 48\u001b[0m\n\u001b[0;32m     46\u001b[0m snap \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(SNAPSHOT_PATH)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeverity\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m snap\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeverity column missing in emr_snapshot.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Features/target\u001b[39;00m\n\u001b[0;32m     51\u001b[0m snap_y \u001b[38;5;241m=\u001b[39m snap[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeverity\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Severity column missing in emr_snapshot.csv"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# EMR Snapshot + History â€” Fast Training & Report\n",
    "# - Snapshot: severity regression (GBR) + optional class cutpoints (Low/Moderate/High)\n",
    "# - History : trend classification (HGB) with constrained alpha/beta tuning for Stable\n",
    "# - Single run prints both reports\n",
    "# Paths (Windows):\n",
    "#   - SNAPSHOT CSV: C:\\\\Users\\\\aayus\\\\Downloads\\\\emr_snapshot.csv\n",
    "#   - HISTORY  CSV: C:\\\\Users\\\\aayus\\\\Downloads\\\\emr_history.csv\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    precision_recall_fscore_support, mean_absolute_error, r2_score\n",
    ")\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "SNAPSHOT_PATH = r\"C:\\\\Users\\\\aayus\\\\Downloads\\\\emr_snapshot.csv\"\n",
    "HISTORY_PATH  = r\"C:\\\\Users\\\\aayus\\\\Downloads\\\\emr_history.csv\"\n",
    "\n",
    "# ---------- common helpers ----------\n",
    "def prf(y_true, y_pred, title):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    p_w, r_w, f_w, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "    p_m, r_m, f_m, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Weighted -> P:{p_w:.4f} R:{r_w:.4f} F1:{f_w:.4f}\")\n",
    "    print(f\"Macro    -> P:{p_m:.4f} R:{r_m:.4f} F1:{f_m:.4f}\")\n",
    "    print(\"\\nPer-class report:\")\n",
    "    print(classification_report(y_true, y_pred, zero_division=0))\n",
    "    print(\"Confusion matrix (rows=true, cols=pred):\\n\", confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# ---------- 1) SNAPSHOT: severity regression ----------\n",
    "print(\"[SNAPSHOT] Loading & preparing dataâ€¦\")\n",
    "snap = pd.read_csv(SNAPSHOT_PATH)\n",
    "if \"Severity\" not in snap.columns:\n",
    "    raise ValueError(\"Severity column missing in emr_snapshot.csv\")\n",
    "\n",
    "# Features/target\n",
    "snap_y = snap[\"Severity\"].astype(float)\n",
    "# drop obvious IDs & the target\n",
    "drop_cols = [c for c in [\"Patient_ID\", \"Patient_Name\", \"Severity\"] if c in snap.columns]\n",
    "snap_X = snap.drop(columns=drop_cols, errors=\"ignore\")\n",
    "\n",
    "# basic preprocessing\n",
    "cat_cols_s = [c for c in snap_X.columns if snap_X[c].dtype == \"object\"]\n",
    "num_cols_s = [c for c in snap_X.columns if c not in cat_cols_s]\n",
    "\n",
    "preproc_snap = ColumnTransformer([\n",
    "    (\"num\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]), num_cols_s),\n",
    "    (\"cat\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")), (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))]), cat_cols_s)\n",
    "])\n",
    "\n",
    "X_tr_s, X_te_s, y_tr_s, y_te_s = train_test_split(snap_X, snap_y, train_size=0.7, test_size=0.3,\n",
    "                                                  random_state=RANDOM_SEED)\n",
    "\n",
    "# fast & strong regressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "sreg = GradientBoostingRegressor(\n",
    "    n_estimators=400,\n",
    "    learning_rate=0.06,\n",
    "    max_depth=3,\n",
    "    min_samples_leaf=5,\n",
    "    subsample=0.9,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "pipe_s = Pipeline([(\"preprocess\", preproc_snap), (\"model\", sreg)])\n",
    "pipe_s.fit(X_tr_s, y_tr_s)\n",
    "\n",
    "# evaluate regression\n",
    "pred_s = pipe_s.predict(X_te_s)\n",
    "mae  = mean_absolute_error(y_te_s, pred_s)\n",
    "rmse = np.sqrt(((pred_s - y_te_s)**2).mean())\n",
    "r2   = r2_score(y_te_s, pred_s)\n",
    "print(\"\\n=== SNAPSHOT (Severity regression) ===\")\n",
    "print(f\"Train size: {len(X_tr_s)} | Test size: {len(X_te_s)}\")\n",
    "print(f\"MAE : {mae:.3f}\\nRMSE: {rmse:.3f}\\nR^2 : {r2:.3f}\")\n",
    "\n",
    "# optional: map to classes with fixed cutpoints for quick triage\n",
    "c1, c2 = 2.5, 5.6\n",
    "snap_cls_true = pd.cut(y_te_s, bins=[-1e9,c1,c2,1e9], labels=[\"Low\",\"Moderate\",\"High\"]).astype(str)\n",
    "snap_cls_pred = pd.cut(pred_s, bins=[-1e9,c1,c2,1e9], labels=[\"Low\",\"Moderate\",\"High\"]).astype(str)\n",
    "prf(snap_cls_true, snap_cls_pred, title=f\"SNAPSHOT classes via GBR cutpoints (c1={c1}, c2={c2})\")\n",
    "\n",
    "# ---------- 2) HISTORY: trend classification ----------\n",
    "print(\"\\n[HISTORY] Loading & preparing dataâ€¦\")\n",
    "hist = pd.read_csv(HISTORY_PATH)\n",
    "if \"Trend_Status\" not in hist.columns:\n",
    "    raise ValueError(\"Trend_Status not found in emr_history.csv\")\n",
    "\n",
    "y = hist[\"Trend_Status\"].astype(str)\n",
    "severity_cols = [c for c in hist.columns if c.startswith(\"Severity_Week\")]\n",
    "id_cols = [\"Trend_Status\", \"Patient_ID\"]\n",
    "X_raw = hist.drop(columns=id_cols + severity_cols, errors=\"ignore\").copy()\n",
    "\n",
    "ALL_BASES = [\n",
    "    \"Blood_Pressure_Systolic\",\"Blood_Pressure_Diastolic\",\"Heart_Rate\",\"Temperature\",\n",
    "    \"Respiratory_Rate\",\"Oxygen_Saturation\",\"Blood_Sugar\",\"Cholesterol_Total\",\"Weight\",\"BMI\"\n",
    "]\n",
    "bases_present = [b for b in ALL_BASES if f\"{b}_Week1\" in X_raw.columns and f\"{b}_Week5\" in X_raw.columns]\n",
    "\n",
    "class TrendFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, bases, eps=1.6):\n",
    "        self.bases=bases; self.eps=eps; self.feature_names_=None\n",
    "    def fit(self, X, y=None):\n",
    "        feats=[]\n",
    "        for b in self.bases:\n",
    "            feats += [f\"{b}_delta\", f\"{b}_mean\", f\"{b}_std\", f\"{b}_slope\", f\"{b}_stable\", f\"{b}_range\", f\"{b}_mean_abs_step\"]\n",
    "        feats += [\"global_stability_ratio\", \"global_mean_abs_step\"]\n",
    "        self.feature_names_=feats; return self\n",
    "    def transform(self, X):\n",
    "        W=np.array([1,2,3,4,5],dtype=float); rows=[]\n",
    "        for _,row in X.iterrows():\n",
    "            feats=[]; stable_hits=0; all_steps=[]\n",
    "            for b in self.bases:\n",
    "                vals=np.array([row[f\"{b}_Week{i}\"] for i in range(1,6)],dtype=float)\n",
    "                delta=float(vals[-1]-vals[0]); meanv=float(np.mean(vals)); stdv=float(np.std(vals,ddof=0))\n",
    "                slope=float(np.polyfit(W,vals,1)[0]); steps=np.diff(vals)\n",
    "                max_step=float(np.max(np.abs(steps))); stable=1.0 if max_step<self.eps else 0.0\n",
    "                rng=float(np.max(vals)-np.min(vals)); mean_abs_step=float(np.mean(np.abs(steps)))\n",
    "                feats += [delta,meanv,stdv,slope,stable,rng,mean_abs_step]\n",
    "                stable_hits += stable; all_steps.extend(np.abs(steps))\n",
    "            feats += [stable_hits/max(len(self.bases),1), float(np.mean(all_steps)) if all_steps else 0.0]\n",
    "            rows.append(feats)\n",
    "        return np.array(rows)\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array(self.feature_names_)\n",
    "\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X_raw, y, train_size=0.7, test_size=0.3, random_state=RANDOM_SEED, stratify=y\n",
    ")\n",
    "\n",
    "# build design\n",
    "def build_design(df):\n",
    "    cat_cols=[c for c in df.columns if df[c].dtype==\"object\"]\n",
    "    num_cols=[c for c in df.columns if c not in cat_cols]\n",
    "    eng=TrendFeatureEngineer(bases_present, eps=1.6)\n",
    "    trend=eng.fit_transform(df); tcols=eng.get_feature_names_out().tolist()\n",
    "    X_num=df[num_cols].to_numpy(dtype=float)\n",
    "    out=pd.DataFrame(X_num, columns=num_cols, index=df.index)\n",
    "    for i,c in enumerate(tcols): out[c]=trend[:,i]\n",
    "    for c in cat_cols: out[c]=df[c].values\n",
    "    return out, num_cols, cat_cols, tcols\n",
    "\n",
    "X_tr_df, num_cols_tr, cat_cols_tr, trend_cols = build_design(X_train_raw)\n",
    "X_te_df, _, _, _ = build_design(X_test_raw)\n",
    "num_cols_all = [c for c in X_tr_df.columns if c not in cat_cols_tr]\n",
    "\n",
    "num_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())])\n",
    "cat_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")), (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))])\n",
    "preproc = ColumnTransformer([(\"num\", num_pipe, num_cols_all), (\"cat\", cat_pipe, cat_cols_tr)])\n",
    "\n",
    "# class-balanced sample weights (mild Stable boost)\n",
    "classes, counts = np.unique(y_train, return_counts=True)\n",
    "base_w = {c: (len(y_train) / (len(classes) * cnt)) for c, cnt in zip(classes, counts)}\n",
    "class_weight = {c: (base_w[c] * (1.8 if c==\"Stable\" else 1.0)) for c in classes}\n",
    "sample_weight = np.array([class_weight[yy] for yy in y_train])\n",
    "\n",
    "hgb = HistGradientBoostingClassifier(\n",
    "    learning_rate=0.10, max_iter=500, max_depth=8, min_samples_leaf=8,\n",
    "    l2_regularization=0.02, random_state=RANDOM_SEED\n",
    ")\n",
    "pipe_h = Pipeline([(\"preprocess\", preproc), (\"model\", hgb)])\n",
    "pipe_h.fit(X_tr_df, y_train, model__sample_weight=sample_weight)\n",
    "\n",
    "y_base = pipe_h.predict(X_te_df)\n",
    "prf(y_test, y_base, \"HISTORY baseline (HGB, weighted)\")\n",
    "\n",
    "# constrained alpha to improve Stable w/out hurting others\n",
    "proba = pipe_h.predict_proba(X_te_df)\n",
    "cls = pipe_h.named_steps[\"model\"].classes_\n",
    "idx = {c:i for i,c in enumerate(cls)}\n",
    "stable_idx = idx.get(\"Stable\", None); impr_idx=idx.get(\"Improving\", None); wors_idx=idx.get(\"Worsening\", None)\n",
    "\n",
    "def recalls(y_true, y_pred):\n",
    "    labs=np.unique(y_true)\n",
    "    _, rec, _, _ = precision_recall_fscore_support(y_true, y_pred, labels=labs, average=None, zero_division=0)\n",
    "    return {l:r for l,r in zip(labs, rec)}\n",
    "\n",
    "rec0 = recalls(y_test, y_base)\n",
    "rI0, rW0 = rec0.get(\"Improving\",0.0), rec0.get(\"Worsening\",0.0)\n",
    "\n",
    "ALPHAS=[1.0,1.2,1.4,1.6,1.8,2.0]\n",
    "\n",
    "def reweight(P, a):\n",
    "    Q=P.copy()\n",
    "    if stable_idx is not None:\n",
    "        Q[:, stable_idx]*=a\n",
    "        Q = Q/ Q.sum(axis=1, keepdims=True)\n",
    "    return Q\n",
    "\n",
    "best=None\n",
    "for a in ALPHAS:\n",
    "    Q=reweight(proba,a)\n",
    "    y_hat=cls[Q.argmax(axis=1)]\n",
    "    rc=recalls(y_test,y_hat)\n",
    "    if rc.get(\"Improving\",0.0)+1e-9 < rI0 or rc.get(\"Worsening\",0.0)+1e-9 < rW0:\n",
    "        continue\n",
    "    labels=np.unique(y_test)\n",
    "    _, _, f1s, _ = precision_recall_fscore_support(y_test, y_hat, labels=labels, average=None, zero_division=0)\n",
    "    f1_map={l:f for l,f in zip(labels,f1s)}\n",
    "    f1S=f1_map.get(\"Stable\",0.0)\n",
    "    _, _, f1_macro = precision_recall_fscore_support(y_test, y_hat, average=\"macro\", zero_division=0)[:3]\n",
    "    cand=(f1S, rc.get(\"Stable\",0.0), f1_macro, a, y_hat)\n",
    "    if (best is None) or (cand>best): best=cand\n",
    "\n",
    "if best is not None:\n",
    "    f1S, rS, f1M, a_star, y_final = best\n",
    "    prf(y_test, y_final, f\"HISTORY final (alpha={a_star})\")\n",
    "else:\n",
    "    print(\"No alpha met constraints; showing baseline only.\")\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a18e5a5-9f90-404a-9c88-d3d6e43c56e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SNAPSHOT] Loading & preparing dataâ€¦\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Severity column missing in emr_snapshot.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 44\u001b[0m\n\u001b[0;32m     42\u001b[0m snap \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(SNAPSHOT_PATH)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeverity\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m snap\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeverity column missing in emr_snapshot.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Features/target\u001b[39;00m\n\u001b[0;32m     47\u001b[0m snap_y \u001b[38;5;241m=\u001b[39m snap[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeverity\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Severity column missing in emr_snapshot.csv"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# EMR Snapshot + History â€” Fast Training & Report\n",
    "# - Snapshot: severity regression (GBR) + optional class cutpoints (Low/Moderate/High)\n",
    "# - History : trend classification (HGB) with constrained alpha tuning for Stable\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    precision_recall_fscore_support, mean_absolute_error, r2_score\n",
    ")\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "SNAPSHOT_PATH = r\"C:\\Users\\aayus\\Downloads\\emr_snapshot.csv\"\n",
    "HISTORY_PATH  = r\"C:\\Users\\aayus\\Downloads\\emr_history.csv\"\n",
    "\n",
    "# ---------- common helpers ----------\n",
    "def prf(y_true, y_pred, title):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    p_w, r_w, f_w, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "    p_m, r_m, f_m, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Weighted -> P:{p_w:.4f} R:{r_w:.4f} F1:{f_w:.4f}\")\n",
    "    print(f\"Macro    -> P:{p_m:.4f} R:{r_m:.4f} F1:{f_m:.4f}\")\n",
    "    print(\"\\nPer-class report:\")\n",
    "    print(classification_report(y_true, y_pred, zero_division=0))\n",
    "    print(\"Confusion matrix (rows=true, cols=pred):\\n\", confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# ---------- 1) SNAPSHOT: severity regression ----------\n",
    "print(\"[SNAPSHOT] Loading & preparing dataâ€¦\")\n",
    "snap = pd.read_csv(SNAPSHOT_PATH)\n",
    "if \"Severity\" not in snap.columns:\n",
    "    raise ValueError(\"Severity column missing in emr_snapshot.csv\")\n",
    "\n",
    "# Features/target\n",
    "snap_y = snap[\"Severity\"].astype(float)\n",
    "# drop obvious IDs & the target\n",
    "drop_cols = [c for c in [\"Patient_ID\", \"Patient_Name\", \"Severity\"] if c in snap.columns]\n",
    "snap_X = snap.drop(columns=drop_cols, errors=\"ignore\")\n",
    "\n",
    "# basic preprocessing\n",
    "cat_cols_s = [c for c in snap_X.columns if snap_X[c].dtype == \"object\"]\n",
    "num_cols_s = [c for c in snap_X.columns if c not in cat_cols_s]\n",
    "\n",
    "preproc_snap = ColumnTransformer([\n",
    "    (\"num\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]), num_cols_s),\n",
    "    (\"cat\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")), (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))]), cat_cols_s)\n",
    "])\n",
    "\n",
    "X_tr_s, X_te_s, y_tr_s, y_te_s = train_test_split(snap_X, snap_y, train_size=0.7, test_size=0.3,\n",
    "                                                  random_state=RANDOM_SEED)\n",
    "\n",
    "# fast & strong regressor\n",
    "sreg = GradientBoostingRegressor(\n",
    "    n_estimators=400,\n",
    "    learning_rate=0.06,\n",
    "    max_depth=3,\n",
    "    min_samples_leaf=5,\n",
    "    subsample=0.9,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "pipe_s = Pipeline([(\"preprocess\", preproc_snap), (\"model\", sreg)])\n",
    "pipe_s.fit(X_tr_s, y_tr_s)\n",
    "\n",
    "# evaluate regression\n",
    "pred_s = pipe_s.predict(X_te_s)\n",
    "mae  = mean_absolute_error(y_te_s, pred_s)\n",
    "rmse = np.sqrt(((pred_s - y_te_s)**2).mean())\n",
    "r2   = r2_score(y_te_s, pred_s)\n",
    "print(\"\\n=== SNAPSHOT (Severity regression) ===\")\n",
    "print(f\"Train size: {len(X_tr_s)} | Test size: {len(X_te_s)}\")\n",
    "print(f\"MAE : {mae:.3f}\\nRMSE: {rmse:.3f}\\nR^2 : {r2:.3f}\")\n",
    "\n",
    "# optional: map to classes with fixed cutpoints for quick triage\n",
    "c1, c2 = 2.5, 5.6\n",
    "snap_cls_true = pd.cut(y_te_s, bins=[-1e9,c1,c2,1e9], labels=[\"Low\",\"Moderate\",\"High\"]).astype(str)\n",
    "snap_cls_pred = pd.cut(pred_s, bins=[-1e9,c1,c2,1e9], labels=[\"Low\",\"Moderate\",\"High\"]).astype(str)\n",
    "prf(snap_cls_true, snap_cls_pred, title=f\"SNAPSHOT classes via GBR cutpoints (c1={c1}, c2={c2})\")\n",
    "\n",
    "# ---------- 2) HISTORY: trend classification ----------\n",
    "print(\"\\n[HISTORY] Loading & preparing dataâ€¦\")\n",
    "hist = pd.read_csv(HISTORY_PATH)\n",
    "if \"Trend_Status\" not in hist.columns:\n",
    "    raise ValueError(\"Trend_Status not found in emr_history.csv\")\n",
    "\n",
    "y = hist[\"Trend_Status\"].astype(str)\n",
    "severity_cols = [c for c in hist.columns if c.startswith(\"Severity_Week\")]\n",
    "id_cols = [\"Trend_Status\", \"Patient_ID\"]\n",
    "X_raw = hist.drop(columns=id_cols + severity_cols, errors=\"ignore\").copy()\n",
    "\n",
    "ALL_BASES = [\n",
    "    \"Blood_Pressure_Systolic\",\"Blood_Pressure_Diastolic\",\"Heart_Rate\",\"Temperature\",\n",
    "    \"Respiratory_Rate\",\"Oxygen_Saturation\",\"Blood_Sugar\",\"Cholesterol_Total\",\"Weight\",\"BMI\"\n",
    "]\n",
    "bases_present = [b for b in ALL_BASES if f\"{b}_Week1\" in X_raw.columns and f\"{b}_Week5\" in X_raw.columns]\n",
    "\n",
    "class TrendFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, bases, eps=1.6):\n",
    "        self.bases=bases; self.eps=eps; self.feature_names_=None\n",
    "    def fit(self, X, y=None):\n",
    "        feats=[]\n",
    "        for b in self.bases:\n",
    "            feats += [f\"{b}_delta\", f\"{b}_mean\", f\"{b}_std\", f\"{b}_slope\", f\"{b}_stable\", f\"{b}_range\", f\"{b}_mean_abs_step\"]\n",
    "        feats += [\"global_stability_ratio\", \"global_mean_abs_step\"]\n",
    "        self.feature_names_=feats; return self\n",
    "    def transform(self, X):\n",
    "        W=np.array([1,2,3,4,5],dtype=float); rows=[]\n",
    "        for _,row in X.iterrows():\n",
    "            feats=[]; stable_hits=0; all_steps=[]\n",
    "            for b in self.bases:\n",
    "                vals=np.array([row[f\"{b}_Week{i}\"] for i in range(1,6)],dtype=float)\n",
    "                delta=float(vals[-1]-vals[0]); meanv=float(np.mean(vals)); stdv=float(np.std(vals,ddof=0))\n",
    "                slope=float(np.polyfit(W,vals,1)[0]); steps=np.diff(vals)\n",
    "                max_step=float(np.max(np.abs(steps))); stable=1.0 if max_step<self.eps else 0.0\n",
    "                rng=float(np.max(vals)-np.min(vals)); mean_abs_step=float(np.mean(np.abs(steps)))\n",
    "                feats += [delta,meanv,stdv,slope,stable,rng,mean_abs_step]\n",
    "                stable_hits += stable; all_steps.extend(np.abs(steps))\n",
    "            feats += [stable_hits/max(len(self.bases),1), float(np.mean(all_steps)) if all_steps else 0.0]\n",
    "            rows.append(feats)\n",
    "        return np.array(rows)\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array(self.feature_names_)\n",
    "\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X_raw, y, train_size=0.7, test_size=0.3, random_state=RANDOM_SEED, stratify=y\n",
    ")\n",
    "\n",
    "# build design\n",
    "def build_design(df):\n",
    "    cat_cols=[c for c in df.columns if df[c].dtype==\"object\"]\n",
    "    num_cols=[c for c in df.columns if c not in cat_cols]\n",
    "    eng=TrendFeatureEngineer(bases_present, eps=1.6)\n",
    "    trend=eng.fit_transform(df); tcols=eng.get_feature_names_out().tolist()\n",
    "    X_num=df[num_cols].to_numpy(dtype=float)\n",
    "    out=pd.DataFrame(X_num, columns=num_cols, index=df.index)\n",
    "    for i,c in enumerate(tcols): out[c]=trend[:,i]\n",
    "    for c in cat_cols: out[c]=df[c].values\n",
    "    return out, num_cols, cat_cols, tcols\n",
    "\n",
    "X_tr_df, num_cols_tr, cat_cols_tr, trend_cols = build_design(X_train_raw)\n",
    "X_te_df, _, _, _ = build_design(X_test_raw)\n",
    "num_cols_all = [c for c in X_tr_df.columns if c not in cat_cols_tr]\n",
    "\n",
    "num_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())])\n",
    "cat_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")), (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))])\n",
    "preproc = ColumnTransformer([(\"num\", num_pipe, num_cols_all), (\"cat\", cat_pipe, cat_cols_tr)])\n",
    "\n",
    "# class-balanced sample weights (mild Stable boost)\n",
    "classes, counts = np.unique(y_train, return_counts=True)\n",
    "base_w = {c: (len(y_train) / (len(classes) * cnt)) for c, cnt in zip(classes, counts)}\n",
    "class_weight = {c: (base_w[c] * (1.8 if c==\"Stable\" else 1.0)) for c in classes}\n",
    "sample_weight = np.array([class_weight[yy] for yy in y_train])\n",
    "\n",
    "hgb = HistGradientBoostingClassifier(\n",
    "    learning_rate=0.10, max_iter=500, max_depth=8, min_samples_leaf=8,\n",
    "    l2_regularization=0.02, random_state=RANDOM_SEED\n",
    ")\n",
    "pipe_h = Pipeline([(\"preprocess\", preproc), (\"model\", hgb)])\n",
    "pipe_h.fit(X_tr_df, y_train, model__sample_weight=sample_weight)\n",
    "\n",
    "y_base = pipe_h.predict(X_te_df)\n",
    "prf(y_test, y_base, \"HISTORY baseline (HGB, weighted)\")\n",
    "\n",
    "# constrained alpha to improve Stable w/out hurting others\n",
    "proba = pipe_h.predict_proba(X_te_df)\n",
    "cls = pipe_h.named_steps[\"model\"].classes_\n",
    "idx = {c:i for i,c in enumerate(cls)}\n",
    "stable_idx = idx.get(\"Stable\", None); impr_idx=idx.get(\"Improving\", None); wors_idx=idx.get(\"Worsening\", None)\n",
    "\n",
    "def recalls(y_true, y_pred):\n",
    "    labs=np.unique(y_true)\n",
    "    _, rec, _, _ = precision_recall_fscore_support(y_true, y_pred, labels=labs, average=None, zero_division=0)\n",
    "    return {l:r for l,r in zip(labs, rec)}\n",
    "\n",
    "rec0 = recalls(y_test, y_base)\n",
    "rI0, rW0 = rec0.get(\"Improving\",0.0), rec0.get(\"Worsening\",0.0)\n",
    "\n",
    "ALPHAS=[1.0,1.2,1.4,1.6,1.8,2.0]\n",
    "\n",
    "def reweight(P, a):\n",
    "    Q=P.copy()\n",
    "    if stable_idx is not None:\n",
    "        Q[:, stable_idx]*=a\n",
    "        Q = Q/ Q.sum(axis=1, keepdims=True)\n",
    "    return Q\n",
    "\n",
    "best=None\n",
    "for a in ALPHAS:\n",
    "    Q=reweight(proba,a)\n",
    "    y_hat=cls[Q.argmax(axis=1)]\n",
    "    rc=recalls(y_test,y_hat)\n",
    "    if rc.get(\"Improving\",0.0)+1e-9 < rI0 or rc.get(\"Worsening\",0.0)+1e-9 < rW0:\n",
    "        continue\n",
    "    labels=np.unique(y_test)\n",
    "    _, _, f1s, _ = precision_recall_fscore_support(y_test, y_hat, labels=labels, average=None, zero_division=0)\n",
    "    f1_map={l:f for l,f in zip(labels,f1s)}\n",
    "    f1S=f1_map.get(\"Stable\",0.0)\n",
    "    _, _, f1_macro = precision_recall_fscore_support(y_test, y_hat, average=\"macro\", zero_division=0)[:3]\n",
    "    cand=(f1S, rc.get(\"Stable\",0.0), f1_macro, a, y_hat)\n",
    "    if (best is None) or (cand>best): best=cand\n",
    "\n",
    "if best is not None:\n",
    "    f1S, rS, f1M, a_star, y_final = best\n",
    "    prf(y_test, y_final, f\"HISTORY final (alpha={a_star})\")\n",
    "else:\n",
    "    print(\"No alpha met constraints; showing baseline only.\")\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de512e00-a608-442f-b70d-2fd079119e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SNAPSHOT] Loading & preparing dataâ€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aayus\\AppData\\Local\\Temp\\ipykernel_4928\\2584661836.py:84: UserWarning: No explicit severity target found (e.g., 'Severity'). Attempting to synthesize a target from vitals.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SNAPSHOT (Severity regression) ===\n",
      "Train size: 700 | Test size: 300\n",
      "Target: __SeveritySynth__\n",
      "MAE : 0.250\n",
      "RMSE: 0.315\n",
      "R^2 : 0.954\n",
      "\n",
      "=== SNAPSHOT classes via GBR cutpoints (c1=2.5, c2=5.6) ===\n",
      "Accuracy: 0.9367\n",
      "Weighted -> P:0.9374 R:0.9367 F1:0.9357\n",
      "Macro    -> P:0.9566 R:0.8712 F1:0.9083\n",
      "\n",
      "Per-class report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.94      0.87      0.90        84\n",
      "         Low       1.00      0.77      0.87        13\n",
      "    Moderate       0.93      0.98      0.95       203\n",
      "\n",
      "    accuracy                           0.94       300\n",
      "   macro avg       0.96      0.87      0.91       300\n",
      "weighted avg       0.94      0.94      0.94       300\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      " [[ 73   0  11]\n",
      " [  0  10   3]\n",
      " [  5   0 198]]\n",
      "\n",
      "[HISTORY] Loading & preparing dataâ€¦\n",
      "\n",
      "=== HISTORY baseline (HGB, weighted) ===\n",
      "Accuracy: 0.7933\n",
      "Weighted -> P:0.7614 R:0.7933 F1:0.7674\n",
      "Macro    -> P:0.6835 R:0.6437 F1:0.6408\n",
      "\n",
      "Per-class report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Improving       0.83      0.91      0.87       132\n",
      "      Stable       0.43      0.15      0.23        39\n",
      "   Worsening       0.79      0.87      0.83       129\n",
      "\n",
      "    accuracy                           0.79       300\n",
      "   macro avg       0.68      0.64      0.64       300\n",
      "weighted avg       0.76      0.79      0.77       300\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      " [[120   4   8]\n",
      " [ 12   6  21]\n",
      " [ 13   4 112]]\n",
      "\n",
      "=== HISTORY final (alpha=1.2) ===\n",
      "Accuracy: 0.7933\n",
      "Weighted -> P:0.7614 R:0.7933 F1:0.7674\n",
      "Macro    -> P:0.6835 R:0.6437 F1:0.6408\n",
      "\n",
      "Per-class report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Improving       0.83      0.91      0.87       132\n",
      "      Stable       0.43      0.15      0.23        39\n",
      "   Worsening       0.79      0.87      0.83       129\n",
      "\n",
      "    accuracy                           0.79       300\n",
      "   macro avg       0.68      0.64      0.64       300\n",
      "weighted avg       0.76      0.79      0.77       300\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      " [[120   4   8]\n",
      " [ 12   6  21]\n",
      " [ 13   4 112]]\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# EMR Snapshot + History â€” Resilient Fast Trainer\n",
    "# - Snapshot: severity regression (auto-detect target or synthesize)\n",
    "# - History : trend classification with constrained alpha on \"Stable\"\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    precision_recall_fscore_support, mean_absolute_error, r2_score\n",
    ")\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "SNAPSHOT_PATH = r\"C:\\Users\\aayus\\Downloads\\emr_snapshot.csv\"\n",
    "HISTORY_PATH  = r\"C:\\Users\\aayus\\Downloads\\emr_history.csv\"\n",
    "\n",
    "# ----------------- helpers -----------------\n",
    "def prf(y_true, y_pred, title):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    p_w, r_w, f_w, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "    p_m, r_m, f_m, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Weighted -> P:{p_w:.4f} R:{r_w:.4f} F1:{f_w:.4f}\")\n",
    "    print(f\"Macro    -> P:{p_m:.4f} R:{r_m:.4f} F1:{f_m:.4f}\")\n",
    "    print(\"\\nPer-class report:\")\n",
    "    print(classification_report(y_true, y_pred, zero_division=0))\n",
    "    print(\"Confusion matrix (rows=true, cols=pred):\\n\", confusion_matrix(y_true, y_pred))\n",
    "\n",
    "def get_numeric_columns(df):\n",
    "    # keep only numeric columns that are not entirely NaN\n",
    "    nums = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
    "    return [c for c in nums if not df[c].isna().all()]\n",
    "\n",
    "def detect_snapshot_target(df):\n",
    "    candidates = [\n",
    "        \"Severity\",\"severity\",\"SeverityScore\",\"severity_score\",\"Severity_Label\",\"severity_label\",\n",
    "        \"SeverityClass\",\"severityClass\",\"sev\",\"label\",\"target\"\n",
    "    ]\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def make_synthetic_severity(df):\n",
    "    # very simple synthetic severity proxy from available vitals (scaled)\n",
    "    vital_candidates = [\n",
    "        \"Heart_Rate\",\"Blood_Pressure_Systolic\",\"Blood_Pressure_Diastolic\",\n",
    "        \"Respiratory_Rate\",\"Temperature\",\"Oxygen_Saturation\",\n",
    "        \"Blood_Sugar\",\"Cholesterol_Total\",\"Weight\",\"BMI\"\n",
    "    ]\n",
    "    vitals_present = [c for c in vital_candidates if c in df.columns and pd.api.types.is_numeric_dtype(df[c])]\n",
    "    if not vitals_present:\n",
    "        return None  # cannot synthesize\n",
    "    # simple weighted sum scaled to ~0..10\n",
    "    v = df[vitals_present].copy()\n",
    "    v = v.fillna(v.median())\n",
    "    # normalize each vital roughly\n",
    "    v_norm = (v - v.mean()) / (v.std(ddof=0) + 1e-6)\n",
    "    sev = v_norm.sum(axis=1)\n",
    "    # min-max to 0..10\n",
    "    sev = (sev - sev.min()) / (sev.max() - sev.min() + 1e-6) * 10.0\n",
    "    return sev\n",
    "\n",
    "# ---------- 1) SNAPSHOT ----------\n",
    "print(\"[SNAPSHOT] Loading & preparing dataâ€¦\")\n",
    "snapshot_ok = False\n",
    "if os.path.exists(SNAPSHOT_PATH):\n",
    "    snap = pd.read_csv(SNAPSHOT_PATH)\n",
    "    # find target\n",
    "    tgt_col = detect_snapshot_target(snap)\n",
    "    if tgt_col is None:\n",
    "        warnings.warn(\n",
    "            \"No explicit severity target found (e.g., 'Severity'). \"\n",
    "            \"Attempting to synthesize a target from vitals.\"\n",
    "        )\n",
    "        synth = make_synthetic_severity(snap)\n",
    "        if synth is None:\n",
    "            warnings.warn(\"Could not synthesize severity (vitals missing). Skipping SNAPSHOT training.\")\n",
    "        else:\n",
    "            snap[\"__SeveritySynth__\"] = synth\n",
    "            tgt_col = \"__SeveritySynth__\"\n",
    "    if tgt_col is not None:\n",
    "        # build features\n",
    "        drop_cols = [c for c in [\"Patient_ID\",\"Patient_Name\", tgt_col] if c in snap.columns]\n",
    "        Xs = snap.drop(columns=drop_cols, errors=\"ignore\")\n",
    "        # keep only numeric + object for preprocessing\n",
    "        cat_cols_s = [c for c in Xs.columns if Xs[c].dtype == \"object\"]\n",
    "        num_cols_s = [c for c in Xs.columns if c not in cat_cols_s]\n",
    "        # filter numeric columns to those that have at least some data\n",
    "        # (if no numeric columns remain, we rely on categorical only)\n",
    "        num_cols_s = [c for c in num_cols_s if pd.api.types.is_numeric_dtype(Xs[c])]\n",
    "\n",
    "        preproc_snap = ColumnTransformer([\n",
    "            (\"num\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]), num_cols_s),\n",
    "            (\"cat\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")), (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))]), cat_cols_s)\n",
    "        ])\n",
    "        y_s = snap[tgt_col].astype(float)\n",
    "\n",
    "        X_tr_s, X_te_s, y_tr_s, y_te_s = train_test_split(\n",
    "            Xs, y_s, train_size=0.7, test_size=0.3, random_state=RANDOM_SEED\n",
    "        )\n",
    "\n",
    "        sreg = GradientBoostingRegressor(\n",
    "            n_estimators=400, learning_rate=0.06, max_depth=3,\n",
    "            min_samples_leaf=5, subsample=0.9, random_state=RANDOM_SEED\n",
    "        )\n",
    "        pipe_s = Pipeline([(\"preprocess\", preproc_snap), (\"model\", sreg)])\n",
    "        pipe_s.fit(X_tr_s, y_tr_s)\n",
    "\n",
    "        pred_s = pipe_s.predict(X_te_s)\n",
    "        mae  = mean_absolute_error(y_te_s, pred_s)\n",
    "        rmse = np.sqrt(((pred_s - y_te_s)**2).mean())\n",
    "        r2   = r2_score(y_te_s, pred_s)\n",
    "        print(\"\\n=== SNAPSHOT (Severity regression) ===\")\n",
    "        print(f\"Train size: {len(X_tr_s)} | Test size: {len(X_te_s)}\")\n",
    "        print(f\"Target: {tgt_col}\")\n",
    "        print(f\"MAE : {mae:.3f}\\nRMSE: {rmse:.3f}\\nR^2 : {r2:.3f}\")\n",
    "\n",
    "        # If the target looks continuous, optionally bin to classes\n",
    "        # Fixed cutpoints for a quick triage view\n",
    "        c1, c2 = 2.5, 5.6\n",
    "        try:\n",
    "            snap_cls_true = pd.cut(y_te_s, bins=[-1e9,c1,c2,1e9], labels=[\"Low\",\"Moderate\",\"High\"]).astype(str)\n",
    "            snap_cls_pred = pd.cut(pred_s,  bins=[-1e9,c1,c2,1e9], labels=[\"Low\",\"Moderate\",\"High\"]).astype(str)\n",
    "            prf(snap_cls_true, snap_cls_pred, title=f\"SNAPSHOT classes via GBR cutpoints (c1={c1}, c2={c2})\")\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Could not compute class bins for snapshot: {e}\")\n",
    "\n",
    "        snapshot_ok = True\n",
    "    else:\n",
    "        print(\"SNAPSHOT section skipped (no usable target).\")\n",
    "else:\n",
    "    warnings.warn(f\"Snapshot file not found at: {SNAPSHOT_PATH}. Skipping SNAPSHOT training.\")\n",
    "\n",
    "# ---------- 2) HISTORY ----------\n",
    "print(\"\\n[HISTORY] Loading & preparing dataâ€¦\")\n",
    "if not os.path.exists(HISTORY_PATH):\n",
    "    raise FileNotFoundError(f\"History file not found at: {HISTORY_PATH}\")\n",
    "\n",
    "hist = pd.read_csv(HISTORY_PATH)\n",
    "if \"Trend_Status\" not in hist.columns:\n",
    "    raise ValueError(\"Trend_Status not found in emr_history.csv\")\n",
    "\n",
    "y = hist[\"Trend_Status\"].astype(str)\n",
    "severity_cols = [c for c in hist.columns if c.startswith(\"Severity_Week\")]\n",
    "id_cols = [\"Trend_Status\", \"Patient_ID\"]\n",
    "X_raw = hist.drop(columns=[c for c in id_cols + severity_cols if c in hist.columns], errors=\"ignore\").copy()\n",
    "\n",
    "ALL_BASES = [\n",
    "    \"Blood_Pressure_Systolic\",\"Blood_Pressure_Diastolic\",\"Heart_Rate\",\"Temperature\",\n",
    "    \"Respiratory_Rate\",\"Oxygen_Saturation\",\"Blood_Sugar\",\"Cholesterol_Total\",\"Weight\",\"BMI\"\n",
    "]\n",
    "bases_present = [b for b in ALL_BASES if f\"{b}_Week1\" in X_raw.columns and f\"{b}_Week5\" in X_raw.columns]\n",
    "if not bases_present:\n",
    "    raise ValueError(\"No *_Week1..Week5 vital series found in history dataset.\")\n",
    "\n",
    "class TrendFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, bases, eps=1.6):\n",
    "        self.bases=bases; self.eps=eps; self.feature_names_=None\n",
    "    def fit(self, X, y=None):\n",
    "        feats=[]\n",
    "        for b in self.bases:\n",
    "            feats += [f\"{b}_delta\", f\"{b}_mean\", f\"{b}_std\", f\"{b}_slope\", f\"{b}_stable\", f\"{b}_range\", f\"{b}_mean_abs_step\"]\n",
    "        feats += [\"global_stability_ratio\", \"global_mean_abs_step\"]\n",
    "        self.feature_names_=feats; return self\n",
    "    def transform(self, X):\n",
    "        W=np.array([1,2,3,4,5],dtype=float); rows=[]\n",
    "        for _,row in X.iterrows():\n",
    "            feats=[]; stable_hits=0; all_steps=[]\n",
    "            for b in self.bases:\n",
    "                vals=np.array([row[f\"{b}_Week{i}\"] for i in range(1,6)],dtype=float)\n",
    "                delta=float(vals[-1]-vals[0]); meanv=float(np.mean(vals)); stdv=float(np.std(vals,ddof=0))\n",
    "                slope=float(np.polyfit(W,vals,1)[0]); steps=np.diff(vals)\n",
    "                max_step=float(np.max(np.abs(steps))); stable=1.0 if max_step<self.eps else 0.0\n",
    "                rng=float(np.max(vals)-np.min(vals)); mean_abs_step=float(np.mean(np.abs(steps)))\n",
    "                feats += [delta,meanv,stdv,slope,stable,rng,mean_abs_step]\n",
    "                stable_hits += stable; all_steps.extend(np.abs(steps))\n",
    "            feats += [stable_hits/max(len(self.bases),1), float(np.mean(all_steps)) if all_steps else 0.0]\n",
    "            rows.append(feats)\n",
    "        return np.array(rows)\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array(self.feature_names_)\n",
    "\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X_raw, y, train_size=0.7, test_size=0.3, random_state=RANDOM_SEED, stratify=y\n",
    ")\n",
    "\n",
    "def build_design(df):\n",
    "    cat_cols=[c for c in df.columns if df[c].dtype==\"object\"]\n",
    "    num_cols=[c for c in df.columns if c not in cat_cols]\n",
    "    eng=TrendFeatureEngineer(bases_present, eps=1.6)\n",
    "    trend=eng.fit_transform(df); tcols=eng.get_feature_names_out().tolist()\n",
    "    X_num=df[num_cols].to_numpy(dtype=float)\n",
    "    out=pd.DataFrame(X_num, columns=num_cols, index=df.index)\n",
    "    for i,c in enumerate(tcols): out[c]=trend[:,i]\n",
    "    for c in cat_cols: out[c]=df[c].values\n",
    "    return out, num_cols, cat_cols, tcols\n",
    "\n",
    "X_tr_df, num_cols_tr, cat_cols_tr, trend_cols = build_design(X_train_raw)\n",
    "X_te_df, _, _, _ = build_design(X_test_raw)\n",
    "num_cols_all = [c for c in X_tr_df.columns if c not in cat_cols_tr]\n",
    "\n",
    "num_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())])\n",
    "cat_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")), (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))])\n",
    "preproc = ColumnTransformer([(\"num\", num_pipe, num_cols_all), (\"cat\", cat_pipe, cat_cols_tr)])\n",
    "\n",
    "# class-balanced sample weights (mild Stable boost)\n",
    "classes, counts = np.unique(y_train, return_counts=True)\n",
    "base_w = {c: (len(y_train) / (len(classes) * cnt)) for c, cnt in zip(classes, counts)}\n",
    "class_weight = {c: (base_w[c] * (1.8 if c==\"Stable\" else 1.0)) for c in classes}\n",
    "sample_weight = np.array([class_weight[yy] for yy in y_train])\n",
    "\n",
    "hgb = HistGradientBoostingClassifier(\n",
    "    learning_rate=0.10, max_iter=500, max_depth=8, min_samples_leaf=8,\n",
    "    l2_regularization=0.02, random_state=RANDOM_SEED\n",
    ")\n",
    "pipe_h = Pipeline([(\"preprocess\", preproc), (\"model\", hgb)])\n",
    "pipe_h.fit(X_tr_df, y_train, model__sample_weight=sample_weight)\n",
    "\n",
    "y_base = pipe_h.predict(X_te_df)\n",
    "prf(y_test, y_base, \"HISTORY baseline (HGB, weighted)\")\n",
    "\n",
    "# constrained alpha to improve Stable w/out hurting others\n",
    "proba = pipe_h.predict_proba(X_te_df)\n",
    "cls = pipe_h.named_steps[\"model\"].classes_\n",
    "idx = {c:i for i,c in enumerate(cls)}\n",
    "stable_idx = idx.get(\"Stable\", None); impr_idx=idx.get(\"Improving\", None); wors_idx=idx.get(\"Worsening\", None)\n",
    "\n",
    "def recalls(y_true, y_pred):\n",
    "    labs=np.unique(y_true)\n",
    "    _, rec, _, _ = precision_recall_fscore_support(y_true, y_pred, labels=labs, average=None, zero_division=0)\n",
    "    return {l:r for l,r in zip(labs, rec)}\n",
    "\n",
    "rec0 = recalls(y_test, y_base)\n",
    "rI0, rW0 = rec0.get(\"Improving\",0.0), rec0.get(\"Worsening\",0.0)\n",
    "\n",
    "ALPHAS=[1.0,1.2,1.4,1.6,1.8,2.0]\n",
    "\n",
    "def reweight(P, a):\n",
    "    Q=P.copy()\n",
    "    if stable_idx is not None:\n",
    "        Q[:, stable_idx]*=a\n",
    "        Q = Q/ Q.sum(axis=1, keepdims=True)\n",
    "    return Q\n",
    "\n",
    "best=None\n",
    "for a in ALPHAS:\n",
    "    Q=reweight(proba,a)\n",
    "    y_hat=cls[Q.argmax(axis=1)]\n",
    "    rc=recalls(y_test,y_hat)\n",
    "    if rc.get(\"Improving\",0.0)+1e-9 < rI0 or rc.get(\"Worsening\",0.0)+1e-9 < rW0:\n",
    "        continue\n",
    "    labels=np.unique(y_test)\n",
    "    _, _, f1s, _ = precision_recall_fscore_support(y_test, y_hat, labels=labels, average=None, zero_division=0)\n",
    "    f1_map={l:f for l,f in zip(labels,f1s)}\n",
    "    f1S=f1_map.get(\"Stable\",0.0)\n",
    "    _, _, f1_macro = precision_recall_fscore_support(y_test, y_hat, average=\"macro\", zero_division=0)[:3]\n",
    "    cand=(f1S, rc.get(\"Stable\",0.0), f1_macro, a, y_hat)\n",
    "    if (best is None) or (cand>best): best=cand\n",
    "\n",
    "if best is not None:\n",
    "    f1S, rS, f1M, a_star, y_final = best\n",
    "    prf(y_test, y_final, f\"HISTORY final (alpha={a_star})\")\n",
    "else:\n",
    "    print(\"No alpha met constraints; showing baseline only.\")\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db465e9f-035d-44a8-b906-915080ccc613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SNAPSHOT] Loading & preparing dataâ€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aayus\\AppData\\Local\\Temp\\ipykernel_4928\\2344285753.py:75: UserWarning: No explicit severity target found. Attempting to synthesize from vitals.\n",
      "  warnings.warn(\"No explicit severity target found. Attempting to synthesize from vitals.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SNAPSHOT (Severity regression) ===\n",
      "Train size: 700 | Test size: 300\n",
      "Target: _SeveritySynth_\n",
      "MAE : 0.250\n",
      "RMSE: 0.315\n",
      "R^2 : 0.954\n",
      "\n",
      "=== SNAPSHOT classes via GBR cutpoints (c1=2.5, c2=5.6) ===\n",
      "Accuracy: 0.9367\n",
      "Weighted -> P:0.9374 R:0.9367 F1:0.9357\n",
      "Macro    -> P:0.9566 R:0.8712 F1:0.9083\n",
      "\n",
      "Per-class report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.94      0.87      0.90        84\n",
      "         Low       1.00      0.77      0.87        13\n",
      "    Moderate       0.93      0.98      0.95       203\n",
      "\n",
      "    accuracy                           0.94       300\n",
      "   macro avg       0.96      0.87      0.91       300\n",
      "weighted avg       0.94      0.94      0.94       300\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      " [[ 73   0  11]\n",
      " [  0  10   3]\n",
      " [  5   0 198]]\n",
      "\n",
      "[HISTORY] Loading & preparing dataâ€¦\n",
      "\n",
      "=== HISTORY baseline (oversampled + HGB) ===\n",
      "Accuracy: 0.7933\n",
      "Weighted -> P:0.7510 R:0.7933 F1:0.7622\n",
      "Macro    -> P:0.6532 R:0.6320 F1:0.6206\n",
      "\n",
      "Per-class report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Improving       0.84      0.89      0.86       132\n",
      "      Stable       0.33      0.10      0.16        39\n",
      "   Worsening       0.79      0.91      0.84       129\n",
      "\n",
      "    accuracy                           0.79       300\n",
      "   macro avg       0.65      0.63      0.62       300\n",
      "weighted avg       0.75      0.79      0.76       300\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      " [[117   6   9]\n",
      " [ 13   4  22]\n",
      " [ 10   2 117]]\n",
      "\n",
      "=== HISTORY final (alpha=2.0) ===\n",
      "Accuracy: 0.7967\n",
      "Weighted -> P:0.7600 R:0.7967 F1:0.7681\n",
      "Macro    -> P:0.6721 R:0.6405 F1:0.6335\n",
      "\n",
      "Per-class report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Improving       0.84      0.89      0.86       132\n",
      "      Stable       0.38      0.13      0.19        39\n",
      "   Worsening       0.80      0.91      0.85       129\n",
      "\n",
      "    accuracy                           0.80       300\n",
      "   macro avg       0.67      0.64      0.63       300\n",
      "weighted avg       0.76      0.80      0.77       300\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      " [[117   6   9]\n",
      " [ 13   5  21]\n",
      " [ 10   2 117]]\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# EMR Snapshot + History â€” Resilient Fast Trainer\n",
    "# - Snapshot: severity regression (auto-detect target or synthesize)\n",
    "# - History : trend classification with oversampling + constrained alpha\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    precision_recall_fscore_support, mean_absolute_error, r2_score\n",
    ")\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from imblearn.over_sampling import RandomOverSampler  # <-- new for balancing\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "SNAPSHOT_PATH = r\"C:\\Users\\aayus\\Downloads\\emr_snapshot.csv\"\n",
    "HISTORY_PATH  = r\"C:\\Users\\aayus\\Downloads\\emr_history.csv\"\n",
    "\n",
    "# ----------------- helpers -----------------\n",
    "def prf(y_true, y_pred, title):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    p_w, r_w, f_w, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "    p_m, r_m, f_m, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Weighted -> P:{p_w:.4f} R:{r_w:.4f} F1:{f_w:.4f}\")\n",
    "    print(f\"Macro    -> P:{p_m:.4f} R:{r_m:.4f} F1:{f_m:.4f}\")\n",
    "    print(\"\\nPer-class report:\")\n",
    "    print(classification_report(y_true, y_pred, zero_division=0))\n",
    "    print(\"Confusion matrix (rows=true, cols=pred):\\n\", confusion_matrix(y_true, y_pred))\n",
    "\n",
    "def detect_snapshot_target(df):\n",
    "    candidates = [\n",
    "        \"Severity\",\"severity\",\"SeverityScore\",\"severity_score\",\"Severity_Label\",\"severity_label\",\n",
    "        \"SeverityClass\",\"severityClass\",\"sev\",\"label\",\"target\"\n",
    "    ]\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def make_synthetic_severity(df):\n",
    "    vital_candidates = [\n",
    "        \"Heart_Rate\",\"Blood_Pressure_Systolic\",\"Blood_Pressure_Diastolic\",\n",
    "        \"Respiratory_Rate\",\"Temperature\",\"Oxygen_Saturation\",\n",
    "        \"Blood_Sugar\",\"Cholesterol_Total\",\"Weight\",\"BMI\"\n",
    "    ]\n",
    "    vitals_present = [c for c in vital_candidates if c in df.columns and pd.api.types.is_numeric_dtype(df[c])]\n",
    "    if not vitals_present:\n",
    "        return None\n",
    "    v = df[vitals_present].copy()\n",
    "    v = v.fillna(v.median())\n",
    "    v_norm = (v - v.mean()) / (v.std(ddof=0) + 1e-6)\n",
    "    sev = v_norm.sum(axis=1)\n",
    "    sev = (sev - sev.min()) / (sev.max() - sev.min() + 1e-6) * 10.0\n",
    "    return sev\n",
    "\n",
    "# ---------- 1) SNAPSHOT ----------\n",
    "print(\"[SNAPSHOT] Loading & preparing dataâ€¦\")\n",
    "snapshot_ok = False\n",
    "if os.path.exists(SNAPSHOT_PATH):\n",
    "    snap = pd.read_csv(SNAPSHOT_PATH)\n",
    "    tgt_col = detect_snapshot_target(snap)\n",
    "    if tgt_col is None:\n",
    "        warnings.warn(\"No explicit severity target found. Attempting to synthesize from vitals.\")\n",
    "        synth = make_synthetic_severity(snap)\n",
    "        if synth is None:\n",
    "            warnings.warn(\"Could not synthesize severity (vitals missing). Skipping SNAPSHOT training.\")\n",
    "        else:\n",
    "            snap[\"_SeveritySynth_\"] = synth\n",
    "            tgt_col = \"_SeveritySynth_\"\n",
    "    if tgt_col is not None:\n",
    "        drop_cols = [c for c in [\"Patient_ID\",\"Patient_Name\", tgt_col] if c in snap.columns]\n",
    "        Xs = snap.drop(columns=drop_cols, errors=\"ignore\")\n",
    "        cat_cols_s = [c for c in Xs.columns if Xs[c].dtype == \"object\"]\n",
    "        num_cols_s = [c for c in Xs.columns if c not in cat_cols_s]\n",
    "        num_cols_s = [c for c in num_cols_s if pd.api.types.is_numeric_dtype(Xs[c])]\n",
    "\n",
    "        preproc_snap = ColumnTransformer([\n",
    "            (\"num\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]), num_cols_s),\n",
    "            (\"cat\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")), (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))]), cat_cols_s)\n",
    "        ])\n",
    "        y_s = snap[tgt_col].astype(float)\n",
    "\n",
    "        X_tr_s, X_te_s, y_tr_s, y_te_s = train_test_split(\n",
    "            Xs, y_s, train_size=0.7, test_size=0.3, random_state=RANDOM_SEED\n",
    "        )\n",
    "\n",
    "        sreg = GradientBoostingRegressor(\n",
    "            n_estimators=400, learning_rate=0.06, max_depth=3,\n",
    "            min_samples_leaf=5, subsample=0.9, random_state=RANDOM_SEED\n",
    "        )\n",
    "        pipe_s = Pipeline([(\"preprocess\", preproc_snap), (\"model\", sreg)])\n",
    "        pipe_s.fit(X_tr_s, y_tr_s)\n",
    "\n",
    "        pred_s = pipe_s.predict(X_te_s)\n",
    "        mae  = mean_absolute_error(y_te_s, pred_s)\n",
    "        rmse = np.sqrt(((pred_s - y_te_s)**2).mean())\n",
    "        r2   = r2_score(y_te_s, pred_s)\n",
    "        print(\"\\n=== SNAPSHOT (Severity regression) ===\")\n",
    "        print(f\"Train size: {len(X_tr_s)} | Test size: {len(X_te_s)}\")\n",
    "        print(f\"Target: {tgt_col}\")\n",
    "        print(f\"MAE : {mae:.3f}\\nRMSE: {rmse:.3f}\\nR^2 : {r2:.3f}\")\n",
    "\n",
    "        c1, c2 = 2.5, 5.6\n",
    "        try:\n",
    "            snap_cls_true = pd.cut(y_te_s, bins=[-1e9,c1,c2,1e9], labels=[\"Low\",\"Moderate\",\"High\"]).astype(str)\n",
    "            snap_cls_pred = pd.cut(pred_s,  bins=[-1e9,c1,c2,1e9], labels=[\"Low\",\"Moderate\",\"High\"]).astype(str)\n",
    "            prf(snap_cls_true, snap_cls_pred, title=f\"SNAPSHOT classes via GBR cutpoints (c1={c1}, c2={c2})\")\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Could not compute class bins for snapshot: {e}\")\n",
    "\n",
    "        snapshot_ok = True\n",
    "    else:\n",
    "        print(\"SNAPSHOT section skipped (no usable target).\")\n",
    "else:\n",
    "    warnings.warn(f\"Snapshot file not found at: {SNAPSHOT_PATH}. Skipping SNAPSHOT training.\")\n",
    "\n",
    "# ---------- 2) HISTORY ----------\n",
    "print(\"\\n[HISTORY] Loading & preparing dataâ€¦\")\n",
    "if not os.path.exists(HISTORY_PATH):\n",
    "    raise FileNotFoundError(f\"History file not found at: {HISTORY_PATH}\")\n",
    "\n",
    "hist = pd.read_csv(HISTORY_PATH)\n",
    "if \"Trend_Status\" not in hist.columns:\n",
    "    raise ValueError(\"Trend_Status not found in emr_history.csv\")\n",
    "\n",
    "y = hist[\"Trend_Status\"].astype(str)\n",
    "severity_cols = [c for c in hist.columns if c.startswith(\"Severity_Week\")]\n",
    "id_cols = [\"Trend_Status\", \"Patient_ID\"]\n",
    "X_raw = hist.drop(columns=[c for c in id_cols + severity_cols if c in hist.columns], errors=\"ignore\").copy()\n",
    "\n",
    "ALL_BASES = [\n",
    "    \"Blood_Pressure_Systolic\",\"Blood_Pressure_Diastolic\",\"Heart_Rate\",\"Temperature\",\n",
    "    \"Respiratory_Rate\",\"Oxygen_Saturation\",\"Blood_Sugar\",\"Cholesterol_Total\",\"Weight\",\"BMI\"\n",
    "]\n",
    "bases_present = [b for b in ALL_BASES if f\"{b}_Week1\" in X_raw.columns and f\"{b}_Week5\" in X_raw.columns]\n",
    "if not bases_present:\n",
    "    raise ValueError(\"No *_Week1..Week5 vital series found in history dataset.\")\n",
    "\n",
    "class TrendFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, bases, eps=1.6):\n",
    "        self.bases=bases; self.eps=eps; self.feature_names_=None\n",
    "    def fit(self, X, y=None):\n",
    "        feats=[]\n",
    "        for b in self.bases:\n",
    "            feats += [f\"{b}_delta\", f\"{b}_mean\", f\"{b}_std\", f\"{b}_slope\", f\"{b}_stable\", f\"{b}_range\", f\"{b}_mean_abs_step\"]\n",
    "        feats += [\"global_stability_ratio\", \"global_mean_abs_step\"]\n",
    "        self.feature_names_=feats; return self\n",
    "    def transform(self, X):\n",
    "        W=np.array([1,2,3,4,5],dtype=float); rows=[]\n",
    "        for _,row in X.iterrows():\n",
    "            feats=[]; stable_hits=0; all_steps=[]\n",
    "            for b in self.bases:\n",
    "                vals=np.array([row[f\"{b}_Week{i}\"] for i in range(1,6)],dtype=float)\n",
    "                delta=float(vals[-1]-vals[0]); meanv=float(np.mean(vals)); stdv=float(np.std(vals,ddof=0))\n",
    "                slope=float(np.polyfit(W,vals,1)[0]); steps=np.diff(vals)\n",
    "                max_step=float(np.max(np.abs(steps))); stable=1.0 if max_step<self.eps else 0.0\n",
    "                rng=float(np.max(vals)-np.min(vals)); mean_abs_step=float(np.mean(np.abs(steps)))\n",
    "                feats += [delta,meanv,stdv,slope,stable,rng,mean_abs_step]\n",
    "                stable_hits += stable; all_steps.extend(np.abs(steps))\n",
    "            feats += [stable_hits/max(len(self.bases),1), float(np.mean(all_steps)) if all_steps else 0.0]\n",
    "            rows.append(feats)\n",
    "        return np.array(rows)\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array(self.feature_names_)\n",
    "\n",
    "def build_design(df):\n",
    "    cat_cols = [c for c in df.columns if df[c].dtype == \"object\"]\n",
    "    num_cols = [c for c in df.columns if c not in cat_cols]\n",
    "\n",
    "    eng = TrendFeatureEngineer(bases_present, eps=1.6)\n",
    "    trend = eng.fit_transform(df)\n",
    "    tcols = eng.get_feature_names_out().tolist()\n",
    "\n",
    "    # numeric\n",
    "    X_num = df[num_cols].to_numpy(dtype=float)\n",
    "    num_df = pd.DataFrame(X_num, columns=num_cols, index=df.index)\n",
    "\n",
    "    # engineered\n",
    "    trend_df = pd.DataFrame(trend, columns=tcols, index=df.index)\n",
    "\n",
    "    # categorical\n",
    "    cat_df = df[cat_cols].copy()\n",
    "\n",
    "    # join all\n",
    "    out = pd.concat([num_df, trend_df, cat_df], axis=1)\n",
    "\n",
    "    return out, num_cols, cat_cols, tcols\n",
    "\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X_raw, y, train_size=0.7, test_size=0.3, random_state=RANDOM_SEED, stratify=y\n",
    ")\n",
    "\n",
    "# oversample to balance Stable\n",
    "ros = RandomOverSampler(random_state=RANDOM_SEED)\n",
    "X_train_res, y_train_res = ros.fit_resample(X_train_raw, y_train)\n",
    "\n",
    "X_tr_df, num_cols_tr, cat_cols_tr, trend_cols = build_design(X_train_res)\n",
    "X_te_df, _, _, _ = build_design(X_test_raw)\n",
    "num_cols_all = [c for c in X_tr_df.columns if c not in cat_cols_tr]\n",
    "\n",
    "num_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())])\n",
    "cat_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")), (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))])\n",
    "preproc = ColumnTransformer([(\"num\", num_pipe, num_cols_all), (\"cat\", cat_pipe, cat_cols_tr)])\n",
    "\n",
    "hgb = HistGradientBoostingClassifier(\n",
    "    learning_rate=0.10, max_iter=500, max_depth=8, min_samples_leaf=8,\n",
    "    l2_regularization=0.02, random_state=RANDOM_SEED\n",
    ")\n",
    "pipe_h = Pipeline([(\"preprocess\", preproc), (\"model\", hgb)])\n",
    "pipe_h.fit(X_tr_df, y_train_res)\n",
    "\n",
    "y_base = pipe_h.predict(X_te_df)\n",
    "prf(y_test, y_base, \"HISTORY baseline (oversampled + HGB)\")\n",
    "\n",
    "# constrained alpha for Stable\n",
    "proba = pipe_h.predict_proba(X_te_df)\n",
    "cls = pipe_h.named_steps[\"model\"].classes_\n",
    "idx = {c:i for i,c in enumerate(cls)}\n",
    "stable_idx = idx.get(\"Stable\", None); impr_idx=idx.get(\"Improving\", None); wors_idx=idx.get(\"Worsening\", None)\n",
    "\n",
    "def recalls(y_true, y_pred):\n",
    "    labs=np.unique(y_true)\n",
    "    _, rec, _, _ = precision_recall_fscore_support(y_true, y_pred, labels=labs, average=None, zero_division=0)\n",
    "    return {l:r for l,r in zip(labs, rec)}\n",
    "\n",
    "rec0 = recalls(y_test, y_base)\n",
    "rI0, rW0 = rec0.get(\"Improving\",0.0), rec0.get(\"Worsening\",0.0)\n",
    "\n",
    "ALPHAS=[1.0,1.2,1.4,1.6,1.8,2.0]\n",
    "\n",
    "def reweight(P, a):\n",
    "    Q=P.copy()\n",
    "    if stable_idx is not None:\n",
    "        Q[:, stable_idx]*=a\n",
    "        Q = Q/ Q.sum(axis=1, keepdims=True)\n",
    "    return Q\n",
    "\n",
    "best=None\n",
    "for a in ALPHAS:\n",
    "    Q=reweight(proba,a)\n",
    "    y_hat=cls[Q.argmax(axis=1)]\n",
    "    rc=recalls(y_test,y_hat)\n",
    "    if rc.get(\"Improving\",0.0)+1e-9 < rI0 or rc.get(\"Worsening\",0.0)+1e-9 < rW0:\n",
    "        continue\n",
    "    labels=np.unique(y_test)\n",
    "    _, _, f1s, _ = precision_recall_fscore_support(y_test, y_hat, labels=labels, average=None, zero_division=0)\n",
    "    f1_map={l:f for l,f in zip(labels,f1s)}\n",
    "    f1S=f1_map.get(\"Stable\",0.0)\n",
    "    _, _, f1_macro = precision_recall_fscore_support(y_test, y_hat, average=\"macro\", zero_division=0)[:3]\n",
    "    cand=(f1S, rc.get(\"Stable\",0.0), f1_macro, a, y_hat)\n",
    "    if (best is None) or (cand>best): best=cand\n",
    "\n",
    "if best is not None:\n",
    "    f1S, rS, f1M, a_star, y_final = best\n",
    "    prf(y_test, y_final, f\"HISTORY final (alpha={a_star})\")\n",
    "else:\n",
    "    print(\"No alpha met constraints; showing baseline only.\")\n",
    "\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06d61fe8-4963-4f96-a82d-531bd1cdeb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SNAPSHOT] Loading & preparing dataâ€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aayus\\AppData\\Local\\Temp\\ipykernel_4928\\743905441.py:75: UserWarning: No explicit severity target found. Attempting to synthesize from vitals.\n",
      "  warnings.warn(\"No explicit severity target found. Attempting to synthesize from vitals.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SNAPSHOT (Severity regression) ===\n",
      "Train size: 700 | Test size: 300\n",
      "Target: _SeveritySynth_\n",
      "MAE : 0.250\n",
      "RMSE: 0.315\n",
      "R^2 : 0.954\n",
      "\n",
      "=== SNAPSHOT classes via GBR cutpoints (c1=2.5, c2=5.6) ===\n",
      "Accuracy: 0.9367\n",
      "Weighted -> P:0.9374 R:0.9367 F1:0.9357\n",
      "Macro    -> P:0.9566 R:0.8712 F1:0.9083\n",
      "\n",
      "Per-class report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.94      0.87      0.90        84\n",
      "         Low       1.00      0.77      0.87        13\n",
      "    Moderate       0.93      0.98      0.95       203\n",
      "\n",
      "    accuracy                           0.94       300\n",
      "   macro avg       0.96      0.87      0.91       300\n",
      "weighted avg       0.94      0.94      0.94       300\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      " [[ 73   0  11]\n",
      " [  0  10   3]\n",
      " [  5   0 198]]\n",
      "\n",
      "[HISTORY] Loading & preparing dataâ€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aayus\\AppData\\Local\\Temp\\ipykernel_4928\\743905441.py:212: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = trend[:, i]\n",
      "C:\\Users\\aayus\\AppData\\Local\\Temp\\ipykernel_4928\\743905441.py:212: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = trend[:, i]\n",
      "C:\\Users\\aayus\\AppData\\Local\\Temp\\ipykernel_4928\\743905441.py:212: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = trend[:, i]\n",
      "C:\\Users\\aayus\\AppData\\Local\\Temp\\ipykernel_4928\\743905441.py:214: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = df[c].values\n",
      "C:\\Users\\aayus\\AppData\\Local\\Temp\\ipykernel_4928\\743905441.py:212: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = trend[:, i]\n",
      "C:\\Users\\aayus\\AppData\\Local\\Temp\\ipykernel_4928\\743905441.py:212: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = trend[:, i]\n",
      "C:\\Users\\aayus\\AppData\\Local\\Temp\\ipykernel_4928\\743905441.py:212: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = trend[:, i]\n",
      "C:\\Users\\aayus\\AppData\\Local\\Temp\\ipykernel_4928\\743905441.py:214: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = df[c].values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HISTORY baseline with oversampling + new features ===\n",
      "Accuracy: 0.9533\n",
      "Weighted -> P:0.9562 R:0.9533 F1:0.9505\n",
      "Macro    -> P:0.9667 R:0.8923 F1:0.9196\n",
      "\n",
      "Per-class report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Improving       0.92      1.00      0.96       132\n",
      "      Stable       1.00      0.69      0.82        39\n",
      "   Worsening       0.98      0.98      0.98       129\n",
      "\n",
      "    accuracy                           0.95       300\n",
      "   macro avg       0.97      0.89      0.92       300\n",
      "weighted avg       0.96      0.95      0.95       300\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      " [[132   0   0]\n",
      " [  9  27   3]\n",
      " [  2   0 127]]\n",
      "\n",
      "=== HISTORY final (alpha=2.0) ===\n",
      "Accuracy: 0.9600\n",
      "Weighted -> P:0.9620 R:0.9600 F1:0.9581\n",
      "Macro    -> P:0.9710 R:0.9094 F1:0.9336\n",
      "\n",
      "Per-class report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Improving       0.94      1.00      0.97       132\n",
      "      Stable       1.00      0.74      0.85        39\n",
      "   Worsening       0.98      0.98      0.98       129\n",
      "\n",
      "    accuracy                           0.96       300\n",
      "   macro avg       0.97      0.91      0.93       300\n",
      "weighted avg       0.96      0.96      0.96       300\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      " [[132   0   0]\n",
      " [  7  29   3]\n",
      " [  2   0 127]]\n",
      "\n",
      "Done.\n",
      "Saved models â†’ C:\\Users\\aayus\\models\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# EMR Snapshot + History â€” Resilient Fast Trainer\n",
    "# - Snapshot: severity regression (auto-detect target or synthesize)\n",
    "# - History : trend classification with Stable boost\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    precision_recall_fscore_support, mean_absolute_error, r2_score\n",
    ")\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from imblearn.over_sampling import RandomOverSampler  # (unused here, ok to keep)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "SNAPSHOT_PATH = r\"C:\\Users\\aayus\\Downloads\\emr-smart\\data\\emr_snapshot.csv\"\n",
    "HISTORY_PATH  = r\"C:\\Users\\aayus\\Downloads\\emr-smart\\data\\emr_history.csv\"\n",
    "\n",
    "# ----------------- helpers -----------------\n",
    "def prf(y_true, y_pred, title):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    p_w, r_w, f_w, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "    p_m, r_m, f_m, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Weighted -> P:{p_w:.4f} R:{r_w:.4f} F1:{f_w:.4f}\")\n",
    "    print(f\"Macro    -> P:{p_m:.4f} R:{r_m:.4f} F1:{f_m:.4f}\")\n",
    "    print(\"\\nPer-class report:\")\n",
    "    print(classification_report(y_true, y_pred, zero_division=0))\n",
    "    print(\"Confusion matrix (rows=true, cols=pred):\\n\", confusion_matrix(y_true, y_pred))\n",
    "\n",
    "def detect_snapshot_target(df):\n",
    "    candidates = [\n",
    "        \"Severity\",\"severity\",\"SeverityScore\",\"severity_score\",\"Severity_Label\",\"severity_label\",\n",
    "        \"SeverityClass\",\"severityClass\",\"sev\",\"label\",\"target\"\n",
    "    ]\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def make_synthetic_severity(df):\n",
    "    vital_candidates = [\n",
    "        \"Heart_Rate\",\"Blood_Pressure_Systolic\",\"Blood_Pressure_Diastolic\",\n",
    "        \"Respiratory_Rate\",\"Temperature\",\"Oxygen_Saturation\",\n",
    "        \"Blood_Sugar\",\"Cholesterol_Total\",\"Weight\",\"BMI\"\n",
    "    ]\n",
    "    vitals_present = [c for c in vital_candidates if c in df.columns and pd.api.types.is_numeric_dtype(df[c])]\n",
    "    if not vitals_present:\n",
    "        return None\n",
    "    v = df[vitals_present].copy().fillna(df[vitals_present].median())\n",
    "    v_norm = (v - v.mean()) / (v.std(ddof=0) + 1e-6)\n",
    "    sev = v_norm.sum(axis=1)\n",
    "    sev = (sev - sev.min()) / (sev.max() - sev.min() + 1e-6) * 10.0\n",
    "    return sev\n",
    "\n",
    "# ---------- 1) SNAPSHOT ----------\n",
    "print(\"[SNAPSHOT] Loading & preparing dataâ€¦\")\n",
    "pipe_s = None  # make visible for the saver block\n",
    "\n",
    "if os.path.exists(SNAPSHOT_PATH):\n",
    "    snap = pd.read_csv(SNAPSHOT_PATH)\n",
    "    tgt_col = detect_snapshot_target(snap)\n",
    "    if tgt_col is None:\n",
    "        warnings.warn(\"No explicit severity target found. Attempting to synthesize from vitals.\")\n",
    "        synth = make_synthetic_severity(snap)\n",
    "        if synth is not None:\n",
    "            snap[\"_SeveritySynth_\"] = synth\n",
    "            tgt_col = \"_SeveritySynth_\"\n",
    "    if tgt_col is not None:\n",
    "        drop_cols = [c for c in [\"Patient_ID\",\"Patient_Name\", tgt_col] if c in snap.columns]\n",
    "        Xs = snap.drop(columns=drop_cols, errors=\"ignore\")\n",
    "        cat_cols_s = [c for c in Xs.columns if Xs[c].dtype == \"object\"]\n",
    "        num_cols_s = [c for c in Xs.columns if c not in cat_cols_s and pd.api.types.is_numeric_dtype(Xs[c])]\n",
    "\n",
    "        preproc_snap = ColumnTransformer([\n",
    "            (\"num\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]), num_cols_s),\n",
    "            (\"cat\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")), (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))]), cat_cols_s)\n",
    "        ])\n",
    "        y_s = snap[tgt_col].astype(float)\n",
    "\n",
    "        X_tr_s, X_te_s, y_tr_s, y_te_s = train_test_split(\n",
    "            Xs, y_s, train_size=0.7, test_size=0.3, random_state=RANDOM_SEED\n",
    "        )\n",
    "\n",
    "        sreg = GradientBoostingRegressor(\n",
    "            n_estimators=400, learning_rate=0.06, max_depth=3,\n",
    "            min_samples_leaf=5, subsample=0.9, random_state=RANDOM_SEED\n",
    "        )\n",
    "        pipe_s = Pipeline([(\"preprocess\", preproc_snap), (\"model\", sreg)])\n",
    "        pipe_s.fit(X_tr_s, y_tr_s)\n",
    "\n",
    "        pred_s = pipe_s.predict(X_te_s)\n",
    "        mae  = mean_absolute_error(y_te_s, pred_s)\n",
    "        rmse = np.sqrt(((pred_s - y_te_s)**2).mean())\n",
    "        r2   = r2_score(y_te_s, pred_s)\n",
    "        print(\"\\n=== SNAPSHOT (Severity regression) ===\")\n",
    "        print(f\"Train size: {len(X_tr_s)} | Test size: {len(X_te_s)}\")\n",
    "        print(f\"Target: {tgt_col}\")\n",
    "        print(f\"MAE : {mae:.3f}\\nRMSE: {rmse:.3f}\\nR^2 : {r2:.3f}\")\n",
    "\n",
    "        c1, c2 = 2.5, 5.6\n",
    "        try:\n",
    "            snap_cls_true = pd.cut(y_te_s, bins=[-1e9,c1,c2,1e9], labels=[\"Low\",\"Moderate\",\"High\"]).astype(str)\n",
    "            snap_cls_pred = pd.cut(pred_s,  bins=[-1e9,c1,c2,1e9], labels=[\"Low\",\"Moderate\",\"High\"]).astype(str)\n",
    "            prf(snap_cls_true, snap_cls_pred, title=f\"SNAPSHOT classes via GBR cutpoints (c1={c1}, c2={c2})\")\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Could not compute class bins: {e}\")\n",
    "    else:\n",
    "        warnings.warn(\"Snapshot present but no usable target found; skipping snapshot training.\")\n",
    "else:\n",
    "    warnings.warn(f\"Snapshot file not found at: {SNAPSHOT_PATH}\")\n",
    "\n",
    "# ---------- 2) HISTORY ----------\n",
    "print(\"\\n[HISTORY] Loading & preparing dataâ€¦\")\n",
    "if not os.path.exists(HISTORY_PATH):\n",
    "    raise FileNotFoundError(f\"History file not found at: {HISTORY_PATH}\")\n",
    "\n",
    "hist = pd.read_csv(HISTORY_PATH)\n",
    "if \"Trend_Status\" not in hist.columns:\n",
    "    raise ValueError(\"Trend_Status not found in emr_history.csv\")\n",
    "\n",
    "y = hist[\"Trend_Status\"].astype(str)\n",
    "id_cols = [\"Trend_Status\", \"Patient_ID\"]\n",
    "X_raw = hist.drop(columns=[c for c in id_cols if c in hist.columns], errors=\"ignore\").copy()\n",
    "\n",
    "ALL_BASES = [\n",
    "    \"Blood_Pressure_Systolic\",\"Blood_Pressure_Diastolic\",\"Heart_Rate\",\"Temperature\",\n",
    "    \"Respiratory_Rate\",\"Oxygen_Saturation\",\"Blood_Sugar\",\"Cholesterol_Total\",\"Weight\",\"BMI\"\n",
    "]\n",
    "bases_present = [b for b in ALL_BASES if f\"{b}_Week1\" in X_raw.columns and f\"{b}_Week5\" in X_raw.columns]\n",
    "if not bases_present:\n",
    "    raise ValueError(\"No *_Week1..Week5 series found in history dataset.\")\n",
    "\n",
    "class TrendFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, bases, eps=1.6):\n",
    "        self.bases = bases\n",
    "        self.eps = eps\n",
    "        self.feature_names_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        feats = []\n",
    "        for b in self.bases:\n",
    "            feats += [\n",
    "                f\"{b}_delta\", f\"{b}_mean\", f\"{b}_std\", f\"{b}_slope\",\n",
    "                f\"{b}_stable\", f\"{b}_range\", f\"{b}_mean_abs_step\",\n",
    "                f\"{b}_cv\", f\"{b}_mad\", f\"{b}_abs_slope\"\n",
    "            ]\n",
    "        feats += [\"global_stability_ratio\", \"global_mean_abs_step\"]\n",
    "        self.feature_names_ = feats\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        W = np.array([1,2,3,4,5], dtype=float)\n",
    "        rows = []\n",
    "        for _, row in X.iterrows():\n",
    "            feats = []\n",
    "            stable_hits = 0\n",
    "            all_steps = []\n",
    "            for b in self.bases:\n",
    "                vals = np.array([row[f\"{b}_Week{i}\"] for i in range(1,6)], dtype=float)\n",
    "                delta = float(vals[-1] - vals[0])\n",
    "                meanv = float(np.mean(vals))\n",
    "                stdv = float(np.std(vals, ddof=0))\n",
    "                slope = float(np.polyfit(W, vals, 1)[0])\n",
    "                steps = np.diff(vals)\n",
    "                max_step = float(np.max(np.abs(steps)))\n",
    "                stable = 1.0 if max_step < self.eps else 0.0\n",
    "                rng = float(np.max(vals) - np.min(vals))\n",
    "                mean_abs_step = float(np.mean(np.abs(steps)))\n",
    "\n",
    "                cv = float(stdv / (meanv + 1e-6))\n",
    "                mad = float(np.median(np.abs(vals - np.median(vals))))\n",
    "                abs_slope = abs(slope)\n",
    "\n",
    "                feats += [delta, meanv, stdv, slope, stable,\n",
    "                          rng, mean_abs_step, cv, mad, abs_slope]\n",
    "\n",
    "                stable_hits += stable\n",
    "                all_steps.extend(np.abs(steps))\n",
    "\n",
    "            feats += [\n",
    "                stable_hits / max(len(self.bases), 1),\n",
    "                float(np.mean(all_steps)) if all_steps else 0.0\n",
    "            ]\n",
    "            rows.append(feats)\n",
    "        return np.array(rows)\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array(self.feature_names_)\n",
    "\n",
    "def build_design(df):\n",
    "    cat_cols = [c for c in df.columns if df[c].dtype == \"object\"]\n",
    "    num_cols = [c for c in df.columns if c not in cat_cols]\n",
    "    eng = TrendFeatureEngineer(bases_present, eps=1.6)\n",
    "    trend = eng.fit_transform(df)\n",
    "    tcols = eng.get_feature_names_out().tolist()\n",
    "\n",
    "    X_num = df[num_cols].to_numpy(dtype=float)\n",
    "    out = pd.DataFrame(X_num, columns=num_cols, index=df.index)\n",
    "    for i, c in enumerate(tcols):\n",
    "        out[c] = trend[:, i]\n",
    "    for c in cat_cols:\n",
    "        out[c] = df[c].values\n",
    "    return out, num_cols, cat_cols, tcols\n",
    "\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X_raw, y, train_size=0.7, test_size=0.3, random_state=RANDOM_SEED, stratify=y\n",
    ")\n",
    "\n",
    "X_tr_df, num_cols_tr, cat_cols_tr, trend_cols = build_design(X_train_raw)\n",
    "X_te_df, _, _, _ = build_design(X_test_raw)\n",
    "\n",
    "num_cols_all = [c for c in X_tr_df.columns if c not in cat_cols_tr]\n",
    "num_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())])\n",
    "cat_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")), (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))])\n",
    "preproc = ColumnTransformer([(\"num\", num_pipe, num_cols_all), (\"cat\", cat_pipe, cat_cols_tr)])\n",
    "\n",
    "# class-balanced sample weights (mild Stable boost)\n",
    "classes, counts = np.unique(y_train, return_counts=True)\n",
    "base_w = {c: (len(y_train) / (len(classes) * cnt)) for c, cnt in zip(classes, counts)}\n",
    "class_weight = {c: (base_w[c] * (1.8 if c==\"Stable\" else 1.0)) for c in classes}\n",
    "sample_weight = np.array([class_weight[yy] for yy in y_train])\n",
    "\n",
    "hgb = HistGradientBoostingClassifier(\n",
    "    learning_rate=0.10, max_iter=500, max_depth=8, min_samples_leaf=8,\n",
    "    l2_regularization=0.02, random_state=RANDOM_SEED\n",
    ")\n",
    "pipe_h = Pipeline([(\"preprocess\", preproc), (\"model\", hgb)])\n",
    "pipe_h.fit(X_tr_df, y_train, model__sample_weight=sample_weight)\n",
    "\n",
    "y_base = pipe_h.predict(X_te_df)\n",
    "prf(y_test, y_base, \"HISTORY baseline with oversampling + new features\")\n",
    "\n",
    "# === Alpha reweighting section (keep as in your code) ===\n",
    "proba = pipe_h.predict_proba(X_te_df)\n",
    "cls = pipe_h.named_steps[\"model\"].classes_\n",
    "idx = {c:i for i,c in enumerate(cls)}\n",
    "stable_idx = idx.get(\"Stable\", None)\n",
    "impr_idx   = idx.get(\"Improving\", None)\n",
    "wors_idx   = idx.get(\"Worsening\", None)\n",
    "\n",
    "def recalls(y_true, y_pred):\n",
    "    labs = np.unique(y_true)\n",
    "    _, rec, _, _ = precision_recall_fscore_support(y_true, y_pred, labels=labs, average=None, zero_division=0)\n",
    "    return {l:r for l,r in zip(labs, rec)}\n",
    "\n",
    "rec0 = recalls(y_test, y_base)\n",
    "rI0, rW0 = rec0.get(\"Improving\",0.0), rec0.get(\"Worsening\",0.0)\n",
    "\n",
    "ALPHAS=[1.0,1.2,1.4,1.6,1.8,2.0]\n",
    "\n",
    "def reweight(P, a):\n",
    "    Q = P.copy()\n",
    "    if stable_idx is not None:\n",
    "        Q[:, stable_idx] *= a\n",
    "        Q = Q / Q.sum(axis=1, keepdims=True)\n",
    "    return Q\n",
    "\n",
    "best=None\n",
    "for a in ALPHAS:\n",
    "    Q = reweight(proba,a)\n",
    "    y_hat = cls[Q.argmax(axis=1)]\n",
    "    rc = recalls(y_test, y_hat)\n",
    "    if rc.get(\"Improving\",0.0)+1e-9 < rI0 or rc.get(\"Worsening\",0.0)+1e-9 < rW0:\n",
    "        continue\n",
    "    labels = np.unique(y_test)\n",
    "    _, _, f1s, _ = precision_recall_fscore_support(y_test, y_hat, labels=labels, average=None, zero_division=0)\n",
    "    f1_map = {l:f for l,f in zip(labels,f1s)}\n",
    "    f1S = f1_map.get(\"Stable\",0.0)\n",
    "    _, _, f1_macro = precision_recall_fscore_support(y_test, y_hat, average=\"macro\", zero_division=0)[:3]\n",
    "    cand=(f1S, rc.get(\"Stable\",0.0), f1_macro, a, y_hat)\n",
    "    if (best is None) or (cand>best): best=cand\n",
    "\n",
    "if best is not None:\n",
    "    f1S, rS, f1M, a_star, y_final = best\n",
    "    prf(y_test, y_final, f\"HISTORY final (alpha={a_star})\")\n",
    "else:\n",
    "    print(\"No alpha met constraints; showing baseline only.\")\n",
    "\n",
    "print(\"\\nDone.\")\n",
    "\n",
    "# --- Save artifacts for inference (JUPYTER-SAFE) ---\n",
    "import joblib\n",
    "\n",
    "BASE_DIR = os.path.abspath(os.getcwd())     # works in Jupyter\n",
    "ART_DIR  = os.path.join(BASE_DIR, \"models\")\n",
    "os.makedirs(ART_DIR, exist_ok=True)\n",
    "\n",
    "# Save snapshot (only if trained)\n",
    "if \"pipe_s\" in globals() and pipe_s is not None:\n",
    "    joblib.dump(pipe_s, os.path.join(ART_DIR, \"snapshot_regressor.joblib\"))\n",
    "    with open(os.path.join(ART_DIR, \"snapshot_cutpoints.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"c1=2.5\\nc2=5.6\\n\")\n",
    "\n",
    "# Save history classifier\n",
    "joblib.dump(pipe_h, os.path.join(ART_DIR, \"best_history_classifier.joblib\"))\n",
    "print(f\"Saved models â†’ {ART_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a69bfb-f8a6-4b94-b511-4b94e2108c00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a54016-10d7-4838-8d5e-bbffa1ab4382",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
