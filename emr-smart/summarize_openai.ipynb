{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3433a9a-07fd-44cf-b258-d4157c1c1285",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[0;32m      5\u001b[0m load_dotenv()\n\u001b[0;32m      6\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI(api_key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'openai'"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "SYSTEM = (\n",
    "    \"You are a clinical summarizer for an EMR assistant. \"\n",
    "    \"Given structured patient fusion data, produce ONE concise paragraph (≤150 words) that: \"\n",
    "    \"1) summarizes current status (only abnormal vitals), \"\n",
    "    \"2) describes 5-week trend (Improving/Stable/Worsening + notable deltas), \"\n",
    "    \"3) lists alerts and practical next steps. \"\n",
    "    \"Use calibrated language ('suggests', 'consider'). Do not diagnose or prescribe. No PII beyond patient_id.\"\n",
    ")\n",
    "\n",
    "USER_TEMPLATE = (\n",
    "    \"FUSION JSON:\\n{fusion}\\n\\n\"\n",
    "    \"Write one paragraph. Include:\\n\"\n",
    "    \"- Current status with abnormal vitals only.\\n\"\n",
    "    \"- Trend summary with notable changes.\\n\"\n",
    "    \"- Alerts + suggested next steps (brief).\\n\"\n",
    "    \"End with: 'This supports decisions and is not a diagnosis.'\"\n",
    ")\n",
    "\n",
    "def summarize_fusion(fu: dict, model=\"gpt-4.1-mini\"):\n",
    "    user = USER_TEMPLATE.format(fusion=json.dumps(fu, ensure_ascii=False))\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.2,\n",
    "        messages=[{\"role\":\"system\",\"content\":SYSTEM},\n",
    "                  {\"role\":\"user\",\"content\":user}]\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ndjson_path = os.path.join(os.path.dirname(__file__), \"outputs\", \"fusion.ndjson\")\n",
    "    out_path    = os.path.join(os.path.dirname(__file__), \"outputs\", \"summaries.txt\")\n",
    "    assert os.path.exists(ndjson_path), \"Run predict_and_fuse.py first.\"\n",
    "    with open(ndjson_path, \"r\", encoding=\"utf-8\") as f, open(out_path, \"w\", encoding=\"utf-8\") as w:\n",
    "        for line in f:\n",
    "            fu = json.loads(line)\n",
    "            para = summarize_fusion(fu)\n",
    "            w.write(f\"{fu['patient_id']}: {para}\\n\\n\")\n",
    "    print(f\"Wrote summaries → {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef122629-9dec-48bd-a552-16ecfce84bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting openai\n",
      "  Downloading openai-1.108.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (0.27.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.11.0-cp312-cp312-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\programdata\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading openai-1.108.0-py3-none-any.whl (948 kB)\n",
      "   ---------------------------------------- 0.0/948.1 kB ? eta -:--:--\n",
      "   ----------- ---------------------------- 262.1/948.1 kB ? eta -:--:--\n",
      "   --------------------------------- ------ 786.4/948.1 kB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 948.1/948.1 kB 2.3 MB/s eta 0:00:00\n",
      "Downloading jiter-0.11.0-cp312-cp312-win_amd64.whl (203 kB)\n",
      "Installing collected packages: jiter, openai\n",
      "Successfully installed jiter-0.11.0 openai-1.108.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script openai.exe is installed in 'C:\\Users\\aayus\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!pip install openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d604ae9f-bf84-4c5e-bafb-a9ca8b582ecc",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "OPENAI_API_KEY missing. Create a .env file in your project root with:\nOPENAI_API_KEY=sk-... (your key)\nExpected .env near: C:\\Users\\aayus\\Downloads\\emr-smart",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m api_key \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m api_key\u001b[38;5;241m.\u001b[39mstrip():\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY missing. Create a .env file in your project root with:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY=sk-... (your key)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected .env near: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mROOT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     24\u001b[0m     )\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[0;32m     27\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI(api_key\u001b[38;5;241m=\u001b[39mapi_key)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: OPENAI_API_KEY missing. Create a .env file in your project root with:\nOPENAI_API_KEY=sk-... (your key)\nExpected .env near: C:\\Users\\aayus\\Downloads\\emr-smart"
     ]
    }
   ],
   "source": [
    "# Summarize EMR fusion records with OpenAI (uses .env)\n",
    "# Paths assume your project layout under C:\\Users\\aayus\\Downloads\\emr-smart\n",
    "\n",
    "import os, json, textwrap\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ---------- config ----------\n",
    "ROOT      = Path(r\"C:\\Users\\aayus\\Downloads\\emr-smart\")\n",
    "OUT_DIR   = ROOT / \"outputs\"\n",
    "IN_PATH   = OUT_DIR / \"fusion.ndjson\"\n",
    "OUT_PATH  = OUT_DIR / \"summaries.txt\"\n",
    "MODEL     = \"gpt-4o-mini\"\n",
    "MAX_WORDS = 150  # soft cap; prompt asks for ≤150 words\n",
    "\n",
    "# ---------- env / client ----------\n",
    "load_dotenv()  # loads .env from current working dir or parents\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key or not api_key.strip():\n",
    "    raise RuntimeError(\n",
    "        \"OPENAI_API_KEY missing. Create a .env file in your project root with:\\n\"\n",
    "        \"OPENAI_API_KEY=sk-... (your key)\\n\"\n",
    "        f\"Expected .env near: {ROOT}\"\n",
    "    )\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# ---------- prompts ----------\n",
    "SYSTEM = (\n",
    "    \"You are a clinical summarizer for an EMR assistant. \"\n",
    "    \"Given structured patient fusion data, produce ONE concise paragraph (≤150 words) that: \"\n",
    "    \"1) starts with quick context (patient_id if available), \"\n",
    "    \"2) states current severity and confidence, \"\n",
    "    \"3) describes 5-week trend with probabilities, \"\n",
    "    \"4) surfaces any alerts, and \"\n",
    "    \"5) ends with a short ‘next steps’ line using calibrated language (e.g., ‘consider review/monitoring’). \"\n",
    "    \"Avoid diagnoses/prescriptions and PII beyond patient_id. Neutral tone.\"\n",
    ")\n",
    "\n",
    "def make_user_prompt(rec: dict) -> str:\n",
    "    pid      = rec.get(\"patient_id\", \"unknown\")\n",
    "    snap     = rec.get(\"snapshot\") or {}\n",
    "    hist     = rec.get(\"history\")  or {}\n",
    "    alerts   = rec.get(\"alerts\")   or []\n",
    "\n",
    "    sev_num  = snap.get(\"severity_numeric\", \"\")\n",
    "    sev_lab  = snap.get(\"severity_label\", \"\")\n",
    "    trend    = hist.get(\"trend\", \"\")\n",
    "    proba    = hist.get(\"proba\", {})\n",
    "\n",
    "    # flatten probs in deterministic order\n",
    "    p_imp = proba.get(\"Improving\",  None)\n",
    "    p_sta = proba.get(\"Stable\",     None)\n",
    "    p_wor = proba.get(\"Worsening\",  None)\n",
    "\n",
    "    # compact text the model can use reliably\n",
    "    lines = [\n",
    "        f\"patient_id: {pid}\",\n",
    "        f\"severity_now: {sev_num} ({sev_lab})\",\n",
    "        f\"trend_5w: {trend}\",\n",
    "        f\"proba: Improving={p_imp}, Stable={p_sta}, Worsening={p_wor}\",\n",
    "        f\"alerts: {alerts}\",\n",
    "        f\"limit_words: {MAX_WORDS}\",\n",
    "    ]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# ---------- IO checks ----------\n",
    "if not IN_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing input: {IN_PATH} (run fuse_infer_generate.py first)\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- run ----------\n",
    "written = 0\n",
    "with open(IN_PATH, \"r\", encoding=\"utf-8\") as fin, open(OUT_PATH, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for line in fin:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        rec = json.loads(line)\n",
    "\n",
    "        # Build the prompt\n",
    "        user_prompt = make_user_prompt(rec)\n",
    "\n",
    "        # Call OpenAI\n",
    "        try:\n",
    "            resp = client.chat.completions.create(\n",
    "                model=MODEL,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM},\n",
    "                    {\"role\": \"user\",   \"content\": user_prompt},\n",
    "                ],\n",
    "                temperature=0.2,\n",
    "            )\n",
    "            text = (resp.choices[0].message.content or \"\").strip()\n",
    "            # safety: hard-trim if the model exceeded requested length\n",
    "            if len(text.split()) > MAX_WORDS + 10:\n",
    "                text = \" \".join(text.split()[:MAX_WORDS]) + \"..A.\"\n",
    "        except Exception as e:\n",
    "            text = f\"(generation error: {e})\"\n",
    "\n",
    "        pid = rec.get(\"patient_id\", \"unknown\")\n",
    "        fout.write(f\"[{pid}]\\n{text}\\n\\n\")\n",
    "        written += 1\n",
    "\n",
    "print(f\"Wrote {written} summaries → {OUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95955ba4-1914-400f-9b2c-8d4778305a02",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "OPENAI_API_KEY missing. Create a .env file in your project root with:\nOPENAI_API_KEY=sk-... (your key)\nExpected .env near: C:\\Users\\aayus\\Downloads\\emr-smart",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m api_key \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m api_key\u001b[38;5;241m.\u001b[39mstrip():\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY missing. Create a .env file in your project root with:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY=sk-... (your key)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected .env near: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mROOT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     24\u001b[0m     )\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[0;32m     27\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI(api_key\u001b[38;5;241m=\u001b[39mapi_key)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: OPENAI_API_KEY missing. Create a .env file in your project root with:\nOPENAI_API_KEY=sk-... (your key)\nExpected .env near: C:\\Users\\aayus\\Downloads\\emr-smart"
     ]
    }
   ],
   "source": [
    "# Summarize EMR fusion records with OpenAI (uses .env)\n",
    "# Paths assume your project layout under C:\\Users\\aayus\\Downloads\\emr-smart\n",
    "\n",
    "import os, json, textwrap\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ---------- config ----------\n",
    "ROOT      = Path(r\"C:\\Users\\aayus\\Downloads\\emr-smart\")\n",
    "OUT_DIR   = ROOT / \"outputs\"\n",
    "IN_PATH   = OUT_DIR / \"fusion.ndjson\"\n",
    "OUT_PATH  = OUT_DIR / \"summaries.txt\"\n",
    "MODEL     = \"gpt-4o-mini\"\n",
    "MAX_WORDS = 150  # soft cap; prompt asks for ≤150 words\n",
    "\n",
    "# ---------- env / client ----------\n",
    "load_dotenv()  # loads .env from current working dir or parents\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key or not api_key.strip():\n",
    "    raise RuntimeError(\n",
    "        \"OPENAI_API_KEY missing. Create a .env file in your project root with:\\n\"\n",
    "        \"OPENAI_API_KEY=sk-... (your key)\\n\"\n",
    "        f\"Expected .env near: {ROOT}\"\n",
    "    )\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# ---------- prompts ----------\n",
    "SYSTEM = (\n",
    "    \"You are a clinical summarizer for an EMR assistant. \"\n",
    "    \"Given structured patient fusion data, produce ONE concise paragraph (≤150 words) that: \"\n",
    "    \"1) starts with quick context (patient_id if available), \"\n",
    "    \"2) states current severity and confidence, \"\n",
    "    \"3) describes 5-week trend with probabilities, \"\n",
    "    \"4) surfaces any alerts, and \"\n",
    "    \"5) ends with a short ‘next steps’ line using calibrated language (e.g., ‘consider review/monitoring’). \"\n",
    "    \"Avoid diagnoses/prescriptions and PII beyond patient_id. Neutral tone.\"\n",
    ")\n",
    "\n",
    "def make_user_prompt(rec: dict) -> str:\n",
    "    pid      = rec.get(\"patient_id\", \"unknown\")\n",
    "    snap     = rec.get(\"snapshot\") or {}\n",
    "    hist     = rec.get(\"history\")  or {}\n",
    "    alerts   = rec.get(\"alerts\")   or []\n",
    "\n",
    "    sev_num  = snap.get(\"severity_numeric\", \"\")\n",
    "    sev_lab  = snap.get(\"severity_label\", \"\")\n",
    "    trend    = hist.get(\"trend\", \"\")\n",
    "    proba    = hist.get(\"proba\", {})\n",
    "\n",
    "    # flatten probs in deterministic order\n",
    "    p_imp = proba.get(\"Improving\",  None)\n",
    "    p_sta = proba.get(\"Stable\",     None)\n",
    "    p_wor = proba.get(\"Worsening\",  None)\n",
    "\n",
    "    # compact text the model can use reliably\n",
    "    lines = [\n",
    "        f\"patient_id: {pid}\",\n",
    "        f\"severity_now: {sev_num} ({sev_lab})\",\n",
    "        f\"trend_5w: {trend}\",\n",
    "        f\"proba: Improving={p_imp}, Stable={p_sta}, Worsening={p_wor}\",\n",
    "        f\"alerts: {alerts}\",\n",
    "        f\"limit_words: {MAX_WORDS}\",\n",
    "    ]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# ---------- IO checks ----------\n",
    "if not IN_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing input: {IN_PATH} (run fuse_infer_generate.py first)\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- run ----------\n",
    "written = 0\n",
    "with open(IN_PATH, \"r\", encoding=\"utf-8\") as fin, open(OUT_PATH, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for line in fin:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        rec = json.loads(line)\n",
    "\n",
    "        # Build the prompt\n",
    "        user_prompt = make_user_prompt(rec)\n",
    "\n",
    "        # Call OpenAI\n",
    "        try:\n",
    "            resp = client.chat.completions.create(\n",
    "                model=MODEL,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM},\n",
    "                    {\"role\": \"user\",   \"content\": user_prompt},\n",
    "                ],\n",
    "                temperature=0.2,\n",
    "            )\n",
    "            text = (resp.choices[0].message.content or \"\").strip()\n",
    "            # safety: hard-trim if the model exceeded requested length\n",
    "            if len(text.split()) > MAX_WORDS + 10:\n",
    "                text = \" \".join(text.split()[:MAX_WORDS]) + \"..A.\"\n",
    "        except Exception as e:\n",
    "            text = f\"(generation error: {e})\"\n",
    "\n",
    "        pid = rec.get(\"patient_id\", \"unknown\")\n",
    "        fout.write(f\"[{pid}]\\n{text}\\n\\n\")\n",
    "        written += 1\n",
    "\n",
    "print(f\"Wrote {written} summaries → {OUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8349f488-1e02-49b5-ba49-d2a89b48bfea",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xff in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ENV_PATH\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.env not found at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mENV_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCreate it with a single line:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOPENAI_API_KEY=sk-...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m load_dotenv(dotenv_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(ENV_PATH))\n\u001b[0;32m     15\u001b[0m api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m api_key \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m api_key\u001b[38;5;241m.\u001b[39mstrip():\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\dotenv\\main.py:342\u001b[0m, in \u001b[0;36mload_dotenv\u001b[1;34m(dotenv_path, stream, verbose, override, interpolate, encoding)\u001b[0m\n\u001b[0;32m    332\u001b[0m     dotenv_path \u001b[38;5;241m=\u001b[39m find_dotenv()\n\u001b[0;32m    334\u001b[0m dotenv \u001b[38;5;241m=\u001b[39m DotEnv(\n\u001b[0;32m    335\u001b[0m     dotenv_path\u001b[38;5;241m=\u001b[39mdotenv_path,\n\u001b[0;32m    336\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    340\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m    341\u001b[0m )\n\u001b[1;32m--> 342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dotenv\u001b[38;5;241m.\u001b[39mset_as_environment_variables()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\dotenv\\main.py:85\u001b[0m, in \u001b[0;36mDotEnv.set_as_environment_variables\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_as_environment_variables\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m     82\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    Load the current dotenv as system environment variable.\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdict():\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdict()\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\dotenv\\main.py:69\u001b[0m, in \u001b[0;36mDotEnv.dict\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     66\u001b[0m raw_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse()\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterpolate:\n\u001b[1;32m---> 69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dict \u001b[38;5;241m=\u001b[39m OrderedDict(resolve_variables(raw_values, override\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverride))\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dict \u001b[38;5;241m=\u001b[39m OrderedDict(raw_values)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\dotenv\\main.py:229\u001b[0m, in \u001b[0;36mresolve_variables\u001b[1;34m(values, override)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresolve_variables\u001b[39m(\n\u001b[0;32m    224\u001b[0m     values: Iterable[Tuple[\u001b[38;5;28mstr\u001b[39m, Optional[\u001b[38;5;28mstr\u001b[39m]]],\n\u001b[0;32m    225\u001b[0m     override: \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m    226\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Mapping[\u001b[38;5;28mstr\u001b[39m, Optional[\u001b[38;5;28mstr\u001b[39m]]:\n\u001b[0;32m    227\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m {}  \u001b[38;5;66;03m# type: Dict[str, Optional[str]]\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (name, value) \u001b[38;5;129;01min\u001b[39;00m values:\n\u001b[0;32m    230\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    231\u001b[0m             result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\dotenv\\main.py:77\u001b[0m, in \u001b[0;36mDotEnv.parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Tuple[\u001b[38;5;28mstr\u001b[39m, Optional[\u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_stream() \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[1;32m---> 77\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m mapping \u001b[38;5;129;01min\u001b[39;00m with_warn_for_invalid_lines(parse_stream(stream)):\n\u001b[0;32m     78\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m mapping\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m                 \u001b[38;5;28;01myield\u001b[39;00m mapping\u001b[38;5;241m.\u001b[39mkey, mapping\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\dotenv\\main.py:19\u001b[0m, in \u001b[0;36mwith_warn_for_invalid_lines\u001b[1;34m(mappings)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwith_warn_for_invalid_lines\u001b[39m(mappings: Iterator[Binding]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Binding]:\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m mapping \u001b[38;5;129;01min\u001b[39;00m mappings:\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mapping\u001b[38;5;241m.\u001b[39merror:\n\u001b[0;32m     21\u001b[0m             logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m     22\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython-dotenv could not parse statement starting at line \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     23\u001b[0m                 mapping\u001b[38;5;241m.\u001b[39moriginal\u001b[38;5;241m.\u001b[39mline,\n\u001b[0;32m     24\u001b[0m             )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\dotenv\\parser.py:180\u001b[0m, in \u001b[0;36mparse_stream\u001b[1;34m(stream)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_stream\u001b[39m(stream: IO[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Binding]:\n\u001b[1;32m--> 180\u001b[0m     reader \u001b[38;5;241m=\u001b[39m Reader(stream)\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m reader\u001b[38;5;241m.\u001b[39mhas_next():\n\u001b[0;32m    182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m parse_binding(reader)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\dotenv\\parser.py:71\u001b[0m, in \u001b[0;36mReader.__init__\u001b[1;34m(self, stream)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, stream: IO[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstring \u001b[38;5;241m=\u001b[39m stream\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition \u001b[38;5;241m=\u001b[39m Position\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmark \u001b[38;5;241m=\u001b[39m Position\u001b[38;5;241m.\u001b[39mstart()\n",
      "File \u001b[1;32m<frozen codecs>:322\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "# Robust OpenAI key loader + summarizer\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- Point to your project root explicitly ---\n",
    "ROOT = Path(r\"C:\\Users\\aayus\\Downloads\\emr-smart\")\n",
    "ENV_PATH = ROOT / \".env\"\n",
    "\n",
    "# Load .env from explicit path (works even if CWD is elsewhere)\n",
    "if not ENV_PATH.exists():\n",
    "    raise RuntimeError(f\".env not found at: {ENV_PATH}\\nCreate it with a single line:\\nOPENAI_API_KEY=sk-...\")\n",
    "\n",
    "load_dotenv(dotenv_path=str(ENV_PATH))\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key or not api_key.strip():\n",
    "    raise RuntimeError(\n",
    "        \"OPENAI_API_KEY not found after loading .env.\\n\"\n",
    "        \"Check for typos (exact name), no quotes, and restart the kernel.\"\n",
    "    )\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# ----- paths -----\n",
    "BASE_OUT = ROOT / \"outputs\"\n",
    "IN_PATH  = BASE_OUT / \"fusion.ndjson\"\n",
    "OUT_PATH = BASE_OUT / \"summaries.txt\"\n",
    "\n",
    "if not IN_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing input: {IN_PATH} (run fuse_infer_generate.py first)\")\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are a clinical assistant. Summarize EMR outputs for a clinician in 5–7 sentences. \"\n",
    "    \"Start with quick patient context, describe current severity, 5-week trend, key abnormal vitals, and any alerts. \"\n",
    "    \"Use concise, neutral, non-alarming language and avoid definitive diagnoses; suggest clinical actions when appropriate.\"\n",
    ")\n",
    "\n",
    "written = 0\n",
    "with open(IN_PATH, \"r\", encoding=\"utf-8\") as fin, open(OUT_PATH, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for line in fin:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        rec = json.loads(line)\n",
    "        pid = rec.get(\"patient_id\", \"unknown\")\n",
    "        snapshot = rec.get(\"snapshot\") or {}\n",
    "        history  = rec.get(\"history\")  or {}\n",
    "        alerts   = rec.get(\"alerts\")   or []\n",
    "\n",
    "        user_prompt = (\n",
    "            f\"Patient ID: {pid}\\n\"\n",
    "            f\"Current severity: {snapshot.get('severity_numeric')} ({snapshot.get('severity_label')})\\n\"\n",
    "            f\"Trend: {history.get('trend')}\\n\"\n",
    "            f\"Class probabilities: {history.get('proba')}\\n\"\n",
    "            f\"Alerts: {alerts}\\n\"\n",
    "            \"Summarize for a clinician.\"\n",
    "        )\n",
    "\n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\",   \"content\": user_prompt},\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "        )\n",
    "        text = (resp.choices[0].message.content or \"\").strip()\n",
    "        fout.write(f\"[{pid}]\\n{text}\\n\\n\")\n",
    "        written += 1\n",
    "\n",
    "print(f\"Wrote {written} summaries -> {OUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c622cf3e-bdb3-45cc-851d-d9e8170f99cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 77\u001b[0m\n\u001b[0;32m     66\u001b[0m alerts   \u001b[38;5;241m=\u001b[39m rec\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malerts\u001b[39m\u001b[38;5;124m\"\u001b[39m)   \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[0;32m     68\u001b[0m user_prompt \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPatient ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent severity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msnapshot\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseverity_numeric\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msnapshot\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseverity_label\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummarize for a clinician.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     75\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m resp \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m     78\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o-mini\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     79\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     80\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: system_prompt},\n\u001b[0;32m     81\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_prompt},\n\u001b[0;32m     82\u001b[0m     ],\n\u001b[0;32m     83\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[0;32m     84\u001b[0m )\n\u001b[0;32m     85\u001b[0m text \u001b[38;5;241m=\u001b[39m (resp\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     86\u001b[0m fout\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\openai\\_utils\\_utils.py:286\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    284\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1147\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m   1103\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m   1145\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m   1146\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m-> 1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m   1148\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1149\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m   1150\u001b[0m             {\n\u001b[0;32m   1151\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m   1152\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m   1153\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[0;32m   1154\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m   1155\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m   1156\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m   1157\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m   1158\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m   1159\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[0;32m   1160\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m   1161\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m   1162\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[0;32m   1163\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m   1164\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m   1165\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[0;32m   1166\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m   1167\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_cache_key\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt_cache_key,\n\u001b[0;32m   1168\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[0;32m   1169\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m   1170\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msafety_identifier\u001b[39m\u001b[38;5;124m\"\u001b[39m: safety_identifier,\n\u001b[0;32m   1171\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m   1172\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m   1173\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m   1174\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[0;32m   1175\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m   1176\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[0;32m   1177\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m   1178\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m   1179\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m   1180\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m   1181\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m   1182\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m   1183\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbosity\u001b[39m\u001b[38;5;124m\"\u001b[39m: verbosity,\n\u001b[0;32m   1184\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweb_search_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: web_search_options,\n\u001b[0;32m   1185\u001b[0m             },\n\u001b[0;32m   1186\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParamsStreaming\n\u001b[0;32m   1187\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[0;32m   1188\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParamsNonStreaming,\n\u001b[0;32m   1189\u001b[0m         ),\n\u001b[0;32m   1190\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m   1191\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m   1192\u001b[0m         ),\n\u001b[0;32m   1193\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m   1194\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1195\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m   1196\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\openai\\_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1258\u001b[0m     )\n\u001b[1;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\openai\\_base_client.py:1047\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1044\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1046\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1047\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "# --- OpenAI EMR Summarizer (robust .env loading) ---\n",
    "\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Project root + outputs\n",
    "ROOT = Path(r\"C:\\Users\\aayus\\Downloads\\emr-smart\")\n",
    "ENV_PATH = ROOT / \".env\"\n",
    "BASE_OUT = ROOT / \"outputs\"\n",
    "IN_PATH  = BASE_OUT / \"fusion.ndjson\"\n",
    "OUT_PATH = BASE_OUT / \"summaries.txt\"\n",
    "\n",
    "# 1) Load .env robustly (handles wrong CWD)\n",
    "if not ENV_PATH.exists():\n",
    "    raise RuntimeError(f\".env not found at: {ENV_PATH}\\nCreate it with one line:\\nOPENAI_API_KEY=sk-...\")\n",
    "\n",
    "# Try multiple encodings in case file was saved with BOM/UTF-16\n",
    "loaded = False\n",
    "for enc in (\"utf-8\", \"utf-8-sig\", \"utf-16\", \"utf-16-le\", \"utf-16-be\"):\n",
    "    try:\n",
    "        if load_dotenv(dotenv_path=str(ENV_PATH), override=True, encoding=enc):\n",
    "            if os.getenv(\"OPENAI_API_KEY\"):\n",
    "                loaded = True\n",
    "                break\n",
    "    except Exception:\n",
    "        pass\n",
    "if not loaded:\n",
    "    raise RuntimeError(\n",
    "        f\"Could not read OPENAI_API_KEY from {ENV_PATH}. \"\n",
    "        \"Re-save the file as UTF-8 (no quotes) with content:\\nOPENAI_API_KEY=sk-...\"\n",
    "    )\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key or not api_key.strip():\n",
    "    raise RuntimeError(\"OPENAI_API_KEY not loaded. Check your .env text and restart the kernel.\")\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# 2) Validate inputs\n",
    "if not IN_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing input: {IN_PATH}\\nRun fuse_infer_generate.py first.\")\n",
    "BASE_OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 3) Prompts\n",
    "system_prompt = (\n",
    "    \"You are a clinical assistant. Summarize EMR outputs for a clinician in 5–7 sentences. \"\n",
    "    \"Start with quick patient context, describe current severity, 5-week trend, key abnormal vitals, and any alerts. \"\n",
    "    \"Use concise, neutral, non-alarming language and avoid definitive diagnoses; suggest clinical actions when appropriate. \"\n",
    "    \"Use calibrated language (e.g., 'suggests', 'consider'); do not include PII beyond patient_id.\"\n",
    ")\n",
    "\n",
    "# 4) Generate summaries\n",
    "written = 0\n",
    "with open(IN_PATH, \"r\", encoding=\"utf-8\") as fin, open(OUT_PATH, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for line in fin:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        rec = json.loads(line)\n",
    "        pid = rec.get(\"patient_id\", \"unknown\")\n",
    "        snapshot = rec.get(\"snapshot\") or {}\n",
    "        history  = rec.get(\"history\")  or {}\n",
    "        alerts   = rec.get(\"alerts\")   or []\n",
    "\n",
    "        user_prompt = (\n",
    "            f\"Patient ID: {pid}\\n\"\n",
    "            f\"Current severity: {snapshot.get('severity_numeric')} ({snapshot.get('severity_label')})\\n\"\n",
    "            f\"Trend: {history.get('trend')}\\n\"\n",
    "            f\"Class probabilities: {history.get('proba')}\\n\"\n",
    "            f\"Alerts: {alerts}\\n\"\n",
    "            \"Summarize for a clinician.\"\n",
    "        )\n",
    "\n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\",   \"content\": user_prompt},\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "        )\n",
    "        text = (resp.choices[0].message.content or \"\").strip()\n",
    "        fout.write(f\"[{pid}]\\n{text}\\n\\n\")\n",
    "        written += 1\n",
    "\n",
    "print(f\"Wrote {written} summaries -> {OUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ea2d11a-683a-41c4-af9a-41468c52dff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 fusion records.\n",
      "[429] Rate limited. Sleeping 2.0s (attempt 1/7)\n",
      "[429] Rate limited. Sleeping 4.0s (attempt 2/7)\n",
      "[429] Rate limited. Sleeping 8.0s (attempt 3/7)\n",
      "[429] Rate limited. Sleeping 16.0s (attempt 4/7)\n",
      "[429] Rate limited. Sleeping 32.0s (attempt 5/7)\n",
      "[429] Rate limited. Sleeping 60.0s (attempt 6/7)\n",
      "[429] Rate limited. Sleeping 60.0s (attempt 7/7)\n",
      "[429] Rate limited. Sleeping 2.0s (attempt 1/7)\n",
      "[429] Rate limited. Sleeping 4.0s (attempt 2/7)\n",
      "[429] Rate limited. Sleeping 8.0s (attempt 3/7)\n",
      "[429] Rate limited. Sleeping 16.0s (attempt 4/7)\n",
      "[429] Rate limited. Sleeping 32.0s (attempt 5/7)\n",
      "[429] Rate limited. Sleeping 60.0s (attempt 6/7)\n",
      "[429] Rate limited. Sleeping 60.0s (attempt 7/7)\n",
      "[429] Rate limited. Sleeping 2.0s (attempt 1/7)\n",
      "[429] Rate limited. Sleeping 4.0s (attempt 2/7)\n",
      "[429] Rate limited. Sleeping 8.0s (attempt 3/7)\n",
      "[429] Rate limited. Sleeping 16.0s (attempt 4/7)\n",
      "[429] Rate limited. Sleeping 32.0s (attempt 5/7)\n",
      "[429] Rate limited. Sleeping 60.0s (attempt 6/7)\n",
      "[429] Rate limited. Sleeping 60.0s (attempt 7/7)\n",
      "[429] Rate limited. Sleeping 2.0s (attempt 1/7)\n",
      "[429] Rate limited. Sleeping 4.0s (attempt 2/7)\n",
      "[429] Rate limited. Sleeping 8.0s (attempt 3/7)\n",
      "[429] Rate limited. Sleeping 16.0s (attempt 4/7)\n",
      "[429] Rate limited. Sleeping 32.0s (attempt 5/7)\n",
      "[429] Rate limited. Sleeping 60.0s (attempt 6/7)\n",
      "[429] Rate limited. Sleeping 60.0s (attempt 7/7)\n",
      "[429] Rate limited. Sleeping 2.0s (attempt 1/7)\n",
      "[429] Rate limited. Sleeping 4.0s (attempt 2/7)\n",
      "[429] Rate limited. Sleeping 8.0s (attempt 3/7)\n",
      "[429] Rate limited. Sleeping 16.0s (attempt 4/7)\n",
      "[429] Rate limited. Sleeping 32.0s (attempt 5/7)\n",
      "[429] Rate limited. Sleeping 60.0s (attempt 6/7)\n",
      "[429] Rate limited. Sleeping 60.0s (attempt 7/7)\n",
      "[APITimeoutError] transient error. Sleeping 2.0s (attempt 1/7)\n",
      "[429] Rate limited. Sleeping 4.0s (attempt 2/7)\n",
      "[429] Rate limited. Sleeping 8.0s (attempt 3/7)\n",
      "[429] Rate limited. Sleeping 16.0s (attempt 4/7)\n",
      "[429] Rate limited. Sleeping 32.0s (attempt 5/7)\n",
      "[429] Rate limited. Sleeping 60.0s (attempt 6/7)\n",
      "[429] Rate limited. Sleeping 60.0s (attempt 7/7)\n",
      "[429] Rate limited. Sleeping 2.0s (attempt 1/7)\n",
      "[429] Rate limited. Sleeping 4.0s (attempt 2/7)\n",
      "[429] Rate limited. Sleeping 8.0s (attempt 3/7)\n",
      "[429] Rate limited. Sleeping 16.0s (attempt 4/7)\n",
      "[429] Rate limited. Sleeping 32.0s (attempt 5/7)\n",
      "[APIConnectionError] transient error. Sleeping 60.0s (attempt 6/7)\n",
      "[429] Rate limited. Sleeping 60.0s (attempt 7/7)\n",
      "[429] Rate limited. Sleeping 2.0s (attempt 1/7)\n",
      "[429] Rate limited. Sleeping 4.0s (attempt 2/7)\n",
      "[429] Rate limited. Sleeping 8.0s (attempt 3/7)\n",
      "[429] Rate limited. Sleeping 16.0s (attempt 4/7)\n",
      "[429] Rate limited. Sleeping 32.0s (attempt 5/7)\n",
      "[429] Rate limited. Sleeping 60.0s (attempt 6/7)\n",
      "[429] Rate limited. Sleeping 60.0s (attempt 7/7)\n",
      "[429] Rate limited. Sleeping 2.0s (attempt 1/7)\n",
      "[429] Rate limited. Sleeping 4.0s (attempt 2/7)\n",
      "[429] Rate limited. Sleeping 8.0s (attempt 3/7)\n",
      "[429] Rate limited. Sleeping 16.0s (attempt 4/7)\n",
      "[429] Rate limited. Sleeping 32.0s (attempt 5/7)\n",
      "[429] Rate limited. Sleeping 60.0s (attempt 6/7)\n",
      "[429] Rate limited. Sleeping 60.0s (attempt 7/7)\n",
      "[429] Rate limited. Sleeping 2.0s (attempt 1/7)\n",
      "[429] Rate limited. Sleeping 4.0s (attempt 2/7)\n",
      "[429] Rate limited. Sleeping 8.0s (attempt 3/7)\n",
      "[429] Rate limited. Sleeping 16.0s (attempt 4/7)\n",
      "[429] Rate limited. Sleeping 32.0s (attempt 5/7)\n",
      "[429] Rate limited. Sleeping 60.0s (attempt 6/7)\n",
      "[429] Rate limited. Sleeping 60.0s (attempt 7/7)\n",
      "[429] Rate limited. Sleeping 2.0s (attempt 1/7)\n",
      "[429] Rate limited. Sleeping 4.0s (attempt 2/7)\n",
      "[429] Rate limited. Sleeping 8.0s (attempt 3/7)\n",
      "[429] Rate limited. Sleeping 16.0s (attempt 4/7)\n",
      "[429] Rate limited. Sleeping 32.0s (attempt 5/7)\n",
      "[429] Rate limited. Sleeping 60.0s (attempt 6/7)\n",
      "[429] Rate limited. Sleeping 60.0s (attempt 7/7)\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:\\\\Users\\\\aayus\\\\Downloads\\\\emr-smart\\\\outputs\\\\summaries.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 193\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutputs:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUT_TXT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUT_CSV\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCACHE_JL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 193\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[12], line 182\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;66;03m# Call API with retries/backoff\u001b[39;00m\n\u001b[0;32m    181\u001b[0m summary \u001b[38;5;241m=\u001b[39m call_with_retries(client, messages)\n\u001b[1;32m--> 182\u001b[0m append_outputs(pid, summary)\n\u001b[0;32m    183\u001b[0m append_cache(pid, fp, summary)\n\u001b[0;32m    184\u001b[0m done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[12], line 86\u001b[0m, in \u001b[0;36mappend_outputs\u001b[1;34m(pid, summary)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# also CSV (append header if missing)\u001b[39;00m\n\u001b[0;32m     85\u001b[0m make_header \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m OUT_CSV\u001b[38;5;241m.\u001b[39mexists()\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(OUT_CSV, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fc:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m make_header:\n\u001b[0;32m     88\u001b[0m         fc\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpatient_id,summary\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C:\\\\Users\\\\aayus\\\\Downloads\\\\emr-smart\\\\outputs\\\\summaries.csv'"
     ]
    }
   ],
   "source": [
    "# summaries_generate.py — resilient OpenAI summarizer with retries, throttle, resume, cache\n",
    "\n",
    "import os, json, time, hashlib\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from openai import APIConnectionError, RateLimitError, InternalServerError, OpenAIError\n",
    "\n",
    "# ---------------- Paths ----------------\n",
    "ROOT      = Path(r\"C:\\Users\\aayus\\Downloads\\emr-smart\")\n",
    "DATA_DIR  = ROOT / \"outputs\"\n",
    "IN_PATH   = DATA_DIR / \"fusion.ndjson\"\n",
    "OUT_TXT   = DATA_DIR / \"summaries.txt\"\n",
    "OUT_CSV   = DATA_DIR / \"summaries.csv\"\n",
    "CACHE_JL  = DATA_DIR / \"summaries_cache.ndjson\"   # patient_id + fingerprint -> summary\n",
    "\n",
    "# ---------------- Config knobs ----------------\n",
    "MODEL_NAME          = \"gpt-4o-mini\"\n",
    "MAX_PER_RUN         = 200          #  set smaller while testing (e.g., 20)\n",
    "START_AT_INDEX      = 0            #  start from a later index to resume a large batch\n",
    "SLEEP_BETWEEN_CALLS = 2.0          #  seconds between requests to avoid rate caps\n",
    "MAX_RETRIES         = 7            #  total attempts per patient on 429/5xx\n",
    "TEMPERATURE         = 0.2\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a clinical assistant. Summarize EMR outputs for a clinician in 5–7 sentences. \"\n",
    "    \"Start with quick patient context, then current severity, 5-week trend, and noteworthy vitals or risks. \"\n",
    "    \"Reference model probabilities only if helpful (e.g., 'high likelihood of Worsening'). \"\n",
    "    \"Use calibrated, non-alarming language (e.g., 'suggests', 'consider'); avoid firm diagnoses or prescriptions. \"\n",
    "    \"Do not include PII beyond patient_id. End with: 'This supports decisions and is not a diagnosis.'\"\n",
    ")\n",
    "\n",
    "USER_TEMPLATE = (\n",
    "    \"Patient ID: {pid}\\n\"\n",
    "    \"Current severity: {sev_num} ({sev_label})\\n\"\n",
    "    \"Trend: {trend}\\n\"\n",
    "    \"Class probabilities: {proba}\\n\"\n",
    "    \"Alerts: {alerts}\\n\"\n",
    "    \"Summarize succinctly for a clinician.\"\n",
    ")\n",
    "\n",
    "# --------------- Helpers ----------------\n",
    "def ensure_env():\n",
    "    env_path = ROOT / \".env\"\n",
    "    if not env_path.exists():\n",
    "        raise RuntimeError(f\".env not found at {env_path}. Create a UTF-8 file with:\\nOPENAI_API_KEY=sk-...\")\n",
    "\n",
    "    loaded = False\n",
    "    for enc in (\"utf-8\", \"utf-8-sig\", \"utf-16\", \"utf-16-le\", \"utf-16-be\"):\n",
    "        try:\n",
    "            if load_dotenv(dotenv_path=str(env_path), override=True, encoding=enc):\n",
    "                if os.getenv(\"OPENAI_API_KEY\"):\n",
    "                    loaded = True\n",
    "                    break\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not loaded:\n",
    "        raise RuntimeError(\"Could not read OPENAI_API_KEY from .env. Re-save as UTF-8 (no quotes).\")\n",
    "\n",
    "def stable_fingerprint(obj) -> str:\n",
    "    \"\"\"Hash the fusion record to skip re-summarization if unchanged.\"\"\"\n",
    "    txt = json.dumps(obj, sort_keys=True, ensure_ascii=False)\n",
    "    return hashlib.sha256(txt.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def load_cache():\n",
    "    seen = {}\n",
    "    if CACHE_JL.exists():\n",
    "        with open(CACHE_JL, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                rec = json.loads(line)\n",
    "                seen[(rec[\"patient_id\"], rec[\"fingerprint\"])] = rec[\"summary\"]\n",
    "    return seen\n",
    "\n",
    "def append_cache(pid, fp, summary):\n",
    "    with open(CACHE_JL, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps({\"patient_id\": pid, \"fingerprint\": fp, \"summary\": summary}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def append_outputs(pid, summary):\n",
    "    OUT_TXT.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(OUT_TXT, \"a\", encoding=\"utf-8\") as ft:\n",
    "        ft.write(f\"[{pid}]\\n{summary}\\n\\n\")\n",
    "    # also CSV (append header if missing)\n",
    "    make_header = not OUT_CSV.exists()\n",
    "    with open(OUT_CSV, \"a\", encoding=\"utf-8\") as fc:\n",
    "        if make_header:\n",
    "            fc.write(\"patient_id,summary\\n\")\n",
    "        # simple CSV escaping for commas/quotes/newlines\n",
    "        clean = summary.replace('\"', '\"\"').replace(\"\\n\", \" \").strip()\n",
    "        fc.write(f'\"{pid}\",\"{clean}\"\\n')\n",
    "\n",
    "def call_with_retries(client, messages):\n",
    "    delay = 2.0\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            resp = client.chat.completions.create(\n",
    "                model=MODEL_NAME,\n",
    "                messages=messages,\n",
    "                temperature=TEMPERATURE,\n",
    "            )\n",
    "            return (resp.choices[0].message.content or \"\").strip()\n",
    "        except RateLimitError as e:\n",
    "            # Honor Retry-After if present\n",
    "            retry_after = None\n",
    "            try:\n",
    "                retry_after = float(getattr(getattr(e, \"response\", None), \"headers\", {}).get(\"Retry-After\", None))\n",
    "            except Exception:\n",
    "                pass\n",
    "            sleep_for = retry_after if retry_after else delay\n",
    "            print(f\"[429] Rate limited. Sleeping {sleep_for:.1f}s (attempt {attempt}/{MAX_RETRIES})\")\n",
    "            time.sleep(sleep_for)\n",
    "            delay = min(delay * 2, 60)  # cap backoff\n",
    "        except (InternalServerError, APIConnectionError) as e:\n",
    "            print(f\"[{type(e).__name__}] transient error. Sleeping {delay:.1f}s (attempt {attempt}/{MAX_RETRIES})\")\n",
    "            time.sleep(delay)\n",
    "            delay = min(delay * 2, 60)\n",
    "        except OpenAIError as e:\n",
    "            # Other non-retriable API errors — return a note but keep pipeline alive\n",
    "            return f\"Summary unavailable due to API error: {str(e)[:200]}\"\n",
    "    return \"Summary unavailable after repeated API rate limits.\"\n",
    "\n",
    "def build_user_prompt(rec):\n",
    "    pid     = rec.get(\"patient_id\", \"unknown\")\n",
    "    snap    = rec.get(\"snapshot\") or {}\n",
    "    hist    = rec.get(\"history\")  or {}\n",
    "    alerts  = rec.get(\"alerts\")   or []\n",
    "\n",
    "    return USER_TEMPLATE.format(\n",
    "        pid=pid,\n",
    "        sev_num=snap.get(\"severity_numeric\"),\n",
    "        sev_label=snap.get(\"severity_label\"),\n",
    "        trend=hist.get(\"trend\"),\n",
    "        proba=hist.get(\"proba\"),\n",
    "        alerts=\"; \".join(alerts) if alerts else \"None\"\n",
    "    )\n",
    "\n",
    "# --------------- Main ----------------\n",
    "def main():\n",
    "    ensure_env()\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "    if not IN_PATH.exists():\n",
    "        raise FileNotFoundError(f\"Missing input {IN_PATH}. Run fuse_infer_generate.py first.\")\n",
    "\n",
    "    cache = load_cache()\n",
    "    done = 0\n",
    "    total = 0\n",
    "\n",
    "    # Read all lines first so indexing works\n",
    "    with open(IN_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        fusion_lines = [ln for ln in (ln.strip() for ln in f) if ln]\n",
    "\n",
    "    print(f\"Found {len(fusion_lines)} fusion records.\")\n",
    "    for idx, line in enumerate(fusion_lines):\n",
    "        total += 1\n",
    "        if idx < START_AT_INDEX:\n",
    "            continue\n",
    "        if done >= MAX_PER_RUN:\n",
    "            break\n",
    "\n",
    "        rec = json.loads(line)\n",
    "        pid = rec.get(\"patient_id\", f\"row_{idx}\")\n",
    "        fp  = stable_fingerprint(rec)\n",
    "\n",
    "        # Skip if exact same record was summarized earlier\n",
    "        if (pid, fp) in cache:\n",
    "            summary = cache[(pid, fp)]\n",
    "            append_outputs(pid, summary)\n",
    "            done += 1\n",
    "            continue\n",
    "\n",
    "        user_prompt = build_user_prompt(rec)\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\",   \"content\": user_prompt},\n",
    "        ]\n",
    "\n",
    "        # Call API with retries/backoff\n",
    "        summary = call_with_retries(client, messages)\n",
    "        append_outputs(pid, summary)\n",
    "        append_cache(pid, fp, summary)\n",
    "        done += 1\n",
    "\n",
    "        # Gentle throttle to avoid 429s\n",
    "        time.sleep(SLEEP_BETWEEN_CALLS)\n",
    "\n",
    "    print(f\"Completed {done} summaries (out of {total})\")\n",
    "    print(f\"Outputs:\\n - {OUT_TXT}\\n - {OUT_CSV}\\n - {CACHE_JL}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601dfa9e-3705-41b7-87af-e6e8d8db383d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- OpenAI EMR Summarizer (limit to 10 requests) ---\n",
    "\n",
    "import os, json, time, random\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# === Paths ===\n",
    "ROOT = Path(r\"C:\\Users\\aayus\\Downloads\\emr-smart\")\n",
    "ENV_PATH = ROOT / \".env\"\n",
    "BASE_OUT = ROOT / \"outputs\"\n",
    "IN_PATH  = BASE_OUT / \"fusion.ndjson\"\n",
    "OUT_PATH = BASE_OUT / \"summaries_10.txt\"\n",
    "\n",
    "# === Load .env (robust encodings) ===\n",
    "if not ENV_PATH.exists():\n",
    "    raise RuntimeError(f\".env not found at: {ENV_PATH}\\nPut a line like:\\nOPENAI_API_KEY=sk-...\")\n",
    "\n",
    "loaded = False\n",
    "for enc in (\"utf-8\", \"utf-8-sig\", \"utf-16\", \"utf-16-le\", \"utf-16-be\"):\n",
    "    try:\n",
    "        if load_dotenv(dotenv_path=str(ENV_PATH), override=True, encoding=enc):\n",
    "            if os.getenv(\"OPENAI_API_KEY\"):\n",
    "                loaded = True\n",
    "                break\n",
    "    except Exception:\n",
    "        pass\n",
    "if not loaded:\n",
    "    raise RuntimeError(\n",
    "        f\"Could not read OPENAI_API_KEY from {ENV_PATH}. \"\n",
    "        \"Re-save the file as UTF-8 (no quotes) with content:\\nOPENAI_API_KEY=sk-...\"\n",
    "    )\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# === Inputs ===\n",
    "if not IN_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing input: {IN_PATH}\\nRun fuse_infer_generate.py first.\")\n",
    "BASE_OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# === Config: make at most N calls ===\n",
    "MAX_REQUESTS = 10\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "SLEEP_BETWEEN_CALLS = 2.0   # seconds, gentle throttle\n",
    "MAX_RETRIES = 2              # per request\n",
    "BACKOFF = [2, 4]             # on 429\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are a clinical assistant. Summarize EMR outputs for a clinician in 5–7 sentences. \"\n",
    "    \"Start with quick patient context, describe current severity, 5-week trend, any key abnormalities, and alerts. \"\n",
    "    \"Use concise, neutral, non-alarming language; avoid definitive diagnoses; suggest reasonable next steps. \"\n",
    "    \"Use calibrated language (e.g., 'suggests', 'consider'); do not include PII beyond patient_id.\"\n",
    ")\n",
    "\n",
    "# --- Read fusion NDJSON and pick up to 10 records ---\n",
    "records = []\n",
    "with open(IN_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            records.append(json.loads(line))\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "if not records:\n",
    "    raise RuntimeError(\"No records found in fusion.ndjson\")\n",
    "\n",
    "# Option A: take the first 10\n",
    "# subset = records[:MAX_REQUESTS]\n",
    "\n",
    "# Option B (default here): random sample 10 to get variety\n",
    "random.seed(42)\n",
    "subset = random.sample(records, min(MAX_REQUESTS, len(records)))\n",
    "\n",
    "print(f\"Generating summaries for {len(subset)} patients…\")\n",
    "\n",
    "def one_summary(rec):\n",
    "    pid = rec.get(\"patient_id\", \"unknown\")\n",
    "    snapshot = rec.get(\"snapshot\") or {}\n",
    "    history  = rec.get(\"history\")  or {}\n",
    "    alerts   = rec.get(\"alerts\")   or []\n",
    "\n",
    "    user_prompt = (\n",
    "        f\"Patient ID: {pid}\\n\"\n",
    "        f\"Current severity: {snapshot.get('severity_numeric')} ({snapshot.get('severity_label')})\\n\"\n",
    "        f\"Trend: {history.get('trend')}\\n\"\n",
    "        f\"Class probabilities: {history.get('proba')}\\n\"\n",
    "        f\"Alerts: {alerts}\\n\"\n",
    "        \"Summarize for a clinician.\"\n",
    "    )\n",
    "\n",
    "    # retry loop (handles rate limits briefly)\n",
    "    last_err = None\n",
    "    for attempt in range(1, MAX_RETRIES + 2):  # e.g., attempts = 1..3 if MAX_RETRIES=2\n",
    "        try:\n",
    "            resp = client.chat.completions.create(\n",
    "                model=MODEL,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\",   \"content\": user_prompt},\n",
    "                ],\n",
    "                temperature=0.2,\n",
    "            )\n",
    "            text = (resp.choices[0].message.content or \"\").strip()\n",
    "            return pid, text, None\n",
    "        except Exception as e:\n",
    "            msg = str(e)\n",
    "            last_err = msg\n",
    "            # If it's an insufficient_quota error, no point in retrying more than once\n",
    "            if \"insufficient_quota\" in msg or \"You exceeded your current quota\" in msg:\n",
    "                return pid, None, \"insufficient_quota\"\n",
    "            # If it's a 429 rate limit, back off gently\n",
    "            if \"429\" in msg or \"Rate limit\" in msg or \"rate\" in msg.lower():\n",
    "                sleep_for = BACKOFF[min(attempt-1, len(BACKOFF)-1)]\n",
    "                print(f\"[429] Sleeping {sleep_for}s (attempt {attempt}/{MAX_RETRIES+1})\")\n",
    "                time.sleep(sleep_for)\n",
    "                continue\n",
    "            # Other transient errors: short sleep and retry\n",
    "            time.sleep(1.0)\n",
    "            continue\n",
    "    return pid, None, last_err or \"unknown_error\"\n",
    "\n",
    "written = 0\n",
    "failed  = 0\n",
    "\n",
    "with open(OUT_PATH, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for i, rec in enumerate(subset, start=1):\n",
    "        pid, text, err = one_summary(rec)\n",
    "        if err == \"insufficient_quota\":\n",
    "            print(\"Stopped: insufficient quota on your OpenAI account.\")\n",
    "            break\n",
    "        if text:\n",
    "            fout.write(f\"[{pid}]\\n{text}\\n\\n\")\n",
    "            written += 1\n",
    "            print(f\"✓ {i}/{len(subset)}  -> wrote summary for patient {pid}\")\n",
    "        else:\n",
    "            failed += 1\n",
    "            print(f\"✗ {i}/{len(subset)}  -> failed for patient {pid}: {err}\")\n",
    "        time.sleep(SLEEP_BETWEEN_CALLS)\n",
    "\n",
    "print(f\"\\nDone. Wrote {written} summaries, failed {failed}.\")\n",
    "print(f\"Output: {OUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7855dd4b-e8b7-495c-9765-406975cccdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize_10.py — make only 10 LLM requests\n",
    "\n",
    "import os, json, time, math\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI, RateLimitError, APIConnectionError, APIStatusError\n",
    "\n",
    "# ---------- paths ----------\n",
    "ROOT = Path(r\"C:\\Users\\aayus\\Downloads\\emr-smart\")\n",
    "ENV_PATH = ROOT / \".env\"\n",
    "BASE_OUT = ROOT / \"outputs\"\n",
    "IN_PATH  = BASE_OUT / \"fusion.ndjson\"\n",
    "OUT_PATH = BASE_OUT / \"summaries.txt\"\n",
    "\n",
    "# ---------- load API key (robust encodings) ----------\n",
    "if not ENV_PATH.exists():\n",
    "    raise RuntimeError(f\".env not found at: {ENV_PATH}\\nPut: OPENAI_API_KEY=sk-...\")\n",
    "\n",
    "loaded = False\n",
    "for enc in (\"utf-8\", \"utf-8-sig\", \"utf-16\", \"utf-16-le\", \"utf-16-be\"):\n",
    "    try:\n",
    "        if load_dotenv(dotenv_path=str(ENV_PATH), override=True, encoding=enc):\n",
    "            if os.getenv(\"OPENAI_API_KEY\"):\n",
    "                loaded = True\n",
    "                break\n",
    "    except Exception:\n",
    "        pass\n",
    "if not loaded:\n",
    "    raise RuntimeError(\"Could not read OPENAI_API_KEY from .env (save as UTF-8, no quotes).\")\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key or not api_key.strip():\n",
    "    raise RuntimeError(\"OPENAI_API_KEY missing after load. Check .env.\")\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# ---------- inputs ----------\n",
    "if not IN_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing input file: {IN_PATH} (run fuse_infer_generate.py first)\")\n",
    "BASE_OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- prompts ----------\n",
    "SYSTEM = (\n",
    "    \"You are a clinical assistant. Summarize EMR outputs for a clinician in ~5–7 sentences. \"\n",
    "    \"Start with quick patient context, describe current severity, 5-week trend, key abnormal vitals, and any alerts. \"\n",
    "    \"Use neutral, concise language; avoid diagnoses; suggest actions with calibrated terms (e.g., 'consider'). \"\n",
    "    \"Do not include PII beyond patient_id.\"\n",
    ")\n",
    "\n",
    "def build_user_prompt(rec: dict) -> str:\n",
    "    pid = rec.get(\"patient_id\", \"unknown\")\n",
    "    snapshot = rec.get(\"snapshot\") or {}\n",
    "    history  = rec.get(\"history\")  or {}\n",
    "    alerts   = rec.get(\"alerts\")   or []\n",
    "    return (\n",
    "        f\"Patient ID: {pid}\\n\"\n",
    "        f\"Current severity: {snapshot.get('severity_numeric')} ({snapshot.get('severity_label')})\\n\"\n",
    "        f\"Trend: {history.get('trend')}\\n\"\n",
    "        f\"Class probabilities: {history.get('proba')}\\n\"\n",
    "        f\"Alerts: {alerts}\\n\"\n",
    "        \"Write one concise, clinician-facing paragraph (≤150 words). \"\n",
    "        \"End with: 'This supports decisions and is not a diagnosis.'\"\n",
    "    )\n",
    "\n",
    "# ---------- small retry wrapper (max 3) ----------\n",
    "def ask_llm(prompt: str, max_retries: int = 3):\n",
    "    delay = 2.0\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            resp = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM},\n",
    "                    {\"role\": \"user\",   \"content\": prompt},\n",
    "                ],\n",
    "                temperature=0.2,\n",
    "            )\n",
    "            return (resp.choices[0].message.content or \"\").strip()\n",
    "        except RateLimitError:\n",
    "            if attempt == max_retries:\n",
    "                return f\"[LLM skipped after rate limit attempts: {attempt}]\"\n",
    "            time.sleep(delay)\n",
    "            delay *= 2\n",
    "        except (APIConnectionError, APIStatusError) as e:\n",
    "            if attempt == max_retries:\n",
    "                return f\"[LLM skipped due to API error: {getattr(e, 'message', str(e))}]\"\n",
    "            time.sleep(delay)\n",
    "            delay *= 2\n",
    "        except Exception as e:\n",
    "            # Any other unexpected error — don’t crash the run\n",
    "            return f\"[LLM skipped due to unexpected error: {e}]\"\n",
    "\n",
    "# ---------- run for only 10 records ----------\n",
    "MAX_REQUESTS = 10\n",
    "written = 0\n",
    "\n",
    "with open(IN_PATH, \"r\", encoding=\"utf-8\") as fin, open(OUT_PATH, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for line_idx, line in enumerate(fin, start=1):\n",
    "        if written >= MAX_REQUESTS:\n",
    "            break\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            rec = json.loads(line)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "        pid = rec.get(\"patient_id\", f\"row_{line_idx}\")\n",
    "        prompt = build_user_prompt(rec)\n",
    "        text = ask_llm(prompt, max_retries=3)\n",
    "\n",
    "        fout.write(f\"[{pid}]\\n{text}\\n\\n\")\n",
    "        written += 1\n",
    "        # tiny spacing to be gentle even when not rate-limited\n",
    "        time.sleep(0.5)\n",
    "\n",
    "print(f\"Made {written} requests. Wrote summaries → {OUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5d03d3-e0d0-45b0-8a73-16b181024349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize_10.py — make at most 10 LLM requests; fallback when rate limited\n",
    "\n",
    "import os, json, time, csv\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from openai import OpenAIError, RateLimitError\n",
    "\n",
    "ROOT = Path(r\"C:\\Users\\aayus\\Downloads\\emr-smart\")\n",
    "ENV_PATH = ROOT / \".env\"\n",
    "BASE_OUT = ROOT / \"outputs\"\n",
    "IN_PATH  = BASE_OUT / \"fusion.ndjson\"\n",
    "OUT_PATH = BASE_OUT / \"summaries.csv\"\n",
    "\n",
    "# ---------- robust .env load ----------\n",
    "if not ENV_PATH.exists():\n",
    "    raise RuntimeError(f\".env not found at: {ENV_PATH}\\nPut: OPENAI_API_KEY=sk-...\")\n",
    "\n",
    "loaded = False\n",
    "for enc in (\"utf-8\", \"utf-8-sig\", \"utf-16\", \"utf-16-le\", \"utf-16-be\"):\n",
    "    try:\n",
    "        if load_dotenv(dotenv_path=str(ENV_PATH), override=True, encoding=enc):\n",
    "            if os.getenv(\"OPENAI_API_KEY\"):\n",
    "                loaded = True\n",
    "                break\n",
    "    except Exception:\n",
    "        pass\n",
    "if not loaded:\n",
    "    raise RuntimeError(\n",
    "        f\"Could not read OPENAI_API_KEY from {ENV_PATH}. \"\n",
    "        \"Re-save the file as UTF-8 (no quotes) with content:\\nOPENAI_API_KEY=sk-...\"\n",
    "    )\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\", \"\").strip()\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"OPENAI_API_KEY not loaded; fix your .env and restart the kernel.\")\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# ---------- input checks ----------\n",
    "if not IN_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing input: {IN_PATH}. Run your fuse script first.\")\n",
    "BASE_OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- prompts ----------\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a clinical assistant. Summarize EMR outputs for a clinician in 5–7 sentences. \"\n",
    "    \"Open with quick context; describe current severity, 5-week trend, notable abnormalities, and any alerts. \"\n",
    "    \"Use concise, neutral, non-alarmist language and calibrated phrasing (e.g., 'suggests', 'consider'). \"\n",
    "    \"Avoid diagnoses and prescriptions. No PII beyond patient_id.\"\n",
    ")\n",
    "\n",
    "def fallback_summary(rec: dict) -> str:\n",
    "    \"\"\"Build a safe paragraph if the API is unavailable.\"\"\"\n",
    "    pid = rec.get(\"patient_id\", \"unknown\")\n",
    "    snap = rec.get(\"snapshot\") or {}\n",
    "    hist = rec.get(\"history\") or {}\n",
    "    alerts = rec.get(\"alerts\") or []\n",
    "    sev_val = snap.get(\"severity_numeric\")\n",
    "    sev_lab = snap.get(\"severity_label\") or \"Unknown\"\n",
    "    trend   = hist.get(\"trend\") or \"Unknown\"\n",
    "    proba   = hist.get(\"proba\") or {}\n",
    "    p_imp = proba.get(\"Improving\")\n",
    "    p_sta = proba.get(\"Stable\")\n",
    "    p_wor = proba.get(\"Worsening\")\n",
    "\n",
    "    parts = [\n",
    "        f\"Patient {pid}: current severity ≈ {sev_val if sev_val is not None else 'N/A'} ({sev_lab}).\",\n",
    "        f\"Five-week trend classified as {trend}.\",\n",
    "        f\"Class probabilities — Improving: {p_imp if p_imp is not None else 'N/A'}, Stable: {p_sta if p_sta is not None else 'N/A'}, Worsening: {p_wor if p_wor is not None else 'N/A'}.\",\n",
    "    ]\n",
    "    if alerts:\n",
    "        parts.append(\"Alerts: \" + \"; \".join(alerts) + \".\")\n",
    "    parts.append(\"Overall, findings suggest the above pattern and should be interpreted alongside clinical judgment. This supports decisions and is not a diagnosis.\")\n",
    "    return \" \".join(parts)\n",
    "\n",
    "def summarize_with_llm(rec: dict) -> str:\n",
    "    \"\"\"Try OpenAI once with short output. If 429/other error, raise for caller to fallback.\"\"\"\n",
    "    pid = rec.get(\"patient_id\", \"unknown\")\n",
    "    snap = rec.get(\"snapshot\") or {}\n",
    "    hist = rec.get(\"history\") or {}\n",
    "    alerts = rec.get(\"alerts\") or []\n",
    "\n",
    "    user_prompt = (\n",
    "        f\"Patient ID: {pid}\\n\"\n",
    "        f\"Current severity: {snap.get('severity_numeric')} ({snap.get('severity_label')})\\n\"\n",
    "        f\"Trend: {hist.get('trend')}\\n\"\n",
    "        f\"Class probabilities: {hist.get('proba')}\\n\"\n",
    "        f\"Alerts: {alerts}\\n\"\n",
    "        \"Write one concise paragraph (≤150 words) for a clinician.\"\n",
    "    )\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\",   \"content\": user_prompt},\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        max_tokens=220,  # short output to reduce token usage\n",
    "        n=1,\n",
    "    )\n",
    "    return (resp.choices[0].message.content or \"\").strip()\n",
    "\n",
    "# ---------- main: only 10 requests ----------\n",
    "rows = []\n",
    "with open(IN_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    all_lines = [ln for ln in f if ln.strip()]\n",
    "\n",
    "N = min(10, len(all_lines))\n",
    "print(f\"Found {len(all_lines)} fusion records. Will summarize first {N}.\")\n",
    "\n",
    "for i in range(N):\n",
    "    rec = json.loads(all_lines[i])\n",
    "    pid = rec.get(\"patient_id\", f\"row_{i+1}\")\n",
    "\n",
    "    try:\n",
    "        text = summarize_with_llm(rec)\n",
    "    except RateLimitError:\n",
    "        # If you are out of quota or rate-limited, use a clear fallback\n",
    "        text = fallback_summary(rec)\n",
    "    except OpenAIError:\n",
    "        text = fallback_summary(rec)\n",
    "    except Exception:\n",
    "        text = fallback_summary(rec)\n",
    "\n",
    "    rows.append({\"patient_id\": pid, \"summary\": text})\n",
    "\n",
    "    # gentle pacing to avoid bursts; tweak as needed\n",
    "    time.sleep(1.0)\n",
    "\n",
    "# write CSV\n",
    "with open(OUT_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as fout:\n",
    "    writer = csv.DictWriter(fout, fieldnames=[\"patient_id\", \"summary\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"Wrote {len(rows)} summaries -> {OUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef422898-e7a2-4b4b-ae84-06cdb5eeb54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 fusion records.\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xff in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 86\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# Try a single batched OpenAI call (optional, only if key present)\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[1;32m---> 86\u001b[0m load_dotenv(dotenv_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(ROOT\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.env\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     88\u001b[0m api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     89\u001b[0m use_llm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(api_key)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\dotenv\\main.py:342\u001b[0m, in \u001b[0;36mload_dotenv\u001b[1;34m(dotenv_path, stream, verbose, override, interpolate, encoding)\u001b[0m\n\u001b[0;32m    332\u001b[0m     dotenv_path \u001b[38;5;241m=\u001b[39m find_dotenv()\n\u001b[0;32m    334\u001b[0m dotenv \u001b[38;5;241m=\u001b[39m DotEnv(\n\u001b[0;32m    335\u001b[0m     dotenv_path\u001b[38;5;241m=\u001b[39mdotenv_path,\n\u001b[0;32m    336\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    340\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m    341\u001b[0m )\n\u001b[1;32m--> 342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dotenv\u001b[38;5;241m.\u001b[39mset_as_environment_variables()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\dotenv\\main.py:85\u001b[0m, in \u001b[0;36mDotEnv.set_as_environment_variables\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_as_environment_variables\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m     82\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    Load the current dotenv as system environment variable.\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdict():\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdict()\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\dotenv\\main.py:69\u001b[0m, in \u001b[0;36mDotEnv.dict\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     66\u001b[0m raw_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse()\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterpolate:\n\u001b[1;32m---> 69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dict \u001b[38;5;241m=\u001b[39m OrderedDict(resolve_variables(raw_values, override\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverride))\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dict \u001b[38;5;241m=\u001b[39m OrderedDict(raw_values)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\dotenv\\main.py:229\u001b[0m, in \u001b[0;36mresolve_variables\u001b[1;34m(values, override)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresolve_variables\u001b[39m(\n\u001b[0;32m    224\u001b[0m     values: Iterable[Tuple[\u001b[38;5;28mstr\u001b[39m, Optional[\u001b[38;5;28mstr\u001b[39m]]],\n\u001b[0;32m    225\u001b[0m     override: \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m    226\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Mapping[\u001b[38;5;28mstr\u001b[39m, Optional[\u001b[38;5;28mstr\u001b[39m]]:\n\u001b[0;32m    227\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m {}  \u001b[38;5;66;03m# type: Dict[str, Optional[str]]\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (name, value) \u001b[38;5;129;01min\u001b[39;00m values:\n\u001b[0;32m    230\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    231\u001b[0m             result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\dotenv\\main.py:77\u001b[0m, in \u001b[0;36mDotEnv.parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Tuple[\u001b[38;5;28mstr\u001b[39m, Optional[\u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_stream() \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[1;32m---> 77\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m mapping \u001b[38;5;129;01min\u001b[39;00m with_warn_for_invalid_lines(parse_stream(stream)):\n\u001b[0;32m     78\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m mapping\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m                 \u001b[38;5;28;01myield\u001b[39;00m mapping\u001b[38;5;241m.\u001b[39mkey, mapping\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\dotenv\\main.py:19\u001b[0m, in \u001b[0;36mwith_warn_for_invalid_lines\u001b[1;34m(mappings)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwith_warn_for_invalid_lines\u001b[39m(mappings: Iterator[Binding]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Binding]:\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m mapping \u001b[38;5;129;01min\u001b[39;00m mappings:\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mapping\u001b[38;5;241m.\u001b[39merror:\n\u001b[0;32m     21\u001b[0m             logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m     22\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython-dotenv could not parse statement starting at line \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     23\u001b[0m                 mapping\u001b[38;5;241m.\u001b[39moriginal\u001b[38;5;241m.\u001b[39mline,\n\u001b[0;32m     24\u001b[0m             )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\dotenv\\parser.py:180\u001b[0m, in \u001b[0;36mparse_stream\u001b[1;34m(stream)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_stream\u001b[39m(stream: IO[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Binding]:\n\u001b[1;32m--> 180\u001b[0m     reader \u001b[38;5;241m=\u001b[39m Reader(stream)\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m reader\u001b[38;5;241m.\u001b[39mhas_next():\n\u001b[0;32m    182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m parse_binding(reader)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\dotenv\\parser.py:71\u001b[0m, in \u001b[0;36mReader.__init__\u001b[1;34m(self, stream)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, stream: IO[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstring \u001b[38;5;241m=\u001b[39m stream\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition \u001b[38;5;241m=\u001b[39m Position\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmark \u001b[38;5;241m=\u001b[39m Position\u001b[38;5;241m.\u001b[39mstart()\n",
      "File \u001b[1;32m<frozen codecs>:322\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "# --- EMR Summaries: single-call LLM with automatic local fallback ---\n",
    "\n",
    "import os, json, math\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "ROOT      = Path(r\"C:\\Users\\aayus\\Downloads\\emr-smart\")\n",
    "OUT_DIR   = ROOT / \"outputs\"\n",
    "IN_NDJSON = OUT_DIR / \"fusion.ndjson\"\n",
    "OUT_CSV   = OUT_DIR / \"summaries.csv\"\n",
    "\n",
    "# how many summaries to produce now\n",
    "MAX_RECORDS = 10          # keep small while testing\n",
    "BATCH_SIZE  = 10          # all 10 in a single LLM call\n",
    "MODEL       = \"gpt-4o-mini\"\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def local_summarize_one(rec: dict) -> str:\n",
    "    \"\"\"Deterministic, no-LLM fallback summary.\"\"\"\n",
    "    pid = rec.get(\"patient_id\",\"unknown\")\n",
    "    hist = rec.get(\"history\") or {}\n",
    "    snap = rec.get(\"snapshot\") or {}\n",
    "    alerts = rec.get(\"alerts\") or []\n",
    "\n",
    "    sev_num = snap.get(\"severity_numeric\", None)\n",
    "    sev_lab = snap.get(\"severity_label\", \"\")\n",
    "    trend   = hist.get(\"trend\", \"\")\n",
    "    proba   = hist.get(\"proba\", {})\n",
    "\n",
    "    # pick top class prob\n",
    "    if isinstance(proba, dict) and proba:\n",
    "        top_class = max(proba.items(), key=lambda kv: kv[1])[0]\n",
    "        top_prob  = max(proba.values())\n",
    "    else:\n",
    "        top_class, top_prob = \"\", None\n",
    "\n",
    "    bits = []\n",
    "    bits.append(f\"Patient {pid}:\")\n",
    "    if sev_num is not None:\n",
    "        bits.append(f\"current severity ≈ {sev_num:.2f} ({sev_lab or 'Unknown'}).\")\n",
    "    else:\n",
    "        bits.append(f\"current severity not available.\")\n",
    "    if trend:\n",
    "        bits.append(f\"5-week trend: {trend}.\")\n",
    "    if top_class:\n",
    "        if top_prob is not None:\n",
    "            bits.append(f\"Model confidence leans {top_class} (~{top_prob:.3f}).\")\n",
    "        else:\n",
    "            bits.append(f\"Model confidence leans {top_class}.\")\n",
    "    if alerts:\n",
    "        bits.append(\"Alerts: \" + \"; \".join(alerts) + \".\")\n",
    "    # simple guidance text\n",
    "    if sev_lab == \"High\" or trend == \"Worsening\":\n",
    "        bits.append(\"Consider closer monitoring, revisit medications, or additional evaluation as clinically indicated.\")\n",
    "    else:\n",
    "        bits.append(\"Continue routine care; monitor for changes.\")\n",
    "\n",
    "    bits.append(\"This supports decisions and is not a diagnosis.\")\n",
    "    return \" \".join(bits)\n",
    "\n",
    "def load_fusion_rows(path: Path, limit: int):\n",
    "    recs = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= limit: break\n",
    "            line = line.strip()\n",
    "            if not line: continue\n",
    "            recs.append(json.loads(line))\n",
    "    return recs\n",
    "\n",
    "def to_rows_for_csv(records, texts):\n",
    "    rows = []\n",
    "    for rec, txt in zip(records, texts):\n",
    "        rows.append({\"patient_id\": rec.get(\"patient_id\",\"unknown\"), \"summary\": txt})\n",
    "    return rows\n",
    "\n",
    "# ---------- main ----------\n",
    "if not IN_NDJSON.exists():\n",
    "    raise FileNotFoundError(f\"Missing {IN_NDJSON}. Run your fuse_infer_generate step first.\")\n",
    "\n",
    "records = load_fusion_rows(IN_NDJSON, MAX_RECORDS)\n",
    "print(f\"Loaded {len(records)} fusion records.\")\n",
    "\n",
    "# Try a single batched OpenAI call (optional, only if key present)\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=str(ROOT/\".env\"))\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\", \"\").strip()\n",
    "use_llm = bool(api_key)\n",
    "\n",
    "all_summaries = []\n",
    "\n",
    "if use_llm:\n",
    "    try:\n",
    "        # Prepare one user prompt that includes up to BATCH_SIZE records\n",
    "        batches = [records[i:i+BATCH_SIZE] for i in range(0, len(records), BATCH_SIZE)]\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI(api_key=api_key)\n",
    "\n",
    "        for batch in batches:\n",
    "            # single prompt for multiple patients\n",
    "            lines = []\n",
    "            for rec in batch:\n",
    "                pid = rec.get(\"patient_id\",\"unknown\")\n",
    "                hist = rec.get(\"history\") or {}\n",
    "                snap = rec.get(\"snapshot\") or {}\n",
    "                alerts = rec.get(\"alerts\") or []\n",
    "\n",
    "                sev_num = snap.get(\"severity_numeric\", None)\n",
    "                sev_lab = snap.get(\"severity_label\", \"\")\n",
    "                trend   = hist.get(\"trend\", \"\")\n",
    "                proba   = hist.get(\"proba\", {})\n",
    "\n",
    "                lines.append(json.dumps({\n",
    "                    \"patient_id\": pid,\n",
    "                    \"severity_numeric\": sev_num,\n",
    "                    \"severity_label\": sev_lab,\n",
    "                    \"trend\": trend,\n",
    "                    \"proba\": proba,\n",
    "                    \"alerts\": alerts\n",
    "                }, ensure_ascii=False))\n",
    "\n",
    "            user_prompt = (\n",
    "                \"You are a clinical assistant. For each JSON line below, write ONE concise 5–7 sentence paragraph \"\n",
    "                \"for clinicians: start with patient context, state current severity and 5-week trend, comment on class \"\n",
    "                \"probabilities (confidence) if useful, include any alerts, and end with \"\n",
    "                \"'This supports decisions and is not a diagnosis.'\\n\\n\"\n",
    "                \"Return output as NDJSON where each line = \"\n",
    "                \"{\\\"patient_id\\\":\\\"...\\\",\\\"summary\\\":\\\"...\\\"} in the same order.\\n\\n\"\n",
    "                \"INPUT LINES:\\n\" + \"\\n\".join(lines)\n",
    "            )\n",
    "\n",
    "            resp = client.chat.completions.create(\n",
    "                model=MODEL,\n",
    "                messages=[\n",
    "                    {\"role\":\"system\",\"content\":\"Be concise, neutral, and clinically appropriate; avoid prescriptions.\"},\n",
    "                    {\"role\":\"user\",\"content\": user_prompt}\n",
    "                ],\n",
    "                temperature=0.2,\n",
    "                max_tokens=900,   # enough for ~10 short paragraphs\n",
    "            )\n",
    "            text = (resp.choices[0].message.content or \"\").strip()\n",
    "\n",
    "            # parse NDJSON back\n",
    "            for ln in text.splitlines():\n",
    "                ln = ln.strip()\n",
    "                if not ln:\n",
    "                    continue\n",
    "                try:\n",
    "                    obj = json.loads(ln)\n",
    "                    all_summaries.append(obj.get(\"summary\",\"\").strip())\n",
    "                except Exception:\n",
    "                    # if parsing fails, fall back for that line\n",
    "                    idx = len(all_summaries)\n",
    "                    if idx < len(batch):\n",
    "                        all_summaries.append(local_summarize_one(batch[idx]))\n",
    "                    else:\n",
    "                        all_summaries.append(\"Summary unavailable (parse error).\")\n",
    "        # safety: if lengths mismatch, fill with local\n",
    "        if len(all_summaries) < len(records):\n",
    "            for i in range(len(all_summaries), len(records)):\n",
    "                all_summaries.append(local_summarize_one(records[i]))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[LLM unavailable: {e}] Falling back to local summaries.\")\n",
    "        all_summaries = [local_summarize_one(r) for r in records]\n",
    "else:\n",
    "    print(\"[No OPENAI_API_KEY] Using local summaries.\")\n",
    "    all_summaries = [local_summarize_one(r) for r in records]\n",
    "\n",
    "# save\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "rows = to_rows_for_csv(records, all_summaries)\n",
    "pd.DataFrame(rows).to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
    "print(f\"Wrote {len(rows)} summaries -> {OUT_CSV}\")\n",
    "\n",
    "# show a preview\n",
    "for i in range(min(3, len(rows))):\n",
    "    print(f\"\\n[{rows[i]['patient_id']}]\\n{rows[i]['summary']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "944753e6-6714-4618-a7c4-159c8ad9b9b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xff in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m enc \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8-sig\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m load_dotenv(dotenv_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(ROOT\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.env\u001b[39m\u001b[38;5;124m\"\u001b[39m), override\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39menc):\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     13\u001b[0m             loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\dotenv\\main.py:342\u001b[0m, in \u001b[0;36mload_dotenv\u001b[1;34m(dotenv_path, stream, verbose, override, interpolate, encoding)\u001b[0m\n\u001b[0;32m    332\u001b[0m     dotenv_path \u001b[38;5;241m=\u001b[39m find_dotenv()\n\u001b[0;32m    334\u001b[0m dotenv \u001b[38;5;241m=\u001b[39m DotEnv(\n\u001b[0;32m    335\u001b[0m     dotenv_path\u001b[38;5;241m=\u001b[39mdotenv_path,\n\u001b[0;32m    336\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    340\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m    341\u001b[0m )\n\u001b[1;32m--> 342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dotenv\u001b[38;5;241m.\u001b[39mset_as_environment_variables()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\dotenv\\main.py:85\u001b[0m, in \u001b[0;36mDotEnv.set_as_environment_variables\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_as_environment_variables\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m     82\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    Load the current dotenv as system environment variable.\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdict():\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdict()\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\dotenv\\main.py:69\u001b[0m, in \u001b[0;36mDotEnv.dict\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     66\u001b[0m raw_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse()\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterpolate:\n\u001b[1;32m---> 69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dict \u001b[38;5;241m=\u001b[39m OrderedDict(resolve_variables(raw_values, override\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverride))\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dict \u001b[38;5;241m=\u001b[39m OrderedDict(raw_values)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\dotenv\\main.py:229\u001b[0m, in \u001b[0;36mresolve_variables\u001b[1;34m(values, override)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresolve_variables\u001b[39m(\n\u001b[0;32m    224\u001b[0m     values: Iterable[Tuple[\u001b[38;5;28mstr\u001b[39m, Optional[\u001b[38;5;28mstr\u001b[39m]]],\n\u001b[0;32m    225\u001b[0m     override: \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m    226\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Mapping[\u001b[38;5;28mstr\u001b[39m, Optional[\u001b[38;5;28mstr\u001b[39m]]:\n\u001b[0;32m    227\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m {}  \u001b[38;5;66;03m# type: Dict[str, Optional[str]]\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (name, value) \u001b[38;5;129;01min\u001b[39;00m values:\n\u001b[0;32m    230\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    231\u001b[0m             result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\dotenv\\main.py:77\u001b[0m, in \u001b[0;36mDotEnv.parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Tuple[\u001b[38;5;28mstr\u001b[39m, Optional[\u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_stream() \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[1;32m---> 77\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m mapping \u001b[38;5;129;01min\u001b[39;00m with_warn_for_invalid_lines(parse_stream(stream)):\n\u001b[0;32m     78\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m mapping\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m                 \u001b[38;5;28;01myield\u001b[39;00m mapping\u001b[38;5;241m.\u001b[39mkey, mapping\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\dotenv\\main.py:19\u001b[0m, in \u001b[0;36mwith_warn_for_invalid_lines\u001b[1;34m(mappings)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwith_warn_for_invalid_lines\u001b[39m(mappings: Iterator[Binding]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Binding]:\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m mapping \u001b[38;5;129;01min\u001b[39;00m mappings:\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mapping\u001b[38;5;241m.\u001b[39merror:\n\u001b[0;32m     21\u001b[0m             logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m     22\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython-dotenv could not parse statement starting at line \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     23\u001b[0m                 mapping\u001b[38;5;241m.\u001b[39moriginal\u001b[38;5;241m.\u001b[39mline,\n\u001b[0;32m     24\u001b[0m             )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\dotenv\\parser.py:180\u001b[0m, in \u001b[0;36mparse_stream\u001b[1;34m(stream)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_stream\u001b[39m(stream: IO[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Binding]:\n\u001b[1;32m--> 180\u001b[0m     reader \u001b[38;5;241m=\u001b[39m Reader(stream)\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m reader\u001b[38;5;241m.\u001b[39mhas_next():\n\u001b[0;32m    182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m parse_binding(reader)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\dotenv\\parser.py:71\u001b[0m, in \u001b[0;36mReader.__init__\u001b[1;34m(self, stream)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, stream: IO[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstring \u001b[38;5;241m=\u001b[39m stream\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition \u001b[38;5;241m=\u001b[39m Position\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmark \u001b[38;5;241m=\u001b[39m Position\u001b[38;5;241m.\u001b[39mstart()\n",
      "File \u001b[1;32m<frozen codecs>:322\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "ROOT = Path(r\"C:\\Users\\aayus\\Downloads\\emr-smart\")\n",
    "assert (ROOT/\".env\").exists(), \".env not found where expected\"\n",
    "\n",
    "# try multiple encodings in case an editor added a BOM\n",
    "loaded = False\n",
    "for enc in (\"utf-8\", \"utf-8-sig\"):\n",
    "    if load_dotenv(dotenv_path=str(ROOT/\".env\"), override=True, encoding=enc):\n",
    "        if os.getenv(\"OPENAI_API_KEY\"):\n",
    "            loaded = True\n",
    "            break\n",
    "assert loaded, \"Could not load OPENAI_API_KEY from .env\"\n",
    "\n",
    "key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(\"Loaded OPENAI_API_KEY:\", key[:10] + \"...\" if key else None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bd0c5cc-2bc2-41be-9e1d-5891c5bdf17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key detected? True | length: 164\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "k = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(\"Key detected?\" , bool(k), \"| length:\", len(k) if k else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cb10231-4fa5-4755-bfa6-567d71477d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 fusion records (target 10).\n",
      "[1] done.\n",
      "[2] done.\n",
      "[3] done.\n",
      "[4] done.\n",
      "[5] done.\n",
      "[6] done.\n",
      "[7] done.\n",
      "[8] done.\n",
      "[9] done.\n",
      "[10] done.\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:\\\\Users\\\\aayus\\\\Downloads\\\\emr-smart\\\\outputs\\\\summaries.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 95\u001b[0m\n\u001b[0;32m     92\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(SLEEP_BETWEEN)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# 5) write CSV + TXT\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(OUT_CSV, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     96\u001b[0m     w \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mDictWriter(f, fieldnames\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpatient_id\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     97\u001b[0m     w\u001b[38;5;241m.\u001b[39mwriteheader()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C:\\\\Users\\\\aayus\\\\Downloads\\\\emr-smart\\\\outputs\\\\summaries.csv'"
     ]
    }
   ],
   "source": [
    "# --- Summarize up to 10 patients with gentle pacing to avoid 429s ---\n",
    "\n",
    "import os, json, time, csv\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "\n",
    "ROOT = Path(r\"C:\\Users\\aayus\\Downloads\\emr-smart\")\n",
    "IN_PATH  = ROOT / \"outputs\" / \"fusion.ndjson\"\n",
    "OUT_CSV  = ROOT / \"outputs\" / \"summaries.csv\"\n",
    "OUT_TXT  = ROOT / \"outputs\" / \"summaries.txt\"\n",
    "\n",
    "# limits/pacing\n",
    "LIMIT = 10          # <= change this later (e.g., 25, 50) once it works\n",
    "SLEEP_BETWEEN = 3   # seconds between requests to be gentle\n",
    "\n",
    "# 1) sanity checks\n",
    "if not IN_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing {IN_PATH}. Run fuse_infer_generate.py first.\")\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\", \"\").strip()\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"OPENAI_API_KEY not visible in this kernel. Restart kernel or Windows session if needed.\")\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are a clinical assistant. Summarize EMR outputs for a clinician in 5–7 sentences. \"\n",
    "    \"Include patient context, current severity (numeric + label if present), 5-week trend, key risks, and alerts. \"\n",
    "    \"Use concise, neutral, non-alarming language and calibrated phrasing ('suggests', 'consider'); \"\n",
    "    \"avoid diagnoses or prescriptions; no PII beyond patient_id.\"\n",
    ")\n",
    "\n",
    "def build_user_prompt(rec: dict) -> str:\n",
    "    pid = rec.get(\"patient_id\", \"unknown\")\n",
    "    snap = rec.get(\"snapshot\") or {}\n",
    "    hist = rec.get(\"history\")  or {}\n",
    "    alerts = rec.get(\"alerts\") or []\n",
    "\n",
    "    return (\n",
    "        f\"Patient ID: {pid}\\n\"\n",
    "        f\"Current severity: {snap.get('severity_numeric')} ({snap.get('severity_label')})\\n\"\n",
    "        f\"Trend: {hist.get('trend')}\\n\"\n",
    "        f\"Class probabilities: {hist.get('proba')}\\n\"\n",
    "        f\"Alerts: {alerts}\\n\"\n",
    "        \"Write one concise paragraph for a clinician.\"\n",
    "    )\n",
    "\n",
    "# 2) load first N fusion rows\n",
    "fusion = []\n",
    "with open(IN_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= LIMIT: break\n",
    "        line=line.strip()\n",
    "        if not line: continue\n",
    "        fusion.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(fusion)} fusion records (target {LIMIT}).\")\n",
    "\n",
    "# 3) make outputs dir and open writers\n",
    "OUT_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "csv_rows = []\n",
    "txt_chunks = []\n",
    "\n",
    "def call_llm(user_prompt: str) -> str:\n",
    "    \"\"\"One gentle call to the API. On 429, return a clear placeholder.\"\"\"\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\",   \"content\": user_prompt},\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            max_tokens=220,  # short, to reduce costs/rate pressure\n",
    "        )\n",
    "        return (resp.choices[0].message.content or \"\").strip()\n",
    "    except Exception as e:\n",
    "        # If it's a rate limit or any other error, return a short stub\n",
    "        return f\"Summary unavailable (API error: {type(e).__name__}).\"\n",
    "\n",
    "# 4) loop with gentle pacing\n",
    "for i, rec in enumerate(fusion, start=1):\n",
    "    pid = rec.get(\"patient_id\", f\"row_{i}\")\n",
    "    prompt = build_user_prompt(rec)\n",
    "    text = call_llm(prompt)\n",
    "\n",
    "    # collect outputs\n",
    "    csv_rows.append({\"patient_id\": pid, \"summary\": text})\n",
    "    txt_chunks.append(f\"[{pid}]\\n{text}\\n\")\n",
    "\n",
    "    print(f\"[{pid}] done.\")\n",
    "    time.sleep(SLEEP_BETWEEN)\n",
    "\n",
    "# 5) write CSV + TXT\n",
    "with open(OUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"patient_id\",\"summary\"])\n",
    "    w.writeheader()\n",
    "    for r in csv_rows: w.writerow(r)\n",
    "\n",
    "with open(OUT_TXT, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(txt_chunks))\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" \", OUT_CSV)\n",
    "print(\" \", OUT_TXT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c356838c-8ce2-44f8-b966-9d08ab720a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 fusion records (target 10).\n",
      "[1] done.\n",
      "[2] done.\n",
      "[3] done.\n",
      "[4] done.\n",
      "[5] done.\n",
      "[6] done.\n",
      "[7] done.\n",
      "[8] done.\n",
      "[9] done.\n",
      "[10] done.\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:\\\\Users\\\\aayus\\\\Downloads\\\\emr-smart\\\\outputs\\\\summaries.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 95\u001b[0m\n\u001b[0;32m     92\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(SLEEP_BETWEEN)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# 5) write CSV + TXT\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(OUT_CSV, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     96\u001b[0m     w \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mDictWriter(f, fieldnames\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpatient_id\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     97\u001b[0m     w\u001b[38;5;241m.\u001b[39mwriteheader()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C:\\\\Users\\\\aayus\\\\Downloads\\\\emr-smart\\\\outputs\\\\summaries.csv'"
     ]
    }
   ],
   "source": [
    "# --- Summarize up to 10 patients with gentle pacing to avoid 429s ---\n",
    "\n",
    "import os, json, time, csv\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "\n",
    "ROOT = Path(r\"C:\\Users\\aayus\\Downloads\\emr-smart\")\n",
    "IN_PATH  = ROOT / \"outputs\" / \"fusion.ndjson\"\n",
    "OUT_CSV  = ROOT / \"outputs\" / \"summaries.csv\"\n",
    "OUT_TXT  = ROOT / \"outputs\" / \"summaries.txt\"\n",
    "\n",
    "# limits/pacing\n",
    "LIMIT = 10          # <= change this later (e.g., 25, 50) once it works\n",
    "SLEEP_BETWEEN = 3   # seconds between requests to be gentle\n",
    "\n",
    "# 1) sanity checks\n",
    "if not IN_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing {IN_PATH}. Run fuse_infer_generate.py first.\")\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\", \"\").strip()\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"OPENAI_API_KEY not visible in this kernel. Restart kernel or Windows session if needed.\")\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are a clinical assistant. Summarize EMR outputs for a clinician in 5–7 sentences. \"\n",
    "    \"Include patient context, current severity (numeric + label if present), 5-week trend, key risks, and alerts. \"\n",
    "    \"Use concise, neutral, non-alarming language and calibrated phrasing ('suggests', 'consider'); \"\n",
    "    \"avoid diagnoses or prescriptions; no PII beyond patient_id.\"\n",
    ")\n",
    "\n",
    "def build_user_prompt(rec: dict) -> str:\n",
    "    pid = rec.get(\"patient_id\", \"unknown\")\n",
    "    snap = rec.get(\"snapshot\") or {}\n",
    "    hist = rec.get(\"history\")  or {}\n",
    "    alerts = rec.get(\"alerts\") or []\n",
    "\n",
    "    return (\n",
    "        f\"Patient ID: {pid}\\n\"\n",
    "        f\"Current severity: {snap.get('severity_numeric')} ({snap.get('severity_label')})\\n\"\n",
    "        f\"Trend: {hist.get('trend')}\\n\"\n",
    "        f\"Class probabilities: {hist.get('proba')}\\n\"\n",
    "        f\"Alerts: {alerts}\\n\"\n",
    "        \"Write one concise paragraph for a clinician.\"\n",
    "    )\n",
    "\n",
    "# 2) load first N fusion rows\n",
    "fusion = []\n",
    "with open(IN_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= LIMIT: break\n",
    "        line=line.strip()\n",
    "        if not line: continue\n",
    "        fusion.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(fusion)} fusion records (target {LIMIT}).\")\n",
    "\n",
    "# 3) make outputs dir and open writers\n",
    "OUT_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "csv_rows = []\n",
    "txt_chunks = []\n",
    "\n",
    "def call_llm(user_prompt: str) -> str:\n",
    "    \"\"\"One gentle call to the API. On 429, return a clear placeholder.\"\"\"\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\",   \"content\": user_prompt},\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            max_tokens=220,  # short, to reduce costs/rate pressure\n",
    "        )\n",
    "        return (resp.choices[0].message.content or \"\").strip()\n",
    "    except Exception as e:\n",
    "        # If it's a rate limit or any other error, return a short stub\n",
    "        return f\"Summary unavailable (API error: {type(e).__name__}).\"\n",
    "\n",
    "# 4) loop with gentle pacing\n",
    "for i, rec in enumerate(fusion, start=1):\n",
    "    pid = rec.get(\"patient_id\", f\"row_{i}\")\n",
    "    prompt = build_user_prompt(rec)\n",
    "    text = call_llm(prompt)\n",
    "\n",
    "    # collect outputs\n",
    "    csv_rows.append({\"patient_id\": pid, \"summary\": text})\n",
    "    txt_chunks.append(f\"[{pid}]\\n{text}\\n\")\n",
    "\n",
    "    print(f\"[{pid}] done.\")\n",
    "    time.sleep(SLEEP_BETWEEN)\n",
    "\n",
    "# 5) write CSV + TXT\n",
    "with open(OUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"patient_id\",\"summary\"])\n",
    "    w.writeheader()\n",
    "    for r in csv_rows: w.writerow(r)\n",
    "\n",
    "with open(OUT_TXT, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(txt_chunks))\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" \", OUT_CSV)\n",
    "print(\" \", OUT_TXT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a81d9570-b27f-4fdf-ac60-84dcc114586b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 fusion records (target 10).\n",
      "[1] fallback used -> RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "[2] fallback used -> RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "[3] fallback used -> RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "[4] fallback used -> RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "[5] fallback used -> RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "[6] fallback used -> RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "[7] fallback used -> RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "[8] fallback used -> RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "[9] fallback used -> RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "[10] fallback used -> RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Wrote:\n",
      " - C:\\Users\\aayus\\Downloads\\emr-smart\\outputs\\summaries.csv\n",
      " - C:\\Users\\aayus\\Downloads\\emr-smart\\outputs\\summaries.txt\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# --- EMR Summaries: robust writer with 10-record cap and .env handling ---\n",
    "\n",
    "import os, json, csv, time, tempfile, shutil\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- paths ----\n",
    "ROOT = Path(r\"C:\\Users\\aayus\\Downloads\\emr-smart\")\n",
    "OUT_DIR = ROOT / \"outputs\"\n",
    "IN_PATH  = OUT_DIR / \"fusion.ndjson\"\n",
    "OUT_CSV  = OUT_DIR / \"summaries.csv\"\n",
    "OUT_TXT  = OUT_DIR / \"summaries.txt\"\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- load .env robustly (utf-8 or utf-8-sig) ----\n",
    "from dotenv import load_dotenv\n",
    "loaded = False\n",
    "for enc in (\"utf-8\", \"utf-8-sig\"):\n",
    "    try:\n",
    "        if load_dotenv(dotenv_path=str(ROOT/\".env\"), override=True, encoding=enc):\n",
    "            if os.getenv(\"OPENAI_API_KEY\"):\n",
    "                loaded = True\n",
    "                break\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "api_key = (os.getenv(\"OPENAI_API_KEY\") or \"\").strip()\n",
    "use_llm = bool(api_key)\n",
    "\n",
    "# ---- read up to 10 fusion records ----\n",
    "if not IN_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing input: {IN_PATH}. Run fuse_infer_generate.py first.\")\n",
    "\n",
    "records = []\n",
    "with open(IN_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        records.append(json.loads(line))\n",
    "        if len(records) >= 10:  # cap at 10 requests\n",
    "            break\n",
    "\n",
    "print(f\"Loaded {len(records)} fusion records (target 10).\")\n",
    "\n",
    "# ---- OpenAI client (optional) ----\n",
    "client = None\n",
    "if use_llm:\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are a clinical assistant. Summarize EMR outputs for a clinician in 5–7 sentences. \"\n",
    "    \"Start with quick patient context, describe current severity, 5-week trend, key abnormal vitals, and any alerts. \"\n",
    "    \"Use concise, neutral, non-alarming language and avoid definitive diagnoses; suggest clinical actions when appropriate. \"\n",
    "    \"Use calibrated language (e.g., 'suggests', 'consider'); do not include PII beyond patient_id.\"\n",
    ")\n",
    "\n",
    "def build_user_prompt(rec):\n",
    "    pid = rec.get(\"patient_id\", \"unknown\")\n",
    "    snapshot = rec.get(\"snapshot\") or {}\n",
    "    history  = rec.get(\"history\")  or {}\n",
    "    alerts   = rec.get(\"alerts\")   or []\n",
    "    return (\n",
    "        f\"Patient ID: {pid}\\n\"\n",
    "        f\"Current severity: {snapshot.get('severity_numeric')} ({snapshot.get('severity_label')})\\n\"\n",
    "        f\"Trend: {history.get('trend')}\\n\"\n",
    "        f\"Class probabilities: {history.get('proba')}\\n\"\n",
    "        f\"Alerts: {alerts}\\n\"\n",
    "        \"Summarize for a clinician (5–7 sentences).\"\n",
    "    )\n",
    "\n",
    "def local_template_summary(rec):\n",
    "    \"\"\"Fallback when no API key or rate-limited.\"\"\"\n",
    "    pid = rec.get(\"patient_id\",\"unknown\")\n",
    "    snapshot = rec.get(\"snapshot\") or {}\n",
    "    history  = rec.get(\"history\")  or {}\n",
    "    alerts   = rec.get(\"alerts\")   or []\n",
    "    sev = snapshot.get(\"severity_label\") or \"N/A\"\n",
    "    sevn = snapshot.get(\"severity_numeric\")\n",
    "    trend = history.get(\"trend\") or \"N/A\"\n",
    "    probs = history.get(\"proba\") or {}\n",
    "    alert_txt = \"; \".join(alerts) if alerts else \"No immediate alerts.\"\n",
    "    return (\n",
    "        f\"Patient {pid}: Current severity {sevn} ({sev}). \"\n",
    "        f\"Five-week trend classified as {trend}. \"\n",
    "        f\"Model confidence — Improving: {probs.get('Improving','-')}, Stable: {probs.get('Stable','-')}, Worsening: {probs.get('Worsening','-')}. \"\n",
    "        f\"Alerts: {alert_txt}. \"\n",
    "        \"Consider reviewing vitals evolution and corroborating with clinical notes; this is decision support, not a diagnosis.\"\n",
    "    )\n",
    "\n",
    "summaries = []\n",
    "\n",
    "# ---- generate summaries (with graceful fallback) ----\n",
    "for i, rec in enumerate(records, start=1):\n",
    "    try:\n",
    "        if client:\n",
    "            resp = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\",   \"content\": build_user_prompt(rec)},\n",
    "                ],\n",
    "                temperature=0.2,\n",
    "            )\n",
    "            text = (resp.choices[0].message.content or \"\").strip()\n",
    "        else:\n",
    "            text = local_template_summary(rec)\n",
    "        summaries.append({\"patient_id\": rec.get(\"patient_id\",\"unknown\"), \"summary\": text})\n",
    "        print(f\"[{i}] done.\")\n",
    "        # Tiny pause to be gentle with API, even for 10\n",
    "        time.sleep(0.3)\n",
    "    except Exception as e:\n",
    "        # fallback for rate limits or any other error\n",
    "        text = local_template_summary(rec)\n",
    "        summaries.append({\"patient_id\": rec.get(\"patient_id\",\"unknown\"),\n",
    "                          \"summary\": text + \" (Note: LLM fallback used due to API error.)\"})\n",
    "        print(f\"[{i}] fallback used -> {e.__class__.__name__}: {e}\")\n",
    "\n",
    "# ---- safe write helpers (atomic with fallback filename on Windows lock) ----\n",
    "def safe_write_text(path: Path, content: str) -> Path:\n",
    "    path = Path(path)\n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(\"w\", delete=False, dir=str(path.parent), encoding=\"utf-8\", newline=\"\") as tmp:\n",
    "            tmp.write(content)\n",
    "            tmp_name = tmp.name\n",
    "        try:\n",
    "            os.replace(tmp_name, path)  # atomic on Windows when possible\n",
    "            return path\n",
    "        except PermissionError:\n",
    "            # destination is locked, write timestamped file\n",
    "            ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            alt = path.with_name(f\"{path.stem}_{ts}{path.suffix}\")\n",
    "            shutil.move(tmp_name, alt)\n",
    "            return alt\n",
    "    except PermissionError:\n",
    "        # parent folder permission or file lock before temp write\n",
    "        ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        alt = path.with_name(f\"{path.stem}_{ts}{path.suffix}\")\n",
    "        with open(alt, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "            f.write(content)\n",
    "        return alt\n",
    "\n",
    "def safe_write_csv(path: Path, rows: list[dict], fieldnames: list[str]) -> Path:\n",
    "    path = Path(path)\n",
    "    # write to temp first\n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(\"w\", delete=False, dir=str(path.parent), encoding=\"utf-8\", newline=\"\") as tmp:\n",
    "            w = csv.DictWriter(tmp, fieldnames=fieldnames)\n",
    "            w.writeheader()\n",
    "            for r in rows:\n",
    "                w.writerow(r)\n",
    "            tmp_name = tmp.name\n",
    "        try:\n",
    "            os.replace(tmp_name, path)\n",
    "            return path\n",
    "        except PermissionError:\n",
    "            ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            alt = path.with_name(f\"{path.stem}_{ts}{path.suffix}\")\n",
    "            shutil.move(tmp_name, alt)\n",
    "            return alt\n",
    "    except PermissionError:\n",
    "        # parent or temp blocked — write to timestamped directly\n",
    "        ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        alt = path.with_name(f\"{path.stem}_{ts}{path.suffix}\")\n",
    "        with open(alt, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "            w = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            w.writeheader()\n",
    "            for r in rows:\n",
    "                w.writerow(r)\n",
    "        return alt\n",
    "\n",
    "# ---- write outputs (atomic, with fallback) ----\n",
    "csv_path = safe_write_csv(OUT_CSV, summaries, fieldnames=[\"patient_id\",\"summary\"])\n",
    "txt_blob = \"\\n\\n\".join(f\"[{s['patient_id']}]\\n{s['summary']}\" for s in summaries)\n",
    "txt_path = safe_write_text(OUT_TXT, txt_blob)\n",
    "\n",
    "print(\"Wrote:\")\n",
    "print(\" -\", csv_path)\n",
    "print(\" -\", txt_path)\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6332fdb4-5dcf-4ad4-a8f1-e0cae3ef366d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting google-generativeai\n",
      "  Downloading google_generativeai-0.8.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: python-dotenv in c:\\programdata\\anaconda3\\lib\\site-packages (0.21.0)\n",
      "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n",
      "  Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting google-api-core (from google-generativeai)\n",
      "  Downloading google_api_core-2.25.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-api-python-client (from google-generativeai)\n",
      "  Downloading google_api_python_client-2.182.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting google-auth>=2.15.0 (from google-generativeai)\n",
      "  Downloading google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: protobuf in c:\\programdata\\anaconda3\\lib\\site-packages (from google-generativeai) (4.25.3)\n",
      "Requirement already satisfied: pydantic in c:\\programdata\\anaconda3\\lib\\site-packages (from google-generativeai) (2.8.2)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from google-generativeai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from google-generativeai) (4.11.0)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core->google-generativeai)\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting httplib2<1.0.0,>=0.19.0 (from google-api-python-client->google-generativeai)\n",
      "  Downloading httplib2-0.31.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai)\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google-generativeai)\n",
      "  Downloading uritemplate-4.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic->google-generativeai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic->google-generativeai) (2.20.1)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\aayus\\appdata\\roaming\\python\\python312\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.73.1)\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading grpcio_status-1.75.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: pyparsing<4,>=3.0.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.1.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2024.8.30)\n",
      "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading grpcio_status-1.74.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.73.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.73.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.72.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.72.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.71.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting protobuf (from google-generativeai)\n",
      "  Downloading protobuf-5.29.5-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Downloading google_generativeai-0.8.5-py3-none-any.whl (155 kB)\n",
      "Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 0.5/1.3 MB 17.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 4.6 MB/s eta 0:00:00\n",
      "Downloading google_api_core-2.25.1-py3-none-any.whl (160 kB)\n",
      "Downloading google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "Downloading google_api_python_client-2.182.0-py3-none-any.whl (14.2 MB)\n",
      "   ---------------------------------------- 0.0/14.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/14.2 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.0/14.2 MB 2.6 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 1.6/14.2 MB 2.5 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 2.4/14.2 MB 2.9 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 2.6/14.2 MB 2.6 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 3.4/14.2 MB 2.8 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 4.2/14.2 MB 2.9 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 5.0/14.2 MB 3.0 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 5.8/14.2 MB 3.1 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 6.6/14.2 MB 3.1 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 6.8/14.2 MB 3.1 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 6.8/14.2 MB 3.1 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 7.1/14.2 MB 2.7 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 7.3/14.2 MB 2.6 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 7.9/14.2 MB 2.6 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 8.9/14.2 MB 2.6 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 9.4/14.2 MB 2.7 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 10.2/14.2 MB 2.7 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 11.0/14.2 MB 2.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 11.8/14.2 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 12.3/14.2 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 12.3/14.2 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.8/14.2 MB 2.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 13.6/14.2 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.2/14.2 MB 2.7 MB/s eta 0:00:00\n",
      "Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Downloading httplib2-0.31.0-py3-none-any.whl (91 kB)\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading uritemplate-4.2.0-py3-none-any.whl (11 kB)\n",
      "Downloading grpcio_status-1.71.2-py3-none-any.whl (14 kB)\n",
      "Downloading protobuf-5.29.5-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Installing collected packages: uritemplate, rsa, protobuf, httplib2, proto-plus, googleapis-common-protos, google-auth, grpcio-status, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google-generativeai\n",
      "Successfully installed google-ai-generativelanguage-0.6.15 google-api-core-2.25.1 google-api-python-client-2.182.0 google-auth-2.40.3 google-auth-httplib2-0.2.0 google-generativeai-0.8.5 googleapis-common-protos-1.70.0 grpcio-status-1.71.2 httplib2-0.31.0 proto-plus-1.26.1 protobuf-5.29.5 rsa-4.9.1 uritemplate-4.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts pyrsa-decrypt.exe, pyrsa-encrypt.exe, pyrsa-keygen.exe, pyrsa-priv2pub.exe, pyrsa-sign.exe and pyrsa-verify.exe are installed in 'C:\\Users\\aayus\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "pip install google-generativeai python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d3d7481-8ce3-4bfb-a2f7-37044ef98594",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Could not read GEMINI_API_KEY from C:\\Users\\aayus\\Downloads\\emr-smart\\.env. Create a UTF-8 .env with:\nGEMINI_API_KEY=AIzaSyDkhQyzvDh4WCN5UGAvuYQFu3RpDsUKlqM",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m loaded:\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not read GEMINI_API_KEY from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mENV_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreate a UTF-8 .env with:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGEMINI_API_KEY=AIzaSyDkhQyzvDh4WCN5UGAvuYQFu3RpDsUKlqM\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     35\u001b[0m     )\n\u001b[0;32m     37\u001b[0m GEMINI_API_KEY \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGEMINI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     38\u001b[0m GEMINI_MODEL   \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGEMINI_MODEL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-2.0-flash\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()  \u001b[38;5;66;03m# change to 'gemini-2.5-flash' if your key supports it\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Could not read GEMINI_API_KEY from C:\\Users\\aayus\\Downloads\\emr-smart\\.env. Create a UTF-8 .env with:\nGEMINI_API_KEY=AIzaSyDkhQyzvDh4WCN5UGAvuYQFu3RpDsUKlqM"
     ]
    }
   ],
   "source": [
    "# --- EMR LLM Summaries via Google Gemini (10 requests) ---\n",
    "# Requires: pip install google-generativeai python-dotenv\n",
    "# Files:\n",
    "#   - Input : C:\\Users\\aayus\\Downloads\\emr-smart\\outputs\\fusion.ndjson\n",
    "#   - Output: C:\\Users\\aayus\\Downloads\\emr-smart\\outputs\\summaries_gemini.csv / .txt\n",
    "\n",
    "import os, json, time, csv\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ---------- paths ----------\n",
    "ROOT = Path(r\"C:\\Users\\aayus\\Downloads\\emr-smart\")\n",
    "OUT_DIR = ROOT / \"outputs\"\n",
    "IN_PATH  = OUT_DIR / \"fusion.ndjson\"\n",
    "CSV_PATH = OUT_DIR / \"summaries_gemini.csv\"\n",
    "TXT_PATH = OUT_DIR / \"summaries_gemini.txt\"\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- load .env robustly ----------\n",
    "ENV_PATH = ROOT / \".env\"\n",
    "loaded = False\n",
    "for enc in (\"utf-8\", \"utf-8-sig\"):\n",
    "    try:\n",
    "        if load_dotenv(dotenv_path=str(ENV_PATH), override=True, encoding=enc):\n",
    "            if os.getenv(\"GEMINI_API_KEY\"):\n",
    "                loaded = True\n",
    "                break\n",
    "    except Exception:\n",
    "        pass\n",
    "if not loaded:\n",
    "    raise RuntimeError(\n",
    "        f\"Could not read GEMINI_API_KEY from {ENV_PATH}. \"\n",
    "        \"Create a UTF-8 .env with:\\nGEMINI_API_KEY=AIzaSyDkhQyzvDh4WCN5UGAvuYQFu3RpDsUKlqM\"\n",
    "    )\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\", \"\").strip()\n",
    "GEMINI_MODEL   = os.getenv(\"GEMINI_MODEL\", \"gemini-2.0-flash\").strip()  # change to 'gemini-2.5-flash' if your key supports it\n",
    "\n",
    "# ---------- import & configure gemini ----------\n",
    "import google.generativeai as genai\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# Build model with a system instruction\n",
    "SYSTEM = (\n",
    "    \"You are a clinical assistant. Summarize EMR outputs for a clinician in 5–7 sentences. \"\n",
    "    \"Start with patient context, describe current severity (with label), 5-week trend, any notable vitals, and the alerts. \"\n",
    "    \"Use neutral, calibrated language (e.g., 'suggests', 'consider'), do not diagnose or prescribe, and avoid PII beyond patient_id. \"\n",
    "    \"End with: 'This supports decisions and is not a diagnosis.'\"\n",
    ")\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=GEMINI_MODEL,\n",
    "    system_instruction=SYSTEM\n",
    ")\n",
    "\n",
    "# ---------- read fusion records ----------\n",
    "if not IN_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing input: {IN_PATH}. Run your fuse_infer_generate step first.\")\n",
    "\n",
    "records = []\n",
    "with open(IN_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            records.append(json.loads(line))\n",
    "        except Exception:\n",
    "            # skip malformed\n",
    "            continue\n",
    "\n",
    "# limit to first 10\n",
    "BATCH_LIMIT = 10\n",
    "records = records[:BATCH_LIMIT]\n",
    "print(f\"Loaded {len(records)} fusion records (target {BATCH_LIMIT}).\")\n",
    "\n",
    "# ---------- helper: one summary with retries + fallback ----------\n",
    "def summarize_one(rec, attempt=0, max_attempts=3):\n",
    "    pid = rec.get(\"patient_id\", \"unknown\")\n",
    "    snapshot = rec.get(\"snapshot\") or {}\n",
    "    history  = rec.get(\"history\")  or {}\n",
    "    alerts   = rec.get(\"alerts\")   or []\n",
    "\n",
    "    user_prompt = (\n",
    "        f\"Patient ID: {pid}\\n\"\n",
    "        f\"Current severity: {snapshot.get('severity_numeric')} ({snapshot.get('severity_label')})\\n\"\n",
    "        f\"Trend: {history.get('trend')}\\n\"\n",
    "        f\"Class probabilities: {history.get('proba')}\\n\"\n",
    "        f\"Alerts: {alerts}\\n\"\n",
    "        \"Summarize for a clinician.\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        resp = model.generate_content(\n",
    "            user_prompt,\n",
    "            generation_config={\n",
    "                \"temperature\": 0.2,\n",
    "                \"max_output_tokens\": 300\n",
    "            }\n",
    "        )\n",
    "        text = (resp.text or \"\").strip()\n",
    "        if not text:\n",
    "            raise RuntimeError(\"Empty response\")\n",
    "        return text, None\n",
    "    except Exception as e:\n",
    "        err = f\"{type(e).__name__}: {e}\"\n",
    "        # basic backoff\n",
    "        if attempt + 1 < max_attempts:\n",
    "            time.sleep(1.5 * (attempt + 1))\n",
    "            return summarize_one(rec, attempt=attempt+1, max_attempts=max_attempts)\n",
    "        # fallback summary if API fails\n",
    "        snapshot_label = snapshot.get(\"severity_label\") or \"Unknown\"\n",
    "        trend_label    = history.get(\"trend\") or \"Unknown\"\n",
    "        proba_str      = history.get(\"proba\") or {}\n",
    "        alert_str      = \"; \".join(alerts) if alerts else \"None\"\n",
    "        fallback = (\n",
    "            f\"Patient {pid}: Current severity appears '{snapshot_label}'. \"\n",
    "            f\"Trend over 5 weeks: {trend_label}. \"\n",
    "            f\"Class probabilities: {proba_str}. \"\n",
    "            f\"Alerts: {alert_str}. \"\n",
    "            \"Overall summary generated without LLM due to API limits. \"\n",
    "            \"This supports decisions and is not a diagnosis.\"\n",
    "        )\n",
    "        return fallback, err\n",
    "\n",
    "# ---------- run & collect ----------\n",
    "rows = []\n",
    "for i, rec in enumerate(records, start=1):\n",
    "    text, err = summarize_one(rec)\n",
    "    if err:\n",
    "        print(f\"[{i}] fallback used -> {err}\")\n",
    "    else:\n",
    "        print(f\"[{i}] done.\")\n",
    "    rows.append({\"patient_id\": rec.get(\"patient_id\", \"unknown\"), \"summary\": text})\n",
    "\n",
    "# ---------- write outputs (CSV + TXT) ----------\n",
    "# Use temp files to avoid PermissionError if the file was left open elsewhere\n",
    "csv_tmp = OUT_DIR / (CSV_PATH.stem + \".tmp.csv\")\n",
    "txt_tmp = OUT_DIR / (TXT_PATH.stem + \".tmp.txt\")\n",
    "\n",
    "# CSV\n",
    "with open(csv_tmp, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"patient_id\",\"summary\"])\n",
    "    w.writeheader()\n",
    "    w.writerows(rows)\n",
    "# TXT\n",
    "with open(txt_tmp, \"w\", encoding=\"utf-8\") as f:\n",
    "    for r in rows:\n",
    "        f.write(f\"[{r['patient_id']}]\\n{r['summary']}\\n\\n\")\n",
    "\n",
    "# Replace old files atomically\n",
    "if CSV_PATH.exists():\n",
    "    CSV_PATH.unlink(missing_ok=True)\n",
    "if TXT_PATH.exists():\n",
    "    TXT_PATH.unlink(missing_ok=True)\n",
    "csv_tmp.replace(CSV_PATH)\n",
    "txt_tmp.replace(TXT_PATH)\n",
    "\n",
    "print(\"Wrote:\")\n",
    "print(\" \", CSV_PATH)\n",
    "print(\" \", TXT_PATH)\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07e54d50-8f05-45b8-9606-bca5c9e2dfe3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Could not read GEMINI_API_KEY from C:\\Users\\aayus\\Downloads\\emr-smart\\.env and none set in environment.\nFix by creating a UTF-8 .env with:\nGEMINI_API_KEY=AIza...your_key\nGEMINI_MODEL=gemini-2.5-flash\nOr set it for this session: os.environ['GEMINI_API_KEY']='AIza...'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 75\u001b[0m\n\u001b[0;32m     73\u001b[0m ok, how \u001b[38;5;241m=\u001b[39m _try_load_env()\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ok:\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not read GEMINI_API_KEY from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mENV_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and none set in environment.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     77\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFix by creating a UTF-8 .env with:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGEMINI_API_KEY=AIza...your_key\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGEMINI_MODEL=gemini-2.5-flash\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOr set it for this session: os.environ[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGEMINI_API_KEY\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAIza...\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     81\u001b[0m     )\n\u001b[0;32m     83\u001b[0m GEMINI_API_KEY \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGEMINI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     84\u001b[0m GEMINI_MODEL   \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGEMINI_MODEL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-2.5-flash\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Could not read GEMINI_API_KEY from C:\\Users\\aayus\\Downloads\\emr-smart\\.env and none set in environment.\nFix by creating a UTF-8 .env with:\nGEMINI_API_KEY=AIza...your_key\nGEMINI_MODEL=gemini-2.5-flash\nOr set it for this session: os.environ['GEMINI_API_KEY']='AIza...'\n"
     ]
    }
   ],
   "source": [
    "# --- EMR LLM Summaries via Google Gemini (robust .env, 10 requests) ---\n",
    "# Requires: pip install google-generativeai python-dotenv\n",
    "\n",
    "import os, json, time, csv, io, sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ---------- paths ----------\n",
    "ROOT = Path(r\"C:\\Users\\aayus\\Downloads\\emr-smart\")\n",
    "OUT_DIR = ROOT / \"outputs\"\n",
    "IN_PATH  = OUT_DIR / \"fusion.ndjson\"\n",
    "CSV_PATH = OUT_DIR / \"summaries_gemini.csv\"\n",
    "TXT_PATH = OUT_DIR / \"summaries_gemini.txt\"\n",
    "ENV_PATH = ROOT / \".env\"\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- OPTIONAL: hardcode fallback (last resort) ----------\n",
    "# If you want to bypass .env entirely, uncomment the next line and paste your key:\n",
    "# os.environ[\"GEMINI_API_KEY\"] = \"AIza...YOUR_KEY...\"\n",
    "\n",
    "def _try_load_env():\n",
    "    \"\"\"Load .env with multiple encodings; fall back to manual parse if needed.\"\"\"\n",
    "    # 1) Already set in environment?\n",
    "    if os.getenv(\"GEMINI_API_KEY\"):\n",
    "        return True, \"envvar-present\"\n",
    "\n",
    "    # 2) Try python-dotenv with utf-8 and utf-8-sig\n",
    "    if ENV_PATH.exists():\n",
    "        for enc in (\"utf-8\", \"utf-8-sig\"):\n",
    "            try:\n",
    "                ok = load_dotenv(dotenv_path=str(ENV_PATH), override=True, encoding=enc)\n",
    "                if ok and os.getenv(\"GEMINI_API_KEY\"):\n",
    "                    return True, f\"dotenv-{enc}\"\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # 3) Manual parse (handles BOM/odd encodings)\n",
    "        try:\n",
    "            with open(ENV_PATH, \"rb\") as fb:\n",
    "                raw = fb.read()\n",
    "            # decode forgivingly\n",
    "            text = raw.decode(\"utf-8-sig\", errors=\"ignore\")\n",
    "            key = None\n",
    "            for line in text.splitlines():\n",
    "                s = line.strip()\n",
    "                if not s or s.startswith(\"#\"):\n",
    "                    continue\n",
    "                if s.upper().startswith(\"GEMINI_API_KEY=\"):\n",
    "                    key = s.split(\"=\", 1)[1].strip()\n",
    "                    # strip optional surrounding quotes\n",
    "                    if (key.startswith('\"') and key.endswith('\"')) or (key.startswith(\"'\") and key.endswith(\"'\")):\n",
    "                        key = key[1:-1]\n",
    "                    break\n",
    "            if key:\n",
    "                os.environ[\"GEMINI_API_KEY\"] = key\n",
    "                # optional model line\n",
    "                for line in text.splitlines():\n",
    "                    s = line.strip()\n",
    "                    if s.upper().startswith(\"GEMINI_MODEL=\"):\n",
    "                        val = s.split(\"=\", 1)[1].strip()\n",
    "                        if (val.startswith('\"') and val.endswith('\"')) or (val.startswith(\"'\") and val.endswith(\"'\")):\n",
    "                            val = val[1:-1]\n",
    "                        os.environ[\"GEMINI_MODEL\"] = val\n",
    "                        break\n",
    "                return True, \"manual-parse\"\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 4) Nothing worked\n",
    "    return False, \"not-found-or-unreadable\"\n",
    "\n",
    "ok, how = _try_load_env()\n",
    "if not ok:\n",
    "    raise RuntimeError(\n",
    "        f\"Could not read GEMINI_API_KEY from {ENV_PATH} and none set in environment.\\n\"\n",
    "        \"Fix by creating a UTF-8 .env with:\\n\"\n",
    "        \"GEMINI_API_KEY=AIza...your_key\\n\"\n",
    "        \"GEMINI_MODEL=gemini-2.5-flash\\n\"\n",
    "        \"Or set it for this session: os.environ['GEMINI_API_KEY']='AIza...'\\n\"\n",
    "    )\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\", \"\").strip()\n",
    "GEMINI_MODEL   = os.getenv(\"GEMINI_MODEL\", \"gemini-2.5-flash\").strip()\n",
    "if not GEMINI_API_KEY:\n",
    "    raise RuntimeError(\"GEMINI_API_KEY resolved empty after loading. Check your .env formatting (no quotes).\")\n",
    "\n",
    "print(f\"[.env] load mode: {how}; model={GEMINI_MODEL}\")\n",
    "\n",
    "# ---------- import & configure gemini ----------\n",
    "try:\n",
    "    import google.generativeai as genai\n",
    "except Exception:\n",
    "    raise RuntimeError(\"Missing dependency. Run: pip install google-generativeai python-dotenv\")\n",
    "\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "SYSTEM = (\n",
    "    \"You are a clinical assistant. Summarize EMR outputs for a clinician in 5–7 sentences. \"\n",
    "    \"Start with patient context, describe current severity (with label), 5-week trend, any notable vitals, and the alerts. \"\n",
    "    \"Use neutral, calibrated language (e.g., 'suggests', 'consider'), do not diagnose or prescribe, and avoid PII beyond patient_id. \"\n",
    "    \"End with: 'This supports decisions and is not a diagnosis.'\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=GEMINI_MODEL,\n",
    "        system_instruction=SYSTEM\n",
    "    )\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Gemini model init failed: {e}\")\n",
    "\n",
    "# ---------- read fusion records ----------\n",
    "if not IN_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing input: {IN_PATH}. Run your fuse_infer_generate step first.\")\n",
    "\n",
    "records = []\n",
    "with open(IN_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            records.append(json.loads(line))\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "BATCH_LIMIT = 10\n",
    "records = records[:BATCH_LIMIT]\n",
    "print(f\"Loaded {len(records)} fusion records (target {BATCH_LIMIT}).\")\n",
    "\n",
    "# ---------- summarize helper ----------\n",
    "def summarize_one(rec, attempt=0, max_attempts=3):\n",
    "    pid = rec.get(\"patient_id\", \"unknown\")\n",
    "    snapshot = rec.get(\"snapshot\") or {}\n",
    "    history  = rec.get(\"history\")  or {}\n",
    "    alerts   = rec.get(\"alerts\")   or []\n",
    "\n",
    "    user_prompt = (\n",
    "        f\"Patient ID: {pid}\\n\"\n",
    "        f\"Current severity: {snapshot.get('severity_numeric')} ({snapshot.get('severity_label')})\\n\"\n",
    "        f\"Trend: {history.get('trend')}\\n\"\n",
    "        f\"Class probabilities: {history.get('proba')}\\n\"\n",
    "        f\"Alerts: {alerts}\\n\"\n",
    "        \"Summarize for a clinician.\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        resp = model.generate_content(\n",
    "            user_prompt,\n",
    "            generation_config={\n",
    "                \"temperature\": 0.2,\n",
    "                \"max_output_tokens\": 300\n",
    "            }\n",
    "        )\n",
    "        text = (resp.text or \"\").strip()\n",
    "        if not text:\n",
    "            raise RuntimeError(\"Empty response\")\n",
    "        return text, None\n",
    "    except Exception as e:\n",
    "        err = f\"{type(e).__name__}: {e}\"\n",
    "        if attempt + 1 < max_attempts:\n",
    "            time.sleep(1.2 * (attempt + 1))\n",
    "            return summarize_one(rec, attempt=attempt+1, max_attempts=max_attempts)\n",
    "        # fallback\n",
    "        snapshot_label = snapshot.get(\"severity_label\") or \"Unknown\"\n",
    "        trend_label    = history.get(\"trend\") or \"Unknown\"\n",
    "        proba_str      = history.get(\"proba\") or {}\n",
    "        alert_str      = \"; \".join(alerts) if alerts else \"None\"\n",
    "        fallback = (\n",
    "            f\"Patient {pid}: Current severity appears '{snapshot_label}'. \"\n",
    "            f\"Trend over 5 weeks: {trend_label}. \"\n",
    "            f\"Class probabilities: {proba_str}. \"\n",
    "            f\"Alerts: {alert_str}. \"\n",
    "            \"Overall summary generated without LLM due to API limits. \"\n",
    "            \"This supports decisions and is not a diagnosis.\"\n",
    "        )\n",
    "        return fallback, err\n",
    "\n",
    "# ---------- run 10 requests ----------\n",
    "rows = []\n",
    "for i, rec in enumerate(records, start=1):\n",
    "    text, err = summarize_one(rec)\n",
    "    if err:\n",
    "        print(f\"[{i}] fallback used -> {err}\")\n",
    "    else:\n",
    "        print(f\"[{i}] done.\")\n",
    "    rows.append({\"patient_id\": rec.get(\"patient_id\", \"unknown\"), \"summary\": text})\n",
    "\n",
    "# ---------- write outputs (atomic replace) ----------\n",
    "csv_tmp = OUT_DIR / (CSV_PATH.stem + \".tmp.csv\")\n",
    "txt_tmp = OUT_DIR / (TXT_PATH.stem + \".tmp.txt\")\n",
    "\n",
    "with open(csv_tmp, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"patient_id\",\"summary\"])\n",
    "    w.writeheader()\n",
    "    w.writerows(rows)\n",
    "\n",
    "with open(txt_tmp, \"w\", encoding=\"utf-8\") as f:\n",
    "    for r in rows:\n",
    "        f.write(f\"[{r['patient_id']}]\\n{r['summary']}\\n\\n\")\n",
    "\n",
    "if CSV_PATH.exists(): CSV_PATH.unlink(missing_ok=True)\n",
    "if TXT_PATH.exists(): TXT_PATH.unlink(missing_ok=True)\n",
    "csv_tmp.replace(CSV_PATH)\n",
    "txt_tmp.replace(TXT_PATH)\n",
    "\n",
    "print(\"Wrote:\")\n",
    "print(\" \", CSV_PATH)\n",
    "print(\" \", TXT_PATH)\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3daac461-bdc2-4a16-99e7-86f17971375f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Could not read GEMINI_API_KEY from C:\\Users\\aayus\\Downloads\\emr-smart\\.env and none set in environment.\nFix by creating a UTF-8 .env with:\nGEMINI_API_KEY=AIza...your_key\nGEMINI_MODEL=gemini-2.5-flash\nOr set it for this session: os.environ['GEMINI_API_KEY']='AIza...'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 75\u001b[0m\n\u001b[0;32m     73\u001b[0m ok, how \u001b[38;5;241m=\u001b[39m _try_load_env()\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ok:\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not read GEMINI_API_KEY from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mENV_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and none set in environment.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     77\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFix by creating a UTF-8 .env with:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGEMINI_API_KEY=AIza...your_key\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGEMINI_MODEL=gemini-2.5-flash\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOr set it for this session: os.environ[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGEMINI_API_KEY\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAIza...\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     81\u001b[0m     )\n\u001b[0;32m     83\u001b[0m GEMINI_API_KEY \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGEMINI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     84\u001b[0m GEMINI_MODEL   \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGEMINI_MODEL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-2.5-flash\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Could not read GEMINI_API_KEY from C:\\Users\\aayus\\Downloads\\emr-smart\\.env and none set in environment.\nFix by creating a UTF-8 .env with:\nGEMINI_API_KEY=AIza...your_key\nGEMINI_MODEL=gemini-2.5-flash\nOr set it for this session: os.environ['GEMINI_API_KEY']='AIza...'\n"
     ]
    }
   ],
   "source": [
    "# --- EMR LLM Summaries via Google Gemini (robust .env, 10 requests) ---\n",
    "# Requires: pip install google-generativeai python-dotenv\n",
    "\n",
    "import os, json, time, csv, io, sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ---------- paths ----------\n",
    "ROOT = Path(r\"C:\\Users\\aayus\\Downloads\\emr-smart\")\n",
    "OUT_DIR = ROOT / \"outputs\"\n",
    "IN_PATH  = OUT_DIR / \"fusion.ndjson\"\n",
    "CSV_PATH = OUT_DIR / \"summaries_gemini.csv\"\n",
    "TXT_PATH = OUT_DIR / \"summaries_gemini.txt\"\n",
    "ENV_PATH = ROOT / \".env\"\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- OPTIONAL: hardcode fallback (last resort) ----------\n",
    "# If you want to bypass .env entirely, uncomment the next line and paste your key:\n",
    "# os.environ[\"GEMINI_API_KEY\"] = \"AIza...YOUR_KEY...\"\n",
    "\n",
    "def _try_load_env():\n",
    "    \"\"\"Load .env with multiple encodings; fall back to manual parse if needed.\"\"\"\n",
    "    # 1) Already set in environment?\n",
    "    if os.getenv(\"GEMINI_API_KEY\"):\n",
    "        return True, \"envvar-present\"\n",
    "\n",
    "    # 2) Try python-dotenv with utf-8 and utf-8-sig\n",
    "    if ENV_PATH.exists():\n",
    "        for enc in (\"utf-8\", \"utf-8-sig\"):\n",
    "            try:\n",
    "                ok = load_dotenv(dotenv_path=str(ENV_PATH), override=True, encoding=enc)\n",
    "                if ok and os.getenv(\"GEMINI_API_KEY\"):\n",
    "                    return True, f\"dotenv-{enc}\"\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # 3) Manual parse (handles BOM/odd encodings)\n",
    "        try:\n",
    "            with open(ENV_PATH, \"rb\") as fb:\n",
    "                raw = fb.read()\n",
    "            # decode forgivingly\n",
    "            text = raw.decode(\"utf-8-sig\", errors=\"ignore\")\n",
    "            key = None\n",
    "            for line in text.splitlines():\n",
    "                s = line.strip()\n",
    "                if not s or s.startswith(\"#\"):\n",
    "                    continue\n",
    "                if s.upper().startswith(\"GEMINI_API_KEY=\"):\n",
    "                    key = s.split(\"=\", 1)[1].strip()\n",
    "                    # strip optional surrounding quotes\n",
    "                    if (key.startswith('\"') and key.endswith('\"')) or (key.startswith(\"'\") and key.endswith(\"'\")):\n",
    "                        key = key[1:-1]\n",
    "                    break\n",
    "            if key:\n",
    "                os.environ[\"GEMINI_API_KEY\"] = key\n",
    "                # optional model line\n",
    "                for line in text.splitlines():\n",
    "                    s = line.strip()\n",
    "                    if s.upper().startswith(\"GEMINI_MODEL=\"):\n",
    "                        val = s.split(\"=\", 1)[1].strip()\n",
    "                        if (val.startswith('\"') and val.endswith('\"')) or (val.startswith(\"'\") and val.endswith(\"'\")):\n",
    "                            val = val[1:-1]\n",
    "                        os.environ[\"GEMINI_MODEL\"] = val\n",
    "                        break\n",
    "                return True, \"manual-parse\"\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 4) Nothing worked\n",
    "    return False, \"not-found-or-unreadable\"\n",
    "\n",
    "ok, how = _try_load_env()\n",
    "if not ok:\n",
    "    raise RuntimeError(\n",
    "        f\"Could not read GEMINI_API_KEY from {ENV_PATH} and none set in environment.\\n\"\n",
    "        \"Fix by creating a UTF-8 .env with:\\n\"\n",
    "        \"GEMINI_API_KEY=AIza...your_key\\n\"\n",
    "        \"GEMINI_MODEL=gemini-2.5-flash\\n\"\n",
    "        \"Or set it for this session: os.environ['GEMINI_API_KEY']='AIza...'\\n\"\n",
    "    )\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\", \"\").strip()\n",
    "GEMINI_MODEL   = os.getenv(\"GEMINI_MODEL\", \"gemini-2.5-flash\").strip()\n",
    "if not GEMINI_API_KEY:\n",
    "    raise RuntimeError(\"GEMINI_API_KEY resolved empty after loading. Check your .env formatting (no quotes).\")\n",
    "\n",
    "print(f\"[.env] load mode: {how}; model={GEMINI_MODEL}\")\n",
    "\n",
    "# ---------- import & configure gemini ----------\n",
    "try:\n",
    "    import google.generativeai as genai\n",
    "except Exception:\n",
    "    raise RuntimeError(\"Missing dependency. Run: pip install google-generativeai python-dotenv\")\n",
    "\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "SYSTEM = (\n",
    "    \"You are a clinical assistant. Summarize EMR outputs for a clinician in 5–7 sentences. \"\n",
    "    \"Start with patient context, describe current severity (with label), 5-week trend, any notable vitals, and the alerts. \"\n",
    "    \"Use neutral, calibrated language (e.g., 'suggests', 'consider'), do not diagnose or prescribe, and avoid PII beyond patient_id. \"\n",
    "    \"End with: 'This supports decisions and is not a diagnosis.'\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=GEMINI_MODEL,\n",
    "        system_instruction=SYSTEM\n",
    "    )\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Gemini model init failed: {e}\")\n",
    "\n",
    "# ---------- read fusion records ----------\n",
    "if not IN_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing input: {IN_PATH}. Run your fuse_infer_generate step first.\")\n",
    "\n",
    "records = []\n",
    "with open(IN_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            records.append(json.loads(line))\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "BATCH_LIMIT = 10\n",
    "records = records[:BATCH_LIMIT]\n",
    "print(f\"Loaded {len(records)} fusion records (target {BATCH_LIMIT}).\")\n",
    "\n",
    "# ---------- summarize helper ----------\n",
    "def summarize_one(rec, attempt=0, max_attempts=3):\n",
    "    pid = rec.get(\"patient_id\", \"unknown\")\n",
    "    snapshot = rec.get(\"snapshot\") or {}\n",
    "    history  = rec.get(\"history\")  or {}\n",
    "    alerts   = rec.get(\"alerts\")   or []\n",
    "\n",
    "    user_prompt = (\n",
    "        f\"Patient ID: {pid}\\n\"\n",
    "        f\"Current severity: {snapshot.get('severity_numeric')} ({snapshot.get('severity_label')})\\n\"\n",
    "        f\"Trend: {history.get('trend')}\\n\"\n",
    "        f\"Class probabilities: {history.get('proba')}\\n\"\n",
    "        f\"Alerts: {alerts}\\n\"\n",
    "        \"Summarize for a clinician.\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        resp = model.generate_content(\n",
    "            user_prompt,\n",
    "            generation_config={\n",
    "                \"temperature\": 0.2,\n",
    "                \"max_output_tokens\": 300\n",
    "            }\n",
    "        )\n",
    "        text = (resp.text or \"\").strip()\n",
    "        if not text:\n",
    "            raise RuntimeError(\"Empty response\")\n",
    "        return text, None\n",
    "    except Exception as e:\n",
    "        err = f\"{type(e).__name__}: {e}\"\n",
    "        if attempt + 1 < max_attempts:\n",
    "            time.sleep(1.2 * (attempt + 1))\n",
    "            return summarize_one(rec, attempt=attempt+1, max_attempts=max_attempts)\n",
    "        # fallback\n",
    "        snapshot_label = snapshot.get(\"severity_label\") or \"Unknown\"\n",
    "        trend_label    = history.get(\"trend\") or \"Unknown\"\n",
    "        proba_str      = history.get(\"proba\") or {}\n",
    "        alert_str      = \"; \".join(alerts) if alerts else \"None\"\n",
    "        fallback = (\n",
    "            f\"Patient {pid}: Current severity appears '{snapshot_label}'. \"\n",
    "            f\"Trend over 5 weeks: {trend_label}. \"\n",
    "            f\"Class probabilities: {proba_str}. \"\n",
    "            f\"Alerts: {alert_str}. \"\n",
    "            \"Overall summary generated without LLM due to API limits. \"\n",
    "            \"This supports decisions and is not a diagnosis.\"\n",
    "        )\n",
    "        return fallback, err\n",
    "\n",
    "# ---------- run 10 requests ----------\n",
    "rows = []\n",
    "for i, rec in enumerate(records, start=1):\n",
    "    text, err = summarize_one(rec)\n",
    "    if err:\n",
    "        print(f\"[{i}] fallback used -> {err}\")\n",
    "    else:\n",
    "        print(f\"[{i}] done.\")\n",
    "    rows.append({\"patient_id\": rec.get(\"patient_id\", \"unknown\"), \"summary\": text})\n",
    "\n",
    "# ---------- write outputs (atomic replace) ----------\n",
    "csv_tmp = OUT_DIR / (CSV_PATH.stem + \".tmp.csv\")\n",
    "txt_tmp = OUT_DIR / (TXT_PATH.stem + \".tmp.txt\")\n",
    "\n",
    "with open(csv_tmp, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"patient_id\",\"summary\"])\n",
    "    w.writeheader()\n",
    "    w.writerows(rows)\n",
    "\n",
    "with open(txt_tmp, \"w\", encoding=\"utf-8\") as f:\n",
    "    for r in rows:\n",
    "        f.write(f\"[{r['patient_id']}]\\n{r['summary']}\\n\\n\")\n",
    "\n",
    "if CSV_PATH.exists(): CSV_PATH.unlink(missing_ok=True)\n",
    "if TXT_PATH.exists(): TXT_PATH.unlink(missing_ok=True)\n",
    "csv_tmp.replace(CSV_PATH)\n",
    "txt_tmp.replace(TXT_PATH)\n",
    "\n",
    "print(\"Wrote:\")\n",
    "print(\" \", CSV_PATH)\n",
    "print(\" \", TXT_PATH)\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60020427-6bfb-4990-915b-bb8f9181386e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini key present? True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyDc7_BiAtyxG3BKmeNS2uOcXgi0zLoIiAI\"\n",
    "os.environ[\"GEMINI_MODEL\"]   = \"gemini-2.5-flash\"  # or gemini-2.0-flash if that's your plan\n",
    "print(\"Gemini key present?\", bool(os.getenv(\"GEMINI_API_KEY\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e54f692-7174-487b-afe1-1419f57bc444",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'HarmCategory' has no attribute 'HARM_CATEGORY_MEDICAL'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 45\u001b[0m\n\u001b[0;32m     36\u001b[0m SYSTEM \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a clinical assistant. Summarize EMR outputs for a clinician in 5–7 sentences. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mState patient context, current severity (with label), 5-week trend, any notable vitals, and the alerts. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse neutral, calibrated language (e.g., \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuggests\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconsider\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m); avoid diagnoses or prescriptions; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno PII beyond patient_id. End with: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis supports decisions and is not a diagnosis.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     41\u001b[0m )\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Relax safety so summaries aren’t blocked as ‘medical advice’\u001b[39;00m\n\u001b[0;32m     44\u001b[0m SAFETY \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m---> 45\u001b[0m     HarmCategory\u001b[38;5;241m.\u001b[39mHARM_CATEGORY_MEDICAL: HarmBlockThreshold\u001b[38;5;241m.\u001b[39mBLOCK_NONE,\n\u001b[0;32m     46\u001b[0m     HarmCategory\u001b[38;5;241m.\u001b[39mHARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold\u001b[38;5;241m.\u001b[39mBLOCK_NONE,\n\u001b[0;32m     47\u001b[0m     HarmCategory\u001b[38;5;241m.\u001b[39mHARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold\u001b[38;5;241m.\u001b[39mBLOCK_MEDIUM_AND_ABOVE,\n\u001b[0;32m     48\u001b[0m     HarmCategory\u001b[38;5;241m.\u001b[39mHARM_CATEGORY_HARASSMENT: HarmBlockThreshold\u001b[38;5;241m.\u001b[39mBLOCK_MEDIUM_AND_ABOVE,\n\u001b[0;32m     49\u001b[0m     HarmCategory\u001b[38;5;241m.\u001b[39mHARM_CATEGORY_SEXUAL: HarmBlockThreshold\u001b[38;5;241m.\u001b[39mBLOCK_MEDIUM_AND_ABOVE,\n\u001b[0;32m     50\u001b[0m }\n\u001b[0;32m     52\u001b[0m model \u001b[38;5;241m=\u001b[39m genai\u001b[38;5;241m.\u001b[39mGenerativeModel(\n\u001b[0;32m     53\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mGEMINI_MODEL,\n\u001b[0;32m     54\u001b[0m     system_instruction\u001b[38;5;241m=\u001b[39mSYSTEM,\n\u001b[0;32m     55\u001b[0m     safety_settings\u001b[38;5;241m=\u001b[39mSAFETY\n\u001b[0;32m     56\u001b[0m )\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# ---------- load fusion ----------\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'HarmCategory' has no attribute 'HARM_CATEGORY_MEDICAL'"
     ]
    }
   ],
   "source": [
    "# --- EMR LLM Summaries via Google Gemini (safe + rate-limited) ---\n",
    "# pip install google-generativeai python-dotenv\n",
    "\n",
    "import os, json, time, csv, re\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT    = Path(r\"C:\\Users\\aayus\\Downloads\\emr-smart\")\n",
    "OUT_DIR = ROOT / \"outputs\"\n",
    "IN_PATH = OUT_DIR / \"fusion.ndjson\"\n",
    "CSV_OUT = OUT_DIR / \"summaries_gemini.csv\"\n",
    "TXT_OUT = OUT_DIR / \"summaries_gemini.txt\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- env (use existing env; read .env only if needed) ----------\n",
    "if not os.getenv(\"GEMINI_API_KEY\"):\n",
    "    try:\n",
    "        from dotenv import load_dotenv\n",
    "        for enc in (\"utf-8\", \"utf-8-sig\"):\n",
    "            if load_dotenv(dotenv_path=str(ROOT/\".env\"), override=False, encoding=enc):\n",
    "                if os.getenv(\"GEMINI_API_KEY\"):\n",
    "                    break\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "GEMINI_API_KEY = (os.getenv(\"GEMINI_API_KEY\") or \"\").strip()\n",
    "GEMINI_MODEL   = (os.getenv(\"GEMINI_MODEL\") or \"gemini-2.5-flash\").strip()\n",
    "if not GEMINI_API_KEY:\n",
    "    raise RuntimeError(\"GEMINI_API_KEY missing. Put it in .env or set os.environ['GEMINI_API_KEY'].\")\n",
    "\n",
    "# ---------- model ----------\n",
    "import google.generativeai as genai\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "SYSTEM = (\n",
    "    \"You are a clinical assistant. Summarize EMR outputs for a clinician in 5–7 sentences. \"\n",
    "    \"State patient context, current severity (with label), 5-week trend, any notable vitals, and the alerts. \"\n",
    "    \"Use neutral, calibrated language (e.g., 'suggests', 'consider'); avoid diagnoses or prescriptions; \"\n",
    "    \"no PII beyond patient_id. End with: 'This supports decisions and is not a diagnosis.'\"\n",
    ")\n",
    "\n",
    "# Relax safety so summaries aren’t blocked as ‘medical advice’\n",
    "SAFETY = {\n",
    "    HarmCategory.HARM_CATEGORY_MEDICAL: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "    HarmCategory.HARM_CATEGORY_SEXUAL: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=GEMINI_MODEL,\n",
    "    system_instruction=SYSTEM,\n",
    "    safety_settings=SAFETY\n",
    ")\n",
    "\n",
    "# ---------- load fusion ----------\n",
    "if not IN_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing input: {IN_PATH} (run fuse_infer_generate first).\")\n",
    "\n",
    "records = []\n",
    "with open(IN_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            try:\n",
    "                records.append(json.loads(line))\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "# keep small batch to stay under free-tier limits\n",
    "BATCH_LIMIT    = 5     # if you need 10 later, set to 10\n",
    "SLEEP_BETWEEN  = 7.0   # seconds between requests (<= ~8/min)\n",
    "MAX_ATTEMPTS   = 3\n",
    "records = records[:BATCH_LIMIT]\n",
    "print(f\"Loaded {len(records)} fusion records (target {BATCH_LIMIT}).\")\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def extract_text(resp):\n",
    "    \"\"\"\n",
    "    Robustly extract text from a Gemini response.\n",
    "    If blocked (no parts), return \"\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if hasattr(resp, \"text\") and resp.text:\n",
    "            return resp.text.strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # fall back: scan candidates/parts\n",
    "    try:\n",
    "        for cand in getattr(resp, \"candidates\", []) or []:\n",
    "            # finish_reason 2 = SAFETY (blocked). Skip these.\n",
    "            if getattr(cand, \"finish_reason\", None) == 2:\n",
    "                continue\n",
    "            parts = getattr(getattr(cand, \"content\", None), \"parts\", []) or []\n",
    "            chunks = []\n",
    "            for p in parts:\n",
    "                # for text parts\n",
    "                if hasattr(p, \"text\") and p.text:\n",
    "                    chunks.append(p.text)\n",
    "                # for dict-like parts\n",
    "                elif isinstance(p, dict) and p.get(\"text\"):\n",
    "                    chunks.append(p[\"text\"])\n",
    "            if chunks:\n",
    "                return \"\\n\".join(chunks).strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return \"\"\n",
    "\n",
    "def parse_retry_delay_seconds(msg: str, default_sec: float = 25.0) -> float:\n",
    "    m = re.search(r\"retry_delay\\s*\\{\\s*seconds:\\s*([0-9]+)\", msg)\n",
    "    if m:\n",
    "        try:\n",
    "            return float(m.group(1))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return default_sec\n",
    "\n",
    "def fallback_summary(rec, note: str):\n",
    "    pid      = rec.get(\"patient_id\",\"unknown\")\n",
    "    snapshot = rec.get(\"snapshot\") or {}\n",
    "    history  = rec.get(\"history\")  or {}\n",
    "    alerts   = rec.get(\"alerts\")   or []\n",
    "    return (\n",
    "        f\"Patient {pid}: Current severity appears '{(snapshot.get('severity_label') or 'Unknown')}'. \"\n",
    "        f\"Trend over 5 weeks: {(history.get('trend') or 'Unknown')}. \"\n",
    "        f\"Class probabilities: {(history.get('proba') or {})}. \"\n",
    "        f\"Alerts: {('; '.join(alerts)) if alerts else 'None'}. \"\n",
    "        f\"(LLM fallback: {note}) This supports decisions and is not a diagnosis.\"\n",
    "    )\n",
    "\n",
    "def summarize_one(rec):\n",
    "    prompt = (\n",
    "        f\"Patient ID: {rec.get('patient_id','unknown')}\\n\"\n",
    "        f\"Current severity: { (rec.get('snapshot') or {}).get('severity_numeric') } \"\n",
    "        f\"({ (rec.get('snapshot') or {}).get('severity_label') })\\n\"\n",
    "        f\"Trend: { (rec.get('history') or {}).get('trend') }\\n\"\n",
    "        f\"Class probabilities: { (rec.get('history') or {}).get('proba') }\\n\"\n",
    "        f\"Alerts: { rec.get('alerts') or [] }\\n\"\n",
    "        \"Summarize for a clinician.\"\n",
    "    )\n",
    "\n",
    "    last_err = None\n",
    "    for attempt in range(1, MAX_ATTEMPTS+1):\n",
    "        try:\n",
    "            resp = model.generate_content(\n",
    "                prompt,\n",
    "                generation_config={\"temperature\": 0.2, \"max_output_tokens\": 300}\n",
    "            )\n",
    "            text = extract_text(resp)\n",
    "            if text:\n",
    "                return text, None\n",
    "            # blocked or empty\n",
    "            last_err = \"Empty/blocked response (finish_reason likely SAFETY)\"\n",
    "            # small wait then try again once\n",
    "            time.sleep(0.8 * attempt)\n",
    "        except Exception as e:\n",
    "            last_err = f\"{type(e).__name__}: {e}\"\n",
    "            # handle 429 with server-provided delay if present\n",
    "            if \"ResourceExhausted\" in str(type(e)) or \"429\" in str(e):\n",
    "                time.sleep(parse_retry_delay_seconds(str(e), 25.0))\n",
    "            else:\n",
    "                time.sleep(1.2 * attempt)\n",
    "\n",
    "    # fallback after retries\n",
    "    return fallback_summary(rec, note=last_err), last_err\n",
    "\n",
    "# ---------- run ----------\n",
    "rows = []\n",
    "for i, rec in enumerate(records, start=1):\n",
    "    txt, err = summarize_one(rec)\n",
    "    if err:\n",
    "        print(f\"[{i}] fallback used -> {err}\")\n",
    "    else:\n",
    "        print(f\"[{i}] done.\")\n",
    "    rows.append({\"patient_id\": rec.get(\"patient_id\",\"unknown\"), \"summary\": txt})\n",
    "    # respect free-tier per-minute limits\n",
    "    if i < len(records):\n",
    "        time.sleep(SLEEP_BETWEEN)\n",
    "\n",
    "# ---------- write outputs atomically ----------\n",
    "tmp_csv = OUT_DIR / (CSV_OUT.stem + \".tmp.csv\")\n",
    "tmp_txt = OUT_DIR / (TXT_OUT.stem + \".tmp.txt\")\n",
    "\n",
    "with open(tmp_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"patient_id\",\"summary\"])\n",
    "    w.writeheader()\n",
    "    w.writerows(rows)\n",
    "\n",
    "with open(tmp_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "    for r in rows:\n",
    "        f.write(f\"[{r['patient_id']}]\\n{r['summary']}\\n\\n\")\n",
    "\n",
    "if CSV_OUT.exists(): CSV_OUT.unlink(missing_ok=True)\n",
    "if TXT_OUT.exists(): TXT_OUT.unlink(missing_ok=True)\n",
    "tmp_csv.replace(CSV_OUT)\n",
    "tmp_txt.replace(TXT_OUT)\n",
    "\n",
    "print(\"Wrote:\")\n",
    "print(\" \", CSV_OUT)\n",
    "print(\" \", TXT_OUT)\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdb951b8-27cc-443e-b105-cfdd62703164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written summary to emr_summary.txt\n",
      "\n",
      "For patient P001, the current assessment indicates a severity score of 0.67, categorizing the patient's condition as Moderate. Over the past five weeks, the patient's trend has been assessed as Stable. Notable vital sign alerts include elevated blood pressure and borderline oxygen saturation. These findings suggest a need to consider these parameters. The system has generated alerts for both elevated BP and borderline O2. This supports decisions and is not a diagnosis.\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "import google.generativeai as genai\n",
    "\n",
    "# ---------- API key ----------\n",
    "API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise RuntimeError(\"Set GEMINI_API_KEY in your environment first\")\n",
    "genai.configure(api_key=API_KEY)\n",
    "\n",
    "# ---------- Model ----------\n",
    "SYSTEM = (\n",
    "    \"You are a clinical assistant. Summarize EMR outputs for a clinician in 5–7 sentences. \"\n",
    "    \"State patient context, current severity (with label), 5-week trend, any notable vitals, and the alerts. \"\n",
    "    \"Use neutral, calibrated language (e.g., 'suggests', 'consider'); avoid diagnoses or prescriptions; \"\n",
    "    \"no PII beyond patient_id. End with: 'This supports decisions and is not a diagnosis.'\"\n",
    ")\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-2.5-flash\",\n",
    "    system_instruction=SYSTEM\n",
    ")\n",
    "\n",
    "# ---------- Example record ----------\n",
    "fusion_example = {\n",
    "    \"patient_id\": \"P001\",\n",
    "    \"snapshot\": {\"severity_score\": 0.67, \"severity_class\": \"Moderate\"},\n",
    "    \"history\": {\"trend_label\": \"Stable\"},\n",
    "    \"derived\": {\"alerts\": [\"BP elevated\", \"O2 borderline\"]}\n",
    "}\n",
    "\n",
    "prompt = f\"Summarize this EMR record:\\n{json.dumps(fusion_example, indent=2)}\"\n",
    "\n",
    "# ---------- Generate ----------\n",
    "resp = model.generate_content(prompt)\n",
    "\n",
    "summary = (resp.text or \"\").strip()\n",
    "if not summary:\n",
    "    summary = \"No text returned by Gemini.\"\n",
    "\n",
    "# ---------- Save ----------\n",
    "out_path = \"emr_summary.txt\"\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(f\"Written summary to {out_path}\\n\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d63bd3-2406-4c2e-85ab-ce7c3ce268e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
