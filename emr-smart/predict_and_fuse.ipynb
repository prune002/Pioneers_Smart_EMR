{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35fc83d1-cbfe-4c65-aae5-49dc631eba85",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m SNAPSHOT_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124maayus\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124memr-smart\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124memr_snapshot.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m HISTORY_PATH  \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124maayus\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124memr-smart\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124memr_history.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 7\u001b[0m MODEL_DIR     \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18m__file__\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m OUT_DIR       \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18m__file__\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(OUT_DIR, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import os, json, datetime as dt\n",
    "import numpy as np, pandas as pd\n",
    "import joblib\n",
    "\n",
    "SNAPSHOT_PATH = r\"C:\\Users\\aayus\\Downloads\\emr-smart\\data\\emr_snapshot.csv\"\n",
    "HISTORY_PATH  = r\"C:\\Users\\aayus\\Downloads\\emr-smart\\data\\emr_history.csv\"\n",
    "MODEL_DIR     = os.path.join(os.path.dirname(__file__), \"models\")\n",
    "OUT_DIR       = os.path.join(os.path.dirname(__file__), \"outputs\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def severity_to_class(s, c1=2.5, c2=5.6):\n",
    "    if s < c1: return \"Low\"\n",
    "    if s < c2: return \"Moderate\"\n",
    "    return \"High\"\n",
    "\n",
    "def rule_alerts(feat):\n",
    "    alerts, why = [], []\n",
    "    o2 = feat.get(\"Oxygen_Saturation\")\n",
    "    if o2 is not None and pd.notna(o2) and o2 < 92:\n",
    "        alerts.append(\"Hypoxemia\"); why.append(f\"O₂ {o2:.0f}%\")\n",
    "    T = feat.get(\"Temperature\")\n",
    "    if T is not None and pd.notna(T) and T >= 38.3:\n",
    "        alerts.append(\"Possible infection/fever\"); why.append(f\"T {T:.1f}°C\")\n",
    "    sbp = feat.get(\"Blood_Pressure_Systolic\"); dbp = feat.get(\"Blood_Pressure_Diastolic\")\n",
    "    if all(v is not None and pd.notna(v) for v in [sbp, dbp]) and (sbp >= 180 or dbp >= 120):\n",
    "        alerts.append(\"Hypertensive crisis (recheck)\"); why.append(f\"BP {int(sbp)}/{int(dbp)}\")\n",
    "    glu = feat.get(\"Blood_Sugar\")\n",
    "    if glu is not None and pd.notna(glu) and glu >= 250:\n",
    "        alerts.append(\"Hyperglycemia\"); why.append(f\"Glucose {int(glu)} mg/dL\")\n",
    "    return alerts, why\n",
    "\n",
    "def series_delta_slope(row, prefix):\n",
    "    weeks = [row.get(f\"{prefix}_Week{i}\", np.nan) for i in range(1,6)]\n",
    "    arr = np.array(weeks, dtype=float)\n",
    "    if np.any(np.isnan(arr)): return {\"delta\": None, \"slope\": None}\n",
    "    x = np.arange(1,6, dtype=float)\n",
    "    slope = float(np.polyfit(x, arr, 1)[0])\n",
    "    delta = float(arr[-1] - arr[0])\n",
    "    return {\"delta\": delta, \"slope\": slope}\n",
    "\n",
    "# ---------- load ----------\n",
    "snap = pd.read_csv(SNAPSHOT_PATH)\n",
    "hist = pd.read_csv(HISTORY_PATH)\n",
    "\n",
    "# models (snapshot may not exist if you skipped severity earlier)\n",
    "snapshot_model = None\n",
    "cut_c1, cut_c2 = 2.5, 5.6\n",
    "sp = os.path.join(MODEL_DIR, \"snapshot_regressor.joblib\")\n",
    "if os.path.exists(sp):\n",
    "    snapshot_model = joblib.load(sp)\n",
    "    cpath = os.path.join(MODEL_DIR, \"snapshot_cutpoints.txt\")\n",
    "    if os.path.exists(cpath):\n",
    "        with open(cpath) as f:\n",
    "            for line in f:\n",
    "                if line.startswith(\"c1=\"): cut_c1 = float(line.split(\"=\")[1])\n",
    "                if line.startswith(\"c2=\"): cut_c2 = float(line.split(\"=\")[1])\n",
    "\n",
    "hist_model = joblib.load(os.path.join(MODEL_DIR, \"history_classifier.joblib\"))\n",
    "\n",
    "# align patients present in both\n",
    "if \"Patient_ID\" not in snap.columns or \"Patient_ID\" not in hist.columns:\n",
    "    raise ValueError(\"Both CSVs must contain Patient_ID\")\n",
    "\n",
    "pids = sorted(set(snap[\"Patient_ID\"]).intersection(set(hist[\"Patient_ID\"])))\n",
    "if not pids:\n",
    "    raise ValueError(\"No overlapping Patient_IDs between snapshot and history\")\n",
    "\n",
    "# choose the same feature set the model expects (model’s ColumnTransformer handles the rest)\n",
    "def current_feats(row):\n",
    "    keys = [\"Heart_Rate\",\"Blood_Pressure_Systolic\",\"Blood_Pressure_Diastolic\",\n",
    "            \"Respiratory_Rate\",\"Temperature\",\"Oxygen_Saturation\",\n",
    "            \"Blood_Sugar\",\"Cholesterol_Total\",\"Weight\",\"BMI\"]\n",
    "    return {k: row.get(k, None) for k in keys}\n",
    "\n",
    "fusions = []\n",
    "for pid in pids:\n",
    "    srow = snap.loc[snap[\"Patient_ID\"] == pid].iloc[0]\n",
    "    hrow = hist.loc[hist[\"Patient_ID\"] == pid].iloc[0]\n",
    "\n",
    "    # SNAPSHOT predict\n",
    "    sev_score = None; sev_class = None\n",
    "    if snapshot_model is not None:\n",
    "        # build X row dropping IDs/Severity if present; ColumnTransformer will handle numeric+cats\n",
    "        drop_cols = [c for c in [\"Patient_ID\",\"Patient_Name\",\"Severity\"] if c in srow.index]\n",
    "        Xs = pd.DataFrame([srow.drop(labels=drop_cols, errors=\"ignore\").to_dict()])\n",
    "        sev_score = float(snapshot_model.predict(Xs)[0])\n",
    "        sev_class = severity_to_class(sev_score, cut_c1, cut_c2)\n",
    "\n",
    "    # HISTORY predict\n",
    "    # the classifier pipeline expects the raw row-like frame it was trained on (build_design was inside pipeline at train)\n",
    "    Xh = pd.DataFrame([hrow.drop(labels=[c for c in [\"Patient_ID\",\"Trend_Status\"] if c in hrow.index]).to_dict()])\n",
    "    probs = hist_model.predict_proba(Xh)[0]\n",
    "    classes = list(hist_model.classes_)\n",
    "    trend_label = classes[int(np.argmax(probs))]\n",
    "    trend_probs  = {cls: float(p) for cls, p in zip(classes, probs)}\n",
    "\n",
    "    # compact trend features (handful)\n",
    "    tf = {\n",
    "        \"Heart_Rate\": series_delta_slope(hrow, \"Heart_Rate\"),\n",
    "        \"Blood_Pressure_Systolic\": series_delta_slope(hrow, \"Blood_Pressure_Systolic\"),\n",
    "    }\n",
    "\n",
    "    cur = current_feats(srow)\n",
    "    alerts, why = rule_alerts(cur)\n",
    "\n",
    "    # simple risk score\n",
    "    risk = 1\n",
    "    if sev_class == \"High\" and trend_label == \"Worsening\":\n",
    "        risk = 3\n",
    "    elif (sev_class == \"High\" and trend_label in [\"Stable\",\"Improving\"]) or (sev_class == \"Moderate\" and trend_label == \"Worsening\"):\n",
    "        risk = 2\n",
    "\n",
    "    fusion = {\n",
    "        \"patient_id\": str(pid),\n",
    "        \"timestamp\": dt.datetime.utcnow().isoformat() + \"Z\",\n",
    "        \"snapshot\": {\n",
    "            \"severity_score\": sev_score,\n",
    "            \"severity_class\": sev_class,\n",
    "            \"features\": cur\n",
    "        },\n",
    "        \"history\": {\n",
    "            \"trend_label\": trend_label,\n",
    "            \"probs\": trend_probs,\n",
    "            \"trend_features\": tf\n",
    "        },\n",
    "        \"derived\": {\n",
    "            \"risk_score\": risk,\n",
    "            \"alerts\": alerts,\n",
    "            \"rationales\": why\n",
    "        }\n",
    "    }\n",
    "    fusions.append(fusion)\n",
    "\n",
    "# write one JSON per patient + one big ndjson\n",
    "ndjson_path = os.path.join(OUT_DIR, \"fusion.ndjson\")\n",
    "with open(ndjson_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for fu in fusions:\n",
    "        f.write(json.dumps(fu) + \"\\n\")\n",
    "\n",
    "print(f\"Fusion written: {ndjson_path}  ({len(fusions)} patients)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d69c1ae-8798-4ec7-b30e-d05b9c76715d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "columns are missing: {'Respiratory_Rate_delta', 'Blood_Sugar_cv', 'Weight_abs_slope', 'Blood_Pressure_Systolic_mad', 'Weight_mad', 'Cholesterol_Total_slope', 'BMI_abs_slope', 'Heart_Rate_range', 'BMI_mean', 'Severity_Week5', 'Cholesterol_Total_delta', 'Blood_Pressure_Diastolic_abs_slope', 'Severity_Week4', 'global_mean_abs_step', 'Respiratory_Rate_cv', 'Blood_Pressure_Diastolic_cv', 'Respiratory_Rate_abs_slope', 'Oxygen_Saturation_slope', 'Blood_Pressure_Systolic_delta', 'Temperature_range', 'Respiratory_Rate_mean_abs_step', 'Severity_Week3', 'Blood_Sugar_std', 'Blood_Pressure_Systolic_mean', 'Weight_delta', 'Temperature_stable', 'Temperature_mean_abs_step', 'Blood_Pressure_Systolic_range', 'Blood_Pressure_Diastolic_slope', 'Blood_Pressure_Systolic_slope', 'Blood_Pressure_Diastolic_mean', 'Blood_Pressure_Diastolic_range', 'BMI_slope', 'Heart_Rate_mean_abs_step', 'Oxygen_Saturation_delta', 'Temperature_abs_slope', 'Weight_stable', 'Temperature_std', 'Heart_Rate_cv', 'Weight_mean', 'Cholesterol_Total_mad', 'BMI_std', 'Temperature_mean', 'Blood_Pressure_Diastolic_stable', 'Blood_Pressure_Systolic_mean_abs_step', 'Weight_cv', 'Heart_Rate_std', 'Blood_Pressure_Systolic_std', 'Temperature_delta', 'BMI_stable', 'Cholesterol_Total_std', 'Respiratory_Rate_std', 'Blood_Sugar_delta', 'Oxygen_Saturation_mean', 'Oxygen_Saturation_mean_abs_step', 'Cholesterol_Total_mean_abs_step', 'global_stability_ratio', 'Cholesterol_Total_cv', 'Blood_Pressure_Systolic_abs_slope', 'Temperature_mad', 'Weight_slope', 'Oxygen_Saturation_cv', 'BMI_mean_abs_step', 'BMI_cv', 'Heart_Rate_mean', 'Oxygen_Saturation_std', 'Weight_std', 'Blood_Sugar_slope', 'BMI_range', 'Blood_Pressure_Diastolic_mean_abs_step', 'Oxygen_Saturation_stable', 'Cholesterol_Total_stable', 'Blood_Pressure_Diastolic_std', 'Respiratory_Rate_range', 'Blood_Pressure_Diastolic_delta', 'Blood_Sugar_abs_slope', 'Heart_Rate_slope', 'Blood_Pressure_Systolic_cv', 'Heart_Rate_abs_slope', 'Weight_range', 'Respiratory_Rate_mean', 'Blood_Sugar_stable', 'Severity_Week2', 'Blood_Sugar_mean', 'Oxygen_Saturation_abs_slope', 'Respiratory_Rate_slope', 'Temperature_slope', 'Blood_Sugar_range', 'BMI_delta', 'Cholesterol_Total_abs_slope', 'Blood_Pressure_Diastolic_mad', 'Heart_Rate_delta', 'Respiratory_Rate_mad', 'Respiratory_Rate_stable', 'Temperature_cv', 'Weight_mean_abs_step', 'Blood_Sugar_mean_abs_step', 'Cholesterol_Total_range', 'Oxygen_Saturation_range', 'Severity_Week1', 'Cholesterol_Total_mean', 'Heart_Rate_mad', 'Blood_Sugar_mad', 'Oxygen_Saturation_mad', 'BMI_mad', 'Blood_Pressure_Systolic_stable', 'Heart_Rate_stable'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 78\u001b[0m\n\u001b[0;32m     75\u001b[0m severity_cols \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m hist\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeverity_Week\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m     76\u001b[0m Xh \u001b[38;5;241m=\u001b[39m hist\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPatient_ID\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrend_Status\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39mseverity_cols] \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m hist\u001b[38;5;241m.\u001b[39mcolumns], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 78\u001b[0m trend_pred \u001b[38;5;241m=\u001b[39m hist_pipe\u001b[38;5;241m.\u001b[39mpredict(Xh)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     P \u001b[38;5;241m=\u001b[39m hist_pipe\u001b[38;5;241m.\u001b[39mpredict_proba(Xh)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:600\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[1;34m(self, X, **params)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _routing_enabled():\n\u001b[0;32m    599\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 600\u001b[0m         Xt \u001b[38;5;241m=\u001b[39m transform\u001b[38;5;241m.\u001b[39mtransform(Xt)\n\u001b[0;32m    601\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    603\u001b[0m \u001b[38;5;66;03m# metadata routing enabled\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    319\u001b[0m         )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:1065\u001b[0m, in \u001b[0;36mColumnTransformer.transform\u001b[1;34m(self, X, **params)\u001b[0m\n\u001b[0;32m   1063\u001b[0m     diff \u001b[38;5;241m=\u001b[39m all_names \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(column_names)\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m diff:\n\u001b[1;32m-> 1065\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns are missing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdiff\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1067\u001b[0m     \u001b[38;5;66;03m# ndarray was used for fitting or transforming, thus we only\u001b[39;00m\n\u001b[0;32m   1068\u001b[0m     \u001b[38;5;66;03m# check that n_features_in_ is consistent\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: columns are missing: {'Respiratory_Rate_delta', 'Blood_Sugar_cv', 'Weight_abs_slope', 'Blood_Pressure_Systolic_mad', 'Weight_mad', 'Cholesterol_Total_slope', 'BMI_abs_slope', 'Heart_Rate_range', 'BMI_mean', 'Severity_Week5', 'Cholesterol_Total_delta', 'Blood_Pressure_Diastolic_abs_slope', 'Severity_Week4', 'global_mean_abs_step', 'Respiratory_Rate_cv', 'Blood_Pressure_Diastolic_cv', 'Respiratory_Rate_abs_slope', 'Oxygen_Saturation_slope', 'Blood_Pressure_Systolic_delta', 'Temperature_range', 'Respiratory_Rate_mean_abs_step', 'Severity_Week3', 'Blood_Sugar_std', 'Blood_Pressure_Systolic_mean', 'Weight_delta', 'Temperature_stable', 'Temperature_mean_abs_step', 'Blood_Pressure_Systolic_range', 'Blood_Pressure_Diastolic_slope', 'Blood_Pressure_Systolic_slope', 'Blood_Pressure_Diastolic_mean', 'Blood_Pressure_Diastolic_range', 'BMI_slope', 'Heart_Rate_mean_abs_step', 'Oxygen_Saturation_delta', 'Temperature_abs_slope', 'Weight_stable', 'Temperature_std', 'Heart_Rate_cv', 'Weight_mean', 'Cholesterol_Total_mad', 'BMI_std', 'Temperature_mean', 'Blood_Pressure_Diastolic_stable', 'Blood_Pressure_Systolic_mean_abs_step', 'Weight_cv', 'Heart_Rate_std', 'Blood_Pressure_Systolic_std', 'Temperature_delta', 'BMI_stable', 'Cholesterol_Total_std', 'Respiratory_Rate_std', 'Blood_Sugar_delta', 'Oxygen_Saturation_mean', 'Oxygen_Saturation_mean_abs_step', 'Cholesterol_Total_mean_abs_step', 'global_stability_ratio', 'Cholesterol_Total_cv', 'Blood_Pressure_Systolic_abs_slope', 'Temperature_mad', 'Weight_slope', 'Oxygen_Saturation_cv', 'BMI_mean_abs_step', 'BMI_cv', 'Heart_Rate_mean', 'Oxygen_Saturation_std', 'Weight_std', 'Blood_Sugar_slope', 'BMI_range', 'Blood_Pressure_Diastolic_mean_abs_step', 'Oxygen_Saturation_stable', 'Cholesterol_Total_stable', 'Blood_Pressure_Diastolic_std', 'Respiratory_Rate_range', 'Blood_Pressure_Diastolic_delta', 'Blood_Sugar_abs_slope', 'Heart_Rate_slope', 'Blood_Pressure_Systolic_cv', 'Heart_Rate_abs_slope', 'Weight_range', 'Respiratory_Rate_mean', 'Blood_Sugar_stable', 'Severity_Week2', 'Blood_Sugar_mean', 'Oxygen_Saturation_abs_slope', 'Respiratory_Rate_slope', 'Temperature_slope', 'Blood_Sugar_range', 'BMI_delta', 'Cholesterol_Total_abs_slope', 'Blood_Pressure_Diastolic_mad', 'Heart_Rate_delta', 'Respiratory_Rate_mad', 'Respiratory_Rate_stable', 'Temperature_cv', 'Weight_mean_abs_step', 'Blood_Sugar_mean_abs_step', 'Cholesterol_Total_range', 'Oxygen_Saturation_range', 'Severity_Week1', 'Cholesterol_Total_mean', 'Heart_Rate_mad', 'Blood_Sugar_mad', 'Oxygen_Saturation_mad', 'BMI_mad', 'Blood_Pressure_Systolic_stable', 'Heart_Rate_stable'}"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os, json, joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---- Paths (project-rooted) ----\n",
    "BASE = Path(r\"C:\\Users\\aayus\\Downloads\\emr-smart\")\n",
    "DATA_DIR  = BASE / \"data\"\n",
    "MODEL_DIR = BASE / \"models\"\n",
    "OUT_DIR   = BASE / \"outputs\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SNAPSHOT_PATH = DATA_DIR / \"emr_snapshot.csv\"\n",
    "HISTORY_PATH  = DATA_DIR / \"emr_history.csv\"\n",
    "\n",
    "# ---- Load data ----\n",
    "snap = pd.read_csv(SNAPSHOT_PATH)\n",
    "hist = pd.read_csv(HISTORY_PATH)\n",
    "\n",
    "# ---- Load models ----\n",
    "hist_model_path = MODEL_DIR / \"best_history_classifier.joblib\"\n",
    "if not hist_model_path.exists():\n",
    "    raise FileNotFoundError(f\"Missing model: {hist_model_path}\")\n",
    "hist_pipe = joblib.load(hist_model_path)\n",
    "\n",
    "snap_pipe = None\n",
    "cutpoints = (2.5, 5.6)\n",
    "try:\n",
    "    snap_model_path = MODEL_DIR / \"snapshot_regressor.joblib\"\n",
    "    if snap_model_path.exists():\n",
    "        snap_pipe = joblib.load(snap_model_path)\n",
    "        cp_txt = MODEL_DIR / \"snapshot_cutpoints.txt\"\n",
    "        if cp_txt.exists():\n",
    "            lines = cp_txt.read_text(encoding=\"utf-8\").strip().splitlines()\n",
    "            kv = {}\n",
    "            for ln in lines:\n",
    "                if \"=\" in ln:\n",
    "                    k, v = ln.split(\"=\", 1)\n",
    "                    kv[k.strip()] = float(v.strip())\n",
    "            cutpoints = (kv.get(\"c1\", 2.5), kv.get(\"c2\", 5.6))\n",
    "except Exception as e:\n",
    "    print(\"Snapshot model not loaded:\", e)\n",
    "\n",
    "# ---- Helper for binning severity ----\n",
    "bins = [-1e9, cutpoints[0], cutpoints[1], 1e9]\n",
    "labels = [\"Low\", \"Moderate\", \"High\"]\n",
    "\n",
    "# ---- Build fusion records by patient ----\n",
    "by_id = {}\n",
    "\n",
    "# Ensure Patient_ID columns\n",
    "if \"Patient_ID\" not in snap.columns:\n",
    "    snap[\"Patient_ID\"] = np.arange(len(snap))\n",
    "if \"Patient_ID\" not in hist.columns:\n",
    "    hist[\"Patient_ID\"] = np.arange(len(hist))\n",
    "\n",
    "snap_ids = snap[\"Patient_ID\"].astype(str)\n",
    "hist_ids = hist[\"Patient_ID\"].astype(str)\n",
    "\n",
    "# ---- Snapshot predictions (optional) ----\n",
    "if snap_pipe is not None:\n",
    "    drop_cols = [c for c in [\"Patient_ID\", \"Patient_Name\", \"Severity\", \"_SeveritySynth_\"] if c in snap.columns]\n",
    "    Xs = snap.drop(columns=drop_cols, errors=\"ignore\")\n",
    "    sev_pred = snap_pipe.predict(Xs)\n",
    "    sev_lbl  = pd.cut(sev_pred, bins=bins, labels=labels).astype(str)\n",
    "    for pid, s_num, s_lbl in zip(snap_ids, sev_pred, sev_lbl):\n",
    "        d = by_id.setdefault(pid, {})\n",
    "        d[\"snapshot\"] = {\n",
    "            \"severity_numeric\": float(s_num),\n",
    "            \"severity_label\": s_lbl\n",
    "        }\n",
    "\n",
    "# ---- History predictions (required) ----\n",
    "# Mirror training drops: remove id + any severity week columns if present\n",
    "severity_cols = [c for c in hist.columns if str(c).startswith(\"Severity_Week\")]\n",
    "Xh = hist.drop(columns=[c for c in [\"Patient_ID\", \"Trend_Status\", *severity_cols] if c in hist.columns], errors=\"ignore\")\n",
    "\n",
    "trend_pred = hist_pipe.predict(Xh)\n",
    "try:\n",
    "    P = hist_pipe.predict_proba(Xh)\n",
    "    cls = hist_pipe.named_steps[\"model\"].classes_\n",
    "except Exception:\n",
    "    P = None\n",
    "    cls = None\n",
    "\n",
    "for i, (pid, tr) in enumerate(zip(hist_ids, trend_pred)):\n",
    "    rec = by_id.setdefault(pid, {})\n",
    "    rec[\"history\"] = {\"trend\": str(tr)}\n",
    "    if P is not None:\n",
    "        pmap = {str(c): float(P[i, j]) for j, c in enumerate(cls)}\n",
    "        rec[\"history\"][\"proba\"] = pmap\n",
    "\n",
    "# ---- Alerts (simple rules) ----\n",
    "def mk_alerts(blob: dict):\n",
    "    alerts = []\n",
    "    snap_blob = blob.get(\"snapshot\", {})\n",
    "    hist_blob = blob.get(\"history\", {})\n",
    "    sev = snap_blob.get(\"severity_numeric\")\n",
    "    sev_lbl = snap_blob.get(\"severity_label\")\n",
    "    trend = hist_blob.get(\"trend\")\n",
    "    probs = hist_blob.get(\"proba\", {})\n",
    "\n",
    "    # Example heuristics\n",
    "    if sev is not None and sev >= 7.5:\n",
    "        alerts.append(\"High predicted severity; consider closer monitoring.\")\n",
    "    if trend == \"Worsening\":\n",
    "        alerts.append(\"Worsening 5-week trend flagged; review recent vitals and medications.\")\n",
    "    if probs and probs.get(\"Stable\", 0.0) < 0.2:\n",
    "        alerts.append(\"Low stability probability; check for emerging issues.\")\n",
    "    return alerts\n",
    "\n",
    "for pid, blob in by_id.items():\n",
    "    blob[\"alerts\"] = mk_alerts(blob)\n",
    "\n",
    "# ---- Write NDJSON ----\n",
    "out_path = OUT_DIR / \"fusion.ndjson\"\n",
    "with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for pid, blob in by_id.items():\n",
    "        f.write(json.dumps({\"patient_id\": pid, **blob}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Wrote:\", str(out_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cc356d7-db65-420f-9fb3-47c3dfa1500f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 175\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(snap)):\n\u001b[0;32m    172\u001b[0m         pid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(snap\u001b[38;5;241m.\u001b[39miloc[i][pid_col]) \u001b[38;5;28;01mif\u001b[39;00m pid_col \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrow_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    173\u001b[0m         snap_records[pid] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    174\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseverity_numeric\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(y_pred_sev[i]),\n\u001b[1;32m--> 175\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseverity_label\u001b[39m\u001b[38;5;124m\"\u001b[39m: sev_cls\u001b[38;5;241m.\u001b[39miloc[i]\n\u001b[0;32m    176\u001b[0m         }\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[SNAPSHOT] Inferred \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(snap_records)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows (with classes via cutpoints c1=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, c2=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# EMR Inference: Snapshot + History → fusion.ndjson\n",
    "# Paths adapted to your layout shown in screenshots\n",
    "# ============================================\n",
    "\n",
    "import os, json, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# ---------- Paths ----------\n",
    "BASE_DIR   = Path(r\"C:\\Users\\aayus\\Downloads\\emr-smart\")\n",
    "DATA_DIR   = BASE_DIR / \"data\"\n",
    "MODEL_DIR  = BASE_DIR / \"models\"\n",
    "OUT_DIR    = BASE_DIR / \"outputs\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "snap_csv = DATA_DIR / \"emr_snapshot.csv\"\n",
    "hist_csv = DATA_DIR / \"emr_history.csv\"\n",
    "\n",
    "snap_model_path = MODEL_DIR / \"snapshot_regressor.joblib\"      # may not exist if snapshot target was synthesized\n",
    "cutpoints_path  = MODEL_DIR / \"snapshot_cutpoints.txt\"          # has c1,c2 used when binning\n",
    "hist_model_path = MODEL_DIR / \"best_history_classifier.joblib\"  # exists per your screenshot\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def prf(y_true, y_pred, title):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    p_w, r_w, f_w, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "    p_m, r_m, f_m, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Weighted -> P:{p_w:.4f} R:{r_w:.4f} F1:{f_w:.4f}\")\n",
    "    print(f\"Macro    -> P:{p_m:.4f} R:{r_m:.4f} F1:{f_m:.4f}\")\n",
    "    print(\"\\nPer-class report:\")\n",
    "    print(classification_report(y_true, y_pred, zero_division=0))\n",
    "    print(\"Confusion matrix (rows=true, cols=pred):\\n\", confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# ---------- Trend features (MUST match training) ----------\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "ALL_BASES = [\n",
    "    \"Blood_Pressure_Systolic\",\"Blood_Pressure_Diastolic\",\"Heart_Rate\",\"Temperature\",\n",
    "    \"Respiratory_Rate\",\"Oxygen_Saturation\",\"Blood_Sugar\",\"Cholesterol_Total\",\"Weight\",\"BMI\"\n",
    "]\n",
    "\n",
    "class TrendFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    # This is the SAME definition we trained with (eps=1.6, full feature list)\n",
    "    def __init__(self, bases, eps=1.6):\n",
    "        self.bases = bases\n",
    "        self.eps = eps\n",
    "        self.feature_names_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        feats = []\n",
    "        for b in self.bases:\n",
    "            feats += [\n",
    "                f\"{b}_delta\", f\"{b}_mean\", f\"{b}_std\", f\"{b}_slope\",\n",
    "                f\"{b}_stable\", f\"{b}_range\", f\"{b}_mean_abs_step\",\n",
    "                f\"{b}_cv\", f\"{b}_mad\", f\"{b}_abs_slope\"\n",
    "            ]\n",
    "        feats += [\"global_stability_ratio\", \"global_mean_abs_step\"]\n",
    "        self.feature_names_ = feats\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        W = np.array([1,2,3,4,5], dtype=float)\n",
    "        rows = []\n",
    "        for _, row in X.iterrows():\n",
    "            feats = []\n",
    "            stable_hits = 0\n",
    "            all_steps = []\n",
    "            for b in self.bases:\n",
    "                vals = np.array([row[f\"{b}_Week{i}\"] for i in range(1,6)], dtype=float)\n",
    "                delta = float(vals[-1] - vals[0])\n",
    "                meanv = float(np.mean(vals))\n",
    "                stdv = float(np.std(vals, ddof=0))\n",
    "                slope = float(np.polyfit(W, vals, 1)[0])\n",
    "                steps = np.diff(vals)\n",
    "                max_step = float(np.max(np.abs(steps)))\n",
    "                stable = 1.0 if max_step < self.eps else 0.0\n",
    "                rng = float(np.max(vals) - np.min(vals))\n",
    "                mean_abs_step = float(np.mean(np.abs(steps)))\n",
    "                cv = float(stdv / (meanv + 1e-6))\n",
    "                mad = float(np.median(np.abs(vals - np.median(vals))))\n",
    "                abs_slope = abs(slope)\n",
    "\n",
    "                feats += [delta, meanv, stdv, slope, stable,\n",
    "                          rng, mean_abs_step, cv, mad, abs_slope]\n",
    "                stable_hits += stable\n",
    "                all_steps.extend(np.abs(steps))\n",
    "\n",
    "            feats += [\n",
    "                stable_hits / max(len(self.bases), 1),\n",
    "                float(np.mean(all_steps)) if all_steps else 0.0\n",
    "            ]\n",
    "            rows.append(feats)\n",
    "        return np.array(rows)\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array(self.feature_names_)\n",
    "\n",
    "def build_history_design(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reconstruct the SAME matrix used at training time:\n",
    "    - Keep all original Week1..Week5 columns (do NOT drop)\n",
    "    - Append engineered trend features\n",
    "    - Keep any categorical columns as-is\n",
    "    \"\"\"\n",
    "    # Do NOT drop Severity_Week* here; training kept them\n",
    "    cat_cols = [c for c in df_raw.columns if df_raw[c].dtype == \"object\"]\n",
    "    num_cols = [c for c in df_raw.columns if c not in cat_cols]\n",
    "\n",
    "    # Determine which bases exist\n",
    "    bases_present = [b for b in ALL_BASES if f\"{b}_Week1\" in df_raw.columns and f\"{b}_Week5\" in df_raw.columns]\n",
    "    if not bases_present:\n",
    "        raise ValueError(\"No *_Week1..Week5 series found in history input; cannot engineer trends.\")\n",
    "\n",
    "    eng = TrendFeatureEngineer(bases_present, eps=1.6)\n",
    "    trend = eng.fit_transform(df_raw)\n",
    "    tcols = eng.get_feature_names_out().tolist()\n",
    "\n",
    "    # Build new frame with numeric (original) + engineered + cats (concat for speed)\n",
    "    X_num = df_raw[num_cols].copy()\n",
    "    X_trend = pd.DataFrame(trend, columns=tcols, index=df_raw.index)\n",
    "    X_cat = df_raw[cat_cols].copy()\n",
    "    out = pd.concat([X_num, X_trend, X_cat], axis=1)\n",
    "    return out\n",
    "\n",
    "# ---------- Load models ----------\n",
    "if not hist_model_path.exists():\n",
    "    raise FileNotFoundError(f\"Missing model: {hist_model_path}\")\n",
    "hist_pipe = joblib.load(hist_model_path)\n",
    "\n",
    "snap_available = snap_model_path.exists()\n",
    "if snap_available:\n",
    "    snap_pipe = joblib.load(snap_model_path)\n",
    "    # load cutpoints if present\n",
    "    c1, c2 = 2.5, 5.6\n",
    "    if cutpoints_path.exists():\n",
    "        try:\n",
    "            text = Path(cutpoints_path).read_text().strip().splitlines()\n",
    "            kv = dict(line.split(\"=\") for line in text if \"=\" in line)\n",
    "            c1 = float(kv.get(\"c1\", c1))\n",
    "            c2 = float(kv.get(\"c2\", c2))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# ---------- SNAPSHOT inference (optional) ----------\n",
    "snap_records = {}\n",
    "if snap_available and snap_csv.exists():\n",
    "    snap = pd.read_csv(snap_csv)\n",
    "    # infer using the saved pipeline (it contains preprocessing); we just drop ID/name if present\n",
    "    Xs = snap.drop(columns=[c for c in [\"Patient_ID\", \"Patient_Name\", \"Severity\", \"severity\",\n",
    "                                        \"SeverityScore\",\"severity_score\",\"Severity_Label\",\"severity_label\",\n",
    "                                        \"SeverityClass\",\"severityClass\",\"sev\",\"label\",\"target\"]\n",
    "                            if c in snap.columns], errors=\"ignore\")\n",
    "    y_pred_sev = snap_pipe.predict(Xs)\n",
    "    # Bin to Low/Moderate/High using saved cutpoints\n",
    "    bins = [-1e9, c1, c2, 1e9]\n",
    "    labels = [\"Low\",\"Moderate\",\"High\"]\n",
    "    sev_cls = pd.cut(y_pred_sev, bins=bins, labels=labels).astype(str)\n",
    "\n",
    "    # stash by patient_id if present; else row index\n",
    "    pid_col = \"Patient_ID\" if \"Patient_ID\" in snap.columns else None\n",
    "    for i in range(len(snap)):\n",
    "        pid = str(snap.iloc[i][pid_col]) if pid_col else f\"row_{i}\"\n",
    "        snap_records[pid] = {\n",
    "            \"severity_numeric\": float(y_pred_sev[i]),\n",
    "            \"severity_label\": sev_cls.iloc[i]\n",
    "        }\n",
    "    print(f\"[SNAPSHOT] Inferred {len(snap_records)} rows (with classes via cutpoints c1={c1}, c2={c2}).\")\n",
    "else:\n",
    "    if not snap_csv.exists():\n",
    "        warnings.warn(f\"Snapshot CSV not found at {snap_csv}; skipping snapshot inference.\")\n",
    "    else:\n",
    "        warnings.warn(\"Snapshot model not found; skipping snapshot inference.\")\n",
    "\n",
    "# ---------- HISTORY inference (rebuild design!) ----------\n",
    "if not hist_csv.exists():\n",
    "    raise FileNotFoundError(f\"History CSV not found at: {hist_csv}\")\n",
    "\n",
    "hist = pd.read_csv(hist_csv)\n",
    "\n",
    "# We only drop obvious IDs/labels; KEEP Severity_Week* columns because training kept them\n",
    "drop_cols = [c for c in [\"Patient_ID\", \"Trend_Status\"] if c in hist.columns]\n",
    "hist_raw = hist.drop(columns=drop_cols, errors=\"ignore\").copy()\n",
    "\n",
    "# Build design to match training schema\n",
    "Xh = build_history_design(hist_raw)\n",
    "\n",
    "# Predict\n",
    "trend_pred = hist_pipe.predict(Xh)\n",
    "try:\n",
    "    P = hist_pipe.predict_proba(Xh)\n",
    "    classes = hist_pipe.named_steps.get(\"model\", hist_pipe).classes_\n",
    "except Exception:\n",
    "    # some estimators lack predict_proba\n",
    "    P = None\n",
    "    classes = np.unique(trend_pred)\n",
    "\n",
    "# Map probs to dict per row\n",
    "def row_proba(i):\n",
    "    if P is None: \n",
    "        return None\n",
    "    return {str(classes[j]): float(P[i, j]) for j in range(len(classes))}\n",
    "\n",
    "# ---------- Fuse and write NDJSON ----------\n",
    "fusion_path = OUT_DIR / \"fusion.ndjson\"\n",
    "with open(fusion_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for i in range(len(hist)):\n",
    "        pid = str(hist.iloc[i][\"Patient_ID\"]) if \"Patient_ID\" in hist.columns else f\"row_{i}\"\n",
    "        rec = {\n",
    "            \"patient_id\": pid,\n",
    "            \"history\": {\n",
    "                \"trend\": str(trend_pred[i]),\n",
    "                \"proba\": row_proba(i)\n",
    "            },\n",
    "            \"snapshot\": snap_records.get(pid, None),\n",
    "            \"alerts\": []  # you can add rule-based alerts here if you want\n",
    "        }\n",
    "        fout.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Wrote:\", fusion_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c387fa5-ad95-46ab-8397-6ca4b24ed6d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'to_numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 147\u001b[0m\n\u001b[0;32m    143\u001b[0m y_pred_sev \u001b[38;5;241m=\u001b[39m snap_pipe\u001b[38;5;241m.\u001b[39mpredict(Xs)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# class via cutpoints — produce a NumPy array\u001b[39;00m\n\u001b[0;32m    146\u001b[0m sev_cls \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mcut(y_pred_sev, bins\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1e9\u001b[39m, c1, c2, \u001b[38;5;241m1e9\u001b[39m],\n\u001b[1;32m--> 147\u001b[0m                  labels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLow\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModerate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHigh\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# build per-patient dict (FIX: index sev_cls by [i], not .iloc)\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(snap)):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'to_numpy'"
     ]
    }
   ],
   "source": [
    "# fuse_infer_generate.py  — fixed sev_cls indexing + rebuilds trend features\n",
    "\n",
    "import os, json, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# ----------------- paths -----------------\n",
    "ROOT = r\"C:\\Users\\aayus\\Downloads\\emr-smart\"\n",
    "DATA_DIR   = os.path.join(ROOT, \"data\")\n",
    "MODEL_DIR  = os.path.join(ROOT, \"models\")\n",
    "OUT_DIR    = os.path.join(ROOT, \"outputs\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "snap_path = os.path.join(DATA_DIR, \"emr_snapshot.csv\")\n",
    "hist_path = os.path.join(DATA_DIR, \"emr_history.csv\")\n",
    "\n",
    "snap_model_path = os.path.join(MODEL_DIR, \"snapshot_regressor.joblib\")\n",
    "hist_model_path = os.path.join(MODEL_DIR, \"best_history_classifier.joblib\")\n",
    "cutpoints_path  = os.path.join(MODEL_DIR, \"snapshot_cutpoints.txt\")\n",
    "\n",
    "# ----------------- helper: trend features (must match training) -----------------\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "ALL_BASES = [\n",
    "    \"Blood_Pressure_Systolic\",\"Blood_Pressure_Diastolic\",\"Heart_Rate\",\"Temperature\",\n",
    "    \"Respiratory_Rate\",\"Oxygen_Saturation\",\"Blood_Sugar\",\"Cholesterol_Total\",\"Weight\",\"BMI\"\n",
    "]\n",
    "\n",
    "class TrendFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, bases, eps=1.6):\n",
    "        self.bases = bases\n",
    "        self.eps   = eps\n",
    "        self.feature_names_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        feats = []\n",
    "        for b in self.bases:\n",
    "            feats += [\n",
    "                f\"{b}_delta\", f\"{b}_mean\", f\"{b}_std\", f\"{b}_slope\",\n",
    "                f\"{b}_stable\", f\"{b}_range\", f\"{b}_mean_abs_step\",\n",
    "                f\"{b}_cv\", f\"{b}_mad\", f\"{b}_abs_slope\"\n",
    "            ]\n",
    "        feats += [\"global_stability_ratio\", \"global_mean_abs_step\"]\n",
    "        self.feature_names_ = feats\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        W = np.array([1,2,3,4,5], dtype=float)\n",
    "        rows = []\n",
    "        for _, row in X.iterrows():\n",
    "            feats = []\n",
    "            stable_hits = 0\n",
    "            all_steps = []\n",
    "            for b in self.bases:\n",
    "                vals = np.array([row[f\"{b}_Week{i}\"] for i in range(1,6)], dtype=float)\n",
    "                delta = float(vals[-1] - vals[0])\n",
    "                meanv = float(np.mean(vals))\n",
    "                stdv  = float(np.std(vals, ddof=0))\n",
    "                slope = float(np.polyfit(W, vals, 1)[0])\n",
    "                steps = np.diff(vals)\n",
    "                max_step = float(np.max(np.abs(steps)))\n",
    "                stable = 1.0 if max_step < self.eps else 0.0\n",
    "                rng = float(np.max(vals) - np.min(vals))\n",
    "                mean_abs_step = float(np.mean(np.abs(steps)))\n",
    "                cv  = float(stdv / (meanv + 1e-6))\n",
    "                mad = float(np.median(np.abs(vals - np.median(vals))))\n",
    "                abs_slope = abs(slope)\n",
    "\n",
    "                feats += [delta, meanv, stdv, slope, stable,\n",
    "                          rng, mean_abs_step, cv, mad, abs_slope]\n",
    "                stable_hits += stable\n",
    "                all_steps.extend(np.abs(steps))\n",
    "\n",
    "            feats += [\n",
    "                stable_hits / max(len(self.bases), 1),\n",
    "                float(np.mean(all_steps)) if all_steps else 0.0\n",
    "            ]\n",
    "            rows.append(feats)\n",
    "        return np.array(rows)\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array(self.feature_names_)\n",
    "\n",
    "def build_history_design(df):\n",
    "    # Rebuild exactly like training\n",
    "    cat_cols = [c for c in df.columns if df[c].dtype == \"object\"]\n",
    "    num_cols = [c for c in df.columns if c not in cat_cols]\n",
    "    bases_present = [b for b in ALL_BASES if f\"{b}_Week1\" in df.columns and f\"{b}_Week5\" in df.columns]\n",
    "    if not bases_present:\n",
    "        raise ValueError(\"No *_Week1..Week5 series found in history dataset.\")\n",
    "\n",
    "    eng = TrendFeatureEngineer(bases_present, eps=1.6)\n",
    "    trend = eng.fit_transform(df)\n",
    "    tcols = eng.get_feature_names_out().tolist()\n",
    "\n",
    "    X_num = df[num_cols].to_numpy(dtype=float)\n",
    "    out = pd.DataFrame(X_num, columns=num_cols, index=df.index)\n",
    "    # concat all new columns at once (avoids fragmentation warnings)\n",
    "    trend_df = pd.DataFrame(trend, columns=tcols, index=df.index)\n",
    "    out = pd.concat([out, trend_df], axis=1)\n",
    "    # reattach categoricals\n",
    "    for c in cat_cols: out[c] = df[c].values\n",
    "    return out\n",
    "\n",
    "# ----------------- load models -----------------\n",
    "if not os.path.exists(hist_model_path):\n",
    "    raise FileNotFoundError(f\"Missing model: {hist_model_path}\")\n",
    "hist_pipe = joblib.load(hist_model_path)\n",
    "\n",
    "snap_pipe = None\n",
    "if os.path.exists(snap_model_path):\n",
    "    snap_pipe = joblib.load(snap_model_path)\n",
    "else:\n",
    "    warnings.warn(f\"Snapshot regressor not found at: {snap_model_path}. Snapshot inference will be skipped.\")\n",
    "\n",
    "# cutpoints for snapshot classes\n",
    "c1, c2 = 2.5, 5.6\n",
    "if os.path.exists(cutpoints_path):\n",
    "    try:\n",
    "        with open(cutpoints_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            txt = f.read()\n",
    "        # allow lines like \"c1=2.5\\nc2=5.6\"\n",
    "        for line in txt.splitlines():\n",
    "            if line.startswith(\"c1=\"): c1 = float(line.split(\"=\",1)[1])\n",
    "            if line.startswith(\"c2=\"): c2 = float(line.split(\"=\",1)[1])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# ----------------- SNAPSHOT inference -----------------\n",
    "snap_records = {}\n",
    "if snap_pipe is not None and os.path.exists(snap_path):\n",
    "    snap = pd.read_csv(snap_path)\n",
    "    pid_col = \"Patient_ID\" if \"Patient_ID\" in snap.columns else None\n",
    "\n",
    "    # use same columns as training: we saved a Pipeline(preproc, model) on raw snapshot features\n",
    "    drop_cols = [c for c in [\"Patient_ID\",\"Patient_Name\"] if c in snap.columns]\n",
    "    Xs = snap.drop(columns=drop_cols, errors=\"ignore\")\n",
    "\n",
    "    # predict\n",
    "    y_pred_sev = snap_pipe.predict(Xs)\n",
    "\n",
    "    # class via cutpoints — produce a NumPy array\n",
    "    sev_cls = pd.cut(y_pred_sev, bins=[-1e9, c1, c2, 1e9],\n",
    "                     labels=[\"Low\",\"Moderate\",\"High\"]).astype(str).to_numpy()\n",
    "\n",
    "    # build per-patient dict (FIX: index sev_cls by [i], not .iloc)\n",
    "    for i in range(len(snap)):\n",
    "        pid = str(snap.iloc[i][pid_col]) if pid_col else f\"row_{i}\"\n",
    "        snap_records[pid] = {\n",
    "            \"severity_numeric\": float(y_pred_sev[i]),\n",
    "            \"severity_label\":   str(sev_cls[i])\n",
    "        }\n",
    "    print(f\"[SNAPSHOT] Inferred {len(snap_records)} rows (classes via cutpoints c1={c1}, c2={c2}).\")\n",
    "else:\n",
    "    print(\"[SNAPSHOT] Skipped (no model or file).\")\n",
    "\n",
    "# ----------------- HISTORY inference (rebuild features) -----------------\n",
    "if not os.path.exists(hist_path):\n",
    "    raise FileNotFoundError(f\"History CSV missing: {hist_path}\")\n",
    "hist = pd.read_csv(hist_path)\n",
    "\n",
    "if \"Trend_Status\" in hist.columns:\n",
    "    y_true_hist = hist[\"Trend_Status\"].astype(str)\n",
    "else:\n",
    "    y_true_hist = None\n",
    "\n",
    "pid_col_h = \"Patient_ID\" if \"Patient_ID\" in hist.columns else None\n",
    "severity_cols = [c for c in hist.columns if str(c).startswith(\"Severity_Week\")]\n",
    "Xh_raw = hist.drop(columns=[c for c in [\"Trend_Status\", *severity_cols] if c in hist.columns], errors=\"ignore\")\n",
    "\n",
    "# rebuild design to match training\n",
    "Xh = build_history_design(Xh_raw)\n",
    "\n",
    "# predict labels + probabilities\n",
    "trend_pred = hist_pipe.predict(Xh)\n",
    "try:\n",
    "    P = hist_pipe.predict_proba(Xh)\n",
    "    classes = hist_pipe.named_steps[\"model\"].classes_\n",
    "except Exception:\n",
    "    # some sklearn classifiers expose classes_ at top-level\n",
    "    P = hist_pipe.predict_proba(Xh)\n",
    "    classes = hist_pipe.classes_\n",
    "\n",
    "cls_to_idx = {c:i for i,c in enumerate(classes)}\n",
    "def row_proba(i):\n",
    "    return {str(c): float(P[i, cls_to_idx[c]]) for c in classes}\n",
    "\n",
    "hist_records = {}\n",
    "for i in range(len(hist)):\n",
    "    pid = str(hist.iloc[i][pid_col_h]) if pid_col_h else f\"row_{i}\"\n",
    "    hist_records[pid] = {\n",
    "        \"trend\": str(trend_pred[i]),\n",
    "        \"proba\": row_proba(i)\n",
    "    }\n",
    "\n",
    "print(f\"[HISTORY] Inferred {len(hist_records)} rows with probabilities.\")\n",
    "\n",
    "# ----------------- FUSE + SAVE -----------------\n",
    "fusion_rows = []\n",
    "matched = 0\n",
    "for pid, hrec in hist_records.items():\n",
    "    srec = snap_records.get(pid)\n",
    "    out = {\n",
    "        \"patient_id\": pid,\n",
    "        \"history\": hrec,\n",
    "        \"snapshot\": srec if srec else None,\n",
    "        \"alerts\": []\n",
    "    }\n",
    "    # simple example alert logic (you can adjust later)\n",
    "    if srec and srec[\"severity_label\"] == \"High\":\n",
    "        out[\"alerts\"].append(\"High current severity\")\n",
    "    if hrec[\"trend\"] == \"Worsening\":\n",
    "        out[\"alerts\"].append(\"Worsening trend over 5 weeks\")\n",
    "    fusion_rows.append(out)\n",
    "    if srec: matched += 1\n",
    "\n",
    "print(f\"[FUSION] Built {len(fusion_rows)} records. Matched snapshot+history for {matched} patients.\")\n",
    "\n",
    "# write NDJSON for LLM step + a CSV for quick scanning\n",
    "ndjson_path = os.path.join(OUT_DIR, \"fusion.ndjson\")\n",
    "with open(ndjson_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for rec in fusion_rows:\n",
    "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "csv_rows = []\n",
    "for rec in fusion_rows:\n",
    "    pid = rec[\"patient_id\"]\n",
    "    sev_num = rec[\"snapshot\"][\"severity_numeric\"] if rec[\"snapshot\"] else np.nan\n",
    "    sev_lab = rec[\"snapshot\"][\"severity_label\"]   if rec[\"snapshot\"] else \"\"\n",
    "    tr_lab  = rec[\"history\"][\"trend\"]\n",
    "    pr      = rec[\"history\"][\"proba\"]\n",
    "    csv_rows.append({\n",
    "        \"patient_id\": pid,\n",
    "        \"severity_numeric\": sev_num,\n",
    "        \"severity_label\": sev_lab,\n",
    "        \"trend\": tr_lab,\n",
    "        \"p_improving\": pr.get(\"Improving\", np.nan),\n",
    "        \"p_stable\":    pr.get(\"Stable\", np.nan),\n",
    "        \"p_worsening\": pr.get(\"Worsening\", np.nan),\n",
    "        \"alerts\": \"; \".join(rec[\"alerts\"])\n",
    "    })\n",
    "\n",
    "fusion_csv_path = os.path.join(OUT_DIR, \"fusion.csv\")\n",
    "pd.DataFrame(csv_rows).to_csv(fusion_csv_path, index=False)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" \", ndjson_path)\n",
    "print(\" \", fusion_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5693ed4-af2c-4da9-8ef4-5e4936399940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SNAPSHOT] Inferred 1000 rows (classes via cutpoints c1=2.5, c2=5.6).\n",
      "[HISTORY] Inferred 1000 rows with probabilities.\n",
      "[FUSION] Built 1000 records. Matched snapshot+history for 1000 patients.\n",
      "Saved:\n",
      "  C:\\Users\\aayus\\Downloads\\emr-smart\\outputs\\fusion.ndjson\n",
      "  C:\\Users\\aayus\\Downloads\\emr-smart\\outputs\\fusion.csv\n"
     ]
    }
   ],
   "source": [
    "# fuse_infer_generate.py — unified inference & fusion (snapshot + history)\n",
    "# - Fix: snapshot class binning uses plain indexing (no .values / .iloc)\n",
    "# - Rebuilds TrendFeatureEngineer features to match training\n",
    "# - Keeps Severity_Week* cols and aligns to expected preprocessor inputs\n",
    "# - Writes outputs/fusion.ndjson and outputs/fusion.csv\n",
    "\n",
    "import os, json, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# ----------------- paths -----------------\n",
    "ROOT = r\"C:\\Users\\aayus\\Downloads\\emr-smart\"\n",
    "DATA_DIR   = os.path.join(ROOT, \"data\")\n",
    "MODEL_DIR  = os.path.join(ROOT, \"models\")\n",
    "OUT_DIR    = os.path.join(ROOT, \"outputs\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "snap_path = os.path.join(DATA_DIR, \"emr_snapshot.csv\")\n",
    "hist_path = os.path.join(DATA_DIR, \"emr_history.csv\")\n",
    "\n",
    "snap_model_path = os.path.join(MODEL_DIR, \"snapshot_regressor.joblib\")\n",
    "hist_model_path = os.path.join(MODEL_DIR, \"best_history_classifier.joblib\")\n",
    "cutpoints_path  = os.path.join(MODEL_DIR, \"snapshot_cutpoints.txt\")\n",
    "\n",
    "# ----------------- helper: trend features (match training) -----------------\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "ALL_BASES = [\n",
    "    \"Blood_Pressure_Systolic\",\"Blood_Pressure_Diastolic\",\"Heart_Rate\",\"Temperature\",\n",
    "    \"Respiratory_Rate\",\"Oxygen_Saturation\",\"Blood_Sugar\",\"Cholesterol_Total\",\"Weight\",\"BMI\"\n",
    "]\n",
    "\n",
    "class TrendFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, bases, eps=1.6):\n",
    "        self.bases = bases\n",
    "        self.eps   = eps\n",
    "        self.feature_names_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        feats = []\n",
    "        for b in self.bases:\n",
    "            feats += [\n",
    "                f\"{b}_delta\", f\"{b}_mean\", f\"{b}_std\", f\"{b}_slope\",\n",
    "                f\"{b}_stable\", f\"{b}_range\", f\"{b}_mean_abs_step\",\n",
    "                f\"{b}_cv\", f\"{b}_mad\", f\"{b}_abs_slope\"\n",
    "            ]\n",
    "        feats += [\"global_stability_ratio\", \"global_mean_abs_step\"]\n",
    "        self.feature_names_ = feats\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        W = np.array([1,2,3,4,5], dtype=float)\n",
    "        rows = []\n",
    "        for _, row in X.iterrows():\n",
    "            feats, stable_hits, all_steps = [], 0, []\n",
    "            for b in self.bases:\n",
    "                vals = np.array([row[f\"{b}_Week{i}\"] for i in range(1,6)], dtype=float)\n",
    "                delta = float(vals[-1] - vals[0])\n",
    "                meanv = float(np.mean(vals))\n",
    "                stdv  = float(np.std(vals, ddof=0))\n",
    "                slope = float(np.polyfit(W, vals, 1)[0])\n",
    "                steps = np.diff(vals)\n",
    "                max_step = float(np.max(np.abs(steps)))\n",
    "                stable = 1.0 if max_step < self.eps else 0.0\n",
    "                rng = float(np.max(vals) - np.min(vals))\n",
    "                mean_abs_step = float(np.mean(np.abs(steps)))\n",
    "                cv  = float(stdv / (meanv + 1e-6))\n",
    "                mad = float(np.median(np.abs(vals - np.median(vals))))\n",
    "                abs_slope = abs(slope)\n",
    "\n",
    "                feats += [delta, meanv, stdv, slope, stable,\n",
    "                          rng, mean_abs_step, cv, mad, abs_slope]\n",
    "                stable_hits += stable\n",
    "                all_steps.extend(np.abs(steps))\n",
    "\n",
    "            feats += [\n",
    "                stable_hits / max(len(self.bases), 1),\n",
    "                float(np.mean(all_steps)) if all_steps else 0.0\n",
    "            ]\n",
    "            rows.append(feats)\n",
    "        return np.array(rows)\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array(self.feature_names_)\n",
    "\n",
    "def build_history_design(df):\n",
    "    \"\"\"\n",
    "    Rebuilds the training-time design:\n",
    "    - Keep all numeric columns (including Severity_Week1..5 etc.)\n",
    "    - Add engineered trend features\n",
    "    - Re-attach categoricals\n",
    "    \"\"\"\n",
    "    cat_cols = [c for c in df.columns if df[c].dtype == \"object\"]\n",
    "    num_cols = [c for c in df.columns if c not in cat_cols]\n",
    "\n",
    "    bases_present = [b for b in ALL_BASES if f\"{b}_Week1\" in df.columns and f\"{b}_Week5\" in df.columns]\n",
    "    if not bases_present:\n",
    "        raise ValueError(\"No *_Week1..Week5 series found in history dataset.\")\n",
    "\n",
    "    eng = TrendFeatureEngineer(bases_present, eps=1.6)\n",
    "    trend = eng.fit_transform(df)\n",
    "    tcols = eng.get_feature_names_out().tolist()\n",
    "\n",
    "    X_num = df[num_cols].to_numpy(dtype=float)\n",
    "    out = pd.DataFrame(X_num, columns=num_cols, index=df.index)\n",
    "\n",
    "    # add all trend columns at once (avoid fragmentation warnings)\n",
    "    trend_df = pd.DataFrame(trend, columns=tcols, index=df.index)\n",
    "    out = pd.concat([out, trend_df], axis=1)\n",
    "\n",
    "    # reattach categoricals\n",
    "    for c in cat_cols:\n",
    "        out[c] = df[c].values\n",
    "\n",
    "    return out\n",
    "\n",
    "# --------------- align-to-preproc helper (robust to training/inference drift) ---------------\n",
    "def expected_columns_from_preproc(preproc):\n",
    "    \"\"\"Get the exact column names the fitted ColumnTransformer expects.\"\"\"\n",
    "    exp = []\n",
    "    for name, trans, cols in preproc.transformers_:\n",
    "        if name == \"remainder\":\n",
    "            continue\n",
    "        exp.extend(list(cols))\n",
    "    # preserve order, unique\n",
    "    return list(dict.fromkeys(exp))\n",
    "\n",
    "def align_to_expected_columns(X, preproc):\n",
    "    \"\"\"Add any missing expected columns as NaN so ColumnTransformer can run.\"\"\"\n",
    "    exp_cols = expected_columns_from_preproc(preproc)\n",
    "    missing = [c for c in exp_cols if c not in X.columns]\n",
    "    if missing:\n",
    "        for c in missing:\n",
    "            X[c] = np.nan\n",
    "    # keep column order starting with exp_cols, then any extras (remainder=\"drop\" by default)\n",
    "    # but ColumnTransformer will select by names, so order isn't critical\n",
    "    return X\n",
    "\n",
    "# ----------------- load models -----------------\n",
    "if not os.path.exists(hist_model_path):\n",
    "    raise FileNotFoundError(f\"Missing model: {hist_model_path}\")\n",
    "hist_pipe = joblib.load(hist_model_path)\n",
    "\n",
    "snap_pipe = None\n",
    "if os.path.exists(snap_model_path):\n",
    "    snap_pipe = joblib.load(snap_model_path)\n",
    "else:\n",
    "    warnings.warn(f\"Snapshot regressor not found at: {snap_model_path}. Snapshot inference will be skipped.\")\n",
    "\n",
    "# cutpoints for snapshot classes\n",
    "c1, c2 = 2.5, 5.6\n",
    "if os.path.exists(cutpoints_path):\n",
    "    try:\n",
    "        with open(cutpoints_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                if line.startswith(\"c1=\"): c1 = float(line.split(\"=\",1)[1])\n",
    "                if line.startswith(\"c2=\"): c2 = float(line.split(\"=\",1)[1])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# ----------------- SNAPSHOT inference -----------------\n",
    "snap_records = {}\n",
    "if snap_pipe is not None and os.path.exists(snap_path):\n",
    "    snap = pd.read_csv(snap_path)\n",
    "    pid_col = \"Patient_ID\" if \"Patient_ID\" in snap.columns else None\n",
    "\n",
    "    # same inputs as training (we trained on raw snapshot features minus IDs)\n",
    "    drop_cols = [c for c in [\"Patient_ID\",\"Patient_Name\"] if c in snap.columns]\n",
    "    Xs = snap.drop(columns=drop_cols, errors=\"ignore\")\n",
    "\n",
    "    # predict numeric severity\n",
    "    y_pred_sev = snap_pipe.predict(Xs)\n",
    "\n",
    "    # class via cutpoints -> keep array-like, no .values / no .iloc\n",
    "    sev_cls = pd.cut(\n",
    "        y_pred_sev, bins=[-1e9, c1, c2, 1e9], labels=[\"Low\",\"Moderate\",\"High\"]\n",
    "    ).astype(str)\n",
    "\n",
    "    # build per-patient dict\n",
    "    for i in range(len(snap)):\n",
    "        pid = str(snap.iloc[i][pid_col]) if pid_col else f\"row_{i}\"\n",
    "        snap_records[pid] = {\n",
    "            \"severity_numeric\": float(y_pred_sev[i]),\n",
    "            \"severity_label\":   str(sev_cls[i])\n",
    "        }\n",
    "    print(f\"[SNAPSHOT] Inferred {len(snap_records)} rows (classes via cutpoints c1={c1}, c2={c2}).\")\n",
    "else:\n",
    "    print(\"[SNAPSHOT] Skipped (no model or file).\")\n",
    "\n",
    "# ----------------- HISTORY inference (rebuild features) -----------------\n",
    "if not os.path.exists(hist_path):\n",
    "    raise FileNotFoundError(f\"History CSV missing: {hist_path}\")\n",
    "hist = pd.read_csv(hist_path)\n",
    "\n",
    "# ground-truth (optional)\n",
    "y_true_hist = hist[\"Trend_Status\"].astype(str) if \"Trend_Status\" in hist.columns else None\n",
    "\n",
    "pid_col_h = \"Patient_ID\" if \"Patient_ID\" in hist.columns else None\n",
    "\n",
    "# IMPORTANT: do NOT drop Severity_Week* (the model likely expects them).\n",
    "# Drop only label/ID columns that were dropped in training.\n",
    "cols_to_drop = [c for c in [\"Trend_Status\", \"Patient_ID\"] if c in hist.columns]\n",
    "Xh_raw = hist.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "\n",
    "# rebuild design (adds engineered trend features)\n",
    "Xh = build_history_design(Xh_raw)\n",
    "\n",
    "# align to the fitted preprocessor's expected input columns (robust to drift)\n",
    "preproc = hist_pipe.named_steps.get(\"preprocess\", None)\n",
    "if preproc is not None:\n",
    "    Xh = align_to_expected_columns(Xh, preproc)\n",
    "\n",
    "# predict labels + probabilities\n",
    "trend_pred = hist_pipe.predict(Xh)\n",
    "try:\n",
    "    P = hist_pipe.predict_proba(Xh)\n",
    "    classes = hist_pipe.named_steps[\"model\"].classes_\n",
    "except Exception:\n",
    "    P = hist_pipe.predict_proba(Xh)\n",
    "    classes = hist_pipe.classes_\n",
    "\n",
    "cls_to_idx = {c:i for i,c in enumerate(classes)}\n",
    "def row_proba(i):\n",
    "    return {str(c): float(P[i, cls_to_idx[c]]) for c in classes}\n",
    "\n",
    "hist_records = {}\n",
    "for i in range(len(hist)):\n",
    "    pid = str(hist.iloc[i][pid_col_h]) if pid_col_h else f\"row_{i}\"\n",
    "    hist_records[pid] = {\n",
    "        \"trend\": str(trend_pred[i]),\n",
    "        \"proba\": row_proba(i)\n",
    "    }\n",
    "\n",
    "print(f\"[HISTORY] Inferred {len(hist_records)} rows with probabilities.\")\n",
    "\n",
    "# ----------------- FUSE + SAVE -----------------\n",
    "fusion_rows = []\n",
    "matched = 0\n",
    "for pid, hrec in hist_records.items():\n",
    "    srec = snap_records.get(pid)\n",
    "    out = {\n",
    "        \"patient_id\": pid,\n",
    "        \"history\": hrec,\n",
    "        \"snapshot\": srec if srec else None,\n",
    "        \"alerts\": []\n",
    "    }\n",
    "    # simple alerts (adjust later as needed)\n",
    "    if srec and srec[\"severity_label\"] == \"High\":\n",
    "        out[\"alerts\"].append(\"High current severity\")\n",
    "    if hrec[\"trend\"] == \"Worsening\":\n",
    "        out[\"alerts\"].append(\"Worsening trend over 5 weeks\")\n",
    "    fusion_rows.append(out)\n",
    "    if srec: matched += 1\n",
    "\n",
    "print(f\"[FUSION] Built {len(fusion_rows)} records. Matched snapshot+history for {matched} patients.\")\n",
    "\n",
    "# NDJSON for LLM + CSV for quick scan\n",
    "ndjson_path = os.path.join(OUT_DIR, \"fusion.ndjson\")\n",
    "with open(ndjson_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for rec in fusion_rows:\n",
    "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "csv_rows = []\n",
    "for rec in fusion_rows:\n",
    "    pid = rec[\"patient_id\"]\n",
    "    sev_num = rec[\"snapshot\"][\"severity_numeric\"] if rec[\"snapshot\"] else np.nan\n",
    "    sev_lab = rec[\"snapshot\"][\"severity_label\"]   if rec[\"snapshot\"] else \"\"\n",
    "    tr_lab  = rec[\"history\"][\"trend\"]\n",
    "    pr      = rec[\"history\"][\"proba\"]\n",
    "    csv_rows.append({\n",
    "        \"patient_id\": pid,\n",
    "        \"severity_numeric\": sev_num,\n",
    "        \"severity_label\": sev_lab,\n",
    "        \"trend\": tr_lab,\n",
    "        \"p_improving\": pr.get(\"Improving\", np.nan),\n",
    "        \"p_stable\":    pr.get(\"Stable\", np.nan),\n",
    "        \"p_worsening\": pr.get(\"Worsening\", np.nan),\n",
    "        \"alerts\": \"; \".join(rec[\"alerts\"])\n",
    "    })\n",
    "\n",
    "fusion_csv_path = os.path.join(OUT_DIR, \"fusion.csv\")\n",
    "pd.DataFrame(csv_rows).to_csv(fusion_csv_path, index=False)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" \", ndjson_path)\n",
    "print(\" \", fusion_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f4ebde-f70c-45f5-9895-5265aab63ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
